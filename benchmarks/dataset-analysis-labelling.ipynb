{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Annotation analisys (labelling)\n",
    "\n",
    "This notebook is an attempt to compute dynamic statistics of the dataset tags and labels."
   ],
   "id": "bcc1bb29155b4b14"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from difflib import SequenceMatcher\n",
    "from pathlib import Path\n",
    "\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag"
   ],
   "id": "a2b8c93b08a6975c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "984d11ca8591c2eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from typing import Union, List\n",
    "\n",
    "\n",
    "def get_nodes(soup) -> Union[List, List[List]]:\n",
    "    tags_text = []\n",
    "\n",
    "    paragraph_tag = \"p\"\n",
    "\n",
    "    for child in soup.tei.children:\n",
    "        if child.name == 'text':\n",
    "            tags_text.extend([paragraph for paragraph in child.find_all(paragraph_tag)])\n",
    "            # tags_captions.extend([paragraph for paragraph in child.find_all(\"ab\")])\n",
    "\n",
    "    return tags_text\n",
    "\n",
    "\n",
    "def process_dir(input):\n",
    "    accumulated_statistics = []\n",
    "    for root, dirs, files in os.walk(input):\n",
    "        for file_ in files:\n",
    "            if not file_.lower().endswith(\".xml\"):\n",
    "                continue\n",
    "            abs_path = os.path.join(root, file_)\n",
    "            print(\"Processing: \" + str(abs_path))\n",
    "            output_data = process_file(abs_path)\n",
    "            accumulated_statistics.append(output_data)\n",
    "\n",
    "    return accumulated_statistics\n",
    "\n",
    "\n",
    "def process_file(input):\n",
    "    with open(input, encoding='utf-8') as fp:\n",
    "        doc = fp.read()\n",
    "\n",
    "    soup = BeautifulSoup(doc, 'xml')\n",
    "\n",
    "    entities_statistics = {}\n",
    "    document_statistics = {\n",
    "        'name': Path(input).name,\n",
    "        'path': str(Path(input).absolute()),\n",
    "        'paragraphs': 0,\n",
    "        'entity_no_type': 0,\n",
    "        'entities': 0,\n",
    "        'uniq_entities': 0,\n",
    "        'classes': 0,\n",
    "        'entities_statistics': entities_statistics\n",
    "    }\n",
    "    \n",
    "    paragraphs = get_nodes(soup)\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        document_statistics['paragraphs'] += 1\n",
    "        j = 0\n",
    "        paragraphText = ''\n",
    "        for item in paragraph.contents:\n",
    "            if type(item) == NavigableString:\n",
    "                paragraphText += str(item)\n",
    "\n",
    "            elif type(item) is Tag and item.name == 'rs':\n",
    "                paragraphText += item.text\n",
    "                document_statistics['entities'] += 1\n",
    "                tag_content = item.text\n",
    "                paragraphText += str(item.text)\n",
    "                tag_name = None\n",
    "                if 'type' not in item.attrs:\n",
    "                    document_statistics['entity_no_type'] += 1\n",
    "                else:\n",
    "                    tag_name = item.attrs['type']\n",
    "\n",
    "                if tag_name not in entities_statistics:\n",
    "                    entities_statistics[tag_name] = {\n",
    "                        'count': 1,\n",
    "                        'content_distribution': {\n",
    "                            tag_content: 1\n",
    "                        }\n",
    "                    }\n",
    "                else:\n",
    "                    content_ = entities_statistics[tag_name]\n",
    "                    content_['count'] += 1\n",
    "                    if tag_content not in content_['content_distribution']:\n",
    "                        content_['content_distribution'][tag_content] = 1\n",
    "                    else:\n",
    "                        content_['content_distribution'][tag_content] += 1\n",
    "\n",
    "            document_statistics['classes'] = len(set(entities_statistics.keys()))\n",
    "\n",
    "    uniq_entities = 0\n",
    "    for key in entities_statistics:\n",
    "        uniq_entities += len(entities_statistics[key]['content_distribution'])\n",
    "\n",
    "    document_statistics['uniq_entities'] = uniq_entities\n",
    "\n",
    "    ## Cross-checks\n",
    "\n",
    "    # Verify that the sum of the content distribution corresponds to the tag distribution\n",
    "    total_entities = 0\n",
    "    for tag in entities_statistics:\n",
    "        count = entities_statistics[tag]['count']\n",
    "        sum_content_distributions = 0\n",
    "        content_distribution_dict = entities_statistics[tag]['content_distribution']\n",
    "        for content in content_distribution_dict:\n",
    "            sum_content_distributions += content_distribution_dict[content]\n",
    "\n",
    "        assert \"Number of total entities per tag does not correspond to the sum.\", count == sum_content_distributions\n",
    "        total_entities += count\n",
    "\n",
    "    assert \"Number of total entities per documnent does not correspond to the sum.\", total_entities == document_statistics['entities']\n",
    "\n",
    "    return document_statistics\n",
    "\n",
    "\n",
    "def group_by_with_soft_matching(input_list, threshold):\n",
    "    matching = {}\n",
    "    last_matching = -1\n",
    "\n",
    "    for index_x, x in enumerate(input_list):\n",
    "        unpacked = [y for x in matching for y in matching[x]]\n",
    "        if x not in matching and x not in unpacked:\n",
    "            matching[x] = []\n",
    "\n",
    "            for index_y, y in enumerate(input_list[index_x + 1:]):\n",
    "                if x == y:\n",
    "                    continue\n",
    "\n",
    "                if SequenceMatcher(None, x.lower(), y.lower()).ratio() > threshold:\n",
    "                    matching[x].append(y)\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return matching\n",
    "\n",
    "\n",
    "def aggregate(entities_statistics, threshold):\n",
    "    \"\"\"\n",
    "    Aggregate the statistics by merging content belonging to the same entity:\n",
    "     - variation of expressions (e.g. cuprates, cuprate, Cuprates, ...)\n",
    "     - synonyms (e.g. 111, cuprates, ...)\n",
    "\n",
    "    :param document_statistics:\n",
    "    :param threshold:\n",
    "    :return: an aggregated statistics for documents\n",
    "    \"\"\"\n",
    "\n",
    "    agg = {}\n",
    "\n",
    "    for tag in entities_statistics:\n",
    "        if tag == 'tcValue' or tag == 'pressure':\n",
    "            continue\n",
    "\n",
    "        distribution = entities_statistics[tag][\"content_distribution\"]\n",
    "\n",
    "        content_list = sorted(distribution.keys())\n",
    "        # hash_list = []\n",
    "        # for content in content_list:\n",
    "        #     hash_value = content.lower().replace(\" \", \"\")\n",
    "        #     hash_list.append((hash_value, content))\n",
    "\n",
    "        aggregated = group_by_with_soft_matching(content_list, threshold)\n",
    "\n",
    "        agg[tag] = aggregated\n",
    "\n",
    "        assert \"Total number of element does not corresponds with the aggregated ones\", len(content_list) == (\n",
    "                len(agg.keys()) + len([y for x in aggregated for y in aggregated[x]]))\n",
    "\n",
    "    return agg\n",
    "\n",
    "\n",
    "def extract_csv(output_data):\n",
    "    entity_statistics = output_data['entities_statistics']\n",
    "    csv_rows = []\n",
    "    for tag in entity_statistics:\n",
    "        for content in entity_statistics[tag]['content_distribution']:\n",
    "            row = [tag, content, entity_statistics[tag]['content_distribution'][content]]\n",
    "            csv_rows.append(row)\n",
    "\n",
    "    return csv_rows\n",
    "\n",
    "\n",
    "def intersection(lst1, lst2):\n",
    "    # Use of hybrid method\n",
    "    temp = set(lst2)\n",
    "    lst3 = [value for value in lst1 if value in temp]\n",
    "    return lst3\n",
    "\n",
    "\n",
    "def extract_inconsistencies(output_data):\n",
    "    entity_statistics = output_data['entities_statistics']\n",
    "    summary_content = {}\n",
    "    for tag in entity_statistics:\n",
    "        for content in entity_statistics[tag]['content_distribution']:\n",
    "            if tag in summary_content:\n",
    "                summary_content[tag].append(content)\n",
    "            else:\n",
    "                summary_content[tag] = [content]\n",
    "\n",
    "    inconsistencies = []\n",
    "\n",
    "    tags = list(summary_content.keys())\n",
    "    for id1 in range(0, len(tags)):\n",
    "        for id2 in range(id1 + 1, len(tags)):\n",
    "            tag1 = tags[id1]\n",
    "            tag2 = tags[id2]\n",
    "\n",
    "            tag1_content = summary_content[tag1]\n",
    "            tag2_content = summary_content[tag2]\n",
    "\n",
    "            intersected_content = intersection(tag1_content, tag2_content)\n",
    "\n",
    "            if len(intersected_content) > 0:\n",
    "                for intersected_content_ in intersected_content:\n",
    "                    frequency1 = entity_statistics[tag1]['content_distribution'][intersected_content_]\n",
    "                    frequency2 = entity_statistics[tag2]['content_distribution'][intersected_content_]\n",
    "                    intersected_tags = [(tag1, frequency1), (tag2, frequency2)]\n",
    "                    inconsistencies.append([intersected_content_, tag1, frequency1, tag2, frequency2])\n",
    "\n",
    "    return inconsistencies\n",
    "\n",
    "def find_longest_entities(output_data, topValues=10): \n",
    "    print(output_data)\n"
   ],
   "id": "f0b601b9bc03454f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Analysis ",
   "id": "62d3fd9da9453063"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_analysis(input): \n",
    "    output_data = {}\n",
    "\n",
    "    input_path = Path(input)\n",
    "    documents_statistics = process_dir(input_path)\n",
    "\n",
    "    aggregated_entities_statistics = {}\n",
    "    output_data = {\n",
    "        'path': str(Path(input_path).absolute()),\n",
    "        'files': len(documents_statistics),\n",
    "        'paragraphs': 0,\n",
    "        'entities_no_type': 0,\n",
    "        'entities': 0,\n",
    "        'uniq_entities': 0,\n",
    "        'classes': 0,\n",
    "        'entities_statistics': aggregated_entities_statistics\n",
    "    }\n",
    "\n",
    "    classes = []\n",
    "\n",
    "    ## Summary of all articles\n",
    "\n",
    "    for document_statistics in documents_statistics:\n",
    "        output_data['paragraphs'] += document_statistics['paragraphs']\n",
    "        output_data['entities_no_type'] += document_statistics['entity_no_type']\n",
    "        output_data['entities'] += document_statistics['entities']\n",
    "        output_data['uniq_entities'] += document_statistics['uniq_entities']\n",
    "\n",
    "        for tag in document_statistics['entities_statistics']:\n",
    "            classes.append(tag)\n",
    "            tag_statistics = document_statistics['entities_statistics'][tag]\n",
    "            if tag not in aggregated_entities_statistics:\n",
    "                aggregated_entities_statistics[tag] = tag_statistics\n",
    "            else:\n",
    "                count = tag_statistics['count']\n",
    "                aggregated_entities_statistics[tag]['count'] += count\n",
    "\n",
    "                dist = tag_statistics['content_distribution']\n",
    "                aggregated_distribution = aggregated_entities_statistics[tag]['content_distribution']\n",
    "\n",
    "                for content in dist:\n",
    "                    if content not in aggregated_distribution:\n",
    "                        aggregated_distribution[content] = dist[content]\n",
    "                    else:\n",
    "                        aggregated_distribution[content] += dist[content]\n",
    "\n",
    "        output_data['classes'] = len(set(classes))\n",
    "\n",
    "    output_data['documents'] = documents_statistics\n",
    "    output_data['aggregated_statistics'] = aggregate(aggregated_entities_statistics, 0.90)\n",
    "    output_data[\"inconsistencies\"] = extract_inconsistencies(output_data)\n",
    "    \n",
    "    #find_longest_entities(output_data)\n",
    "\n",
    "    return output_data "
   ],
   "id": "c84ed79d7d56b87b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Run Analysis ",
   "id": "373406d920a98afe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "input = \"../documents/tei-annotated\"\n",
    "    \n",
    "result = run_analysis(input)    "
   ],
   "id": "e0501865c539f1a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Extract summary on class repartition by frequency",
   "id": "ccf2ff3d6265b4fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "columns=['files', 'paragraphs', 'entities_no_type', 'entities', 'uniq_entities', 'classes']\n",
    "rows = [result[c] for c in columns]\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame([rows], columns=columns)"
   ],
   "id": "f9ed995d3569dde2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "aggregated_statistics = result['aggregated_statistics']\n",
    "entities_statistics = result['entities_statistics']\n",
    "\n",
    "# print(json.dumps(entities_statistics, indent=4))\n",
    "\n",
    "labels = []\n",
    "values = []\n",
    "\n",
    "for label in entities_statistics.keys(): \n",
    "    labels.append(label)\n",
    "    values.append(entities_statistics[label]['count'])\n",
    "\n",
    "    \n",
    "## PIE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def func(pct, allvals):\n",
    "    absolute = int(pct/100.*np.sum(allvals))\n",
    "    return \"{:.1f}%\\n({:d})\".format(pct, absolute)\n",
    "    \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 8), subplot_kw=dict(aspect=\"equal\"))\n",
    "\n",
    "wedges, texts, autotexts = ax.pie(values, autopct=lambda pct: func(pct, values),\n",
    "                                  textprops=dict(color=\"w\"))\n",
    "\n",
    "\n",
    "ax.legend(wedges, labels,\n",
    "          title=\"Labels\",\n",
    "          loc=\"center left\",\n",
    "          bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "\n",
    "plt.setp(autotexts, size=8, weight=\"bold\")\n",
    "ax.set_title(\"Class repartition by frequency\")\n",
    "plt.show()\n",
    "\n",
    "## HISTOGRAM\n",
    "\n",
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.barh(labels, values, align='center')\n",
    "ax.set_yticklabels(labels)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Frequency')\n",
    "ax.set_title('Class repartition by frequency')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#csv_row = extract_csv(output_data)\n"
   ],
   "id": "531a8d004987dde8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Inconsistencies \n",
    "\n",
    "In the following section we show annotations that have been annotated in different ways. \n",
    "For example if a certain name \"xyz\" has been annotated twice with two different annotations, it is listed under here. \n",
    "\n",
    "\n",
    "This doesn't means necessarly it's an error. We can conclude that the inconsistency tc-material can be attributed to mistakes, while material-class is due to ambiguities and overlapping in the definition of both labels where the outcome depends strongly from the context. "
   ],
   "id": "560b7dd9d655a0d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "inconsistent_classes = result[\"inconsistencies\"]\n",
    "\n",
    "# print(inconsistent_classes)\n",
    "\n",
    "rows = inconsistent_classes\n",
    "columns=['name', 'class 1', 'frequency', 'class 2', 'frequency']\n",
    "    \n",
    "import pandas as pd\n",
    "pd.DataFrame(rows, columns=columns)"
   ],
   "id": "5f9570b49f4f5e78"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The following code identifies content in entities that it's length is more than 400% the average length of the content for each class.",
   "id": "f984bcc9be80ace"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Label summary\n",
    "\n",
    "In this section we analise each label and we output \n",
    " - top terms \n",
    " - the aggregation of similar terms using soft-matching"
   ],
   "id": "77d796105bde45f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_top_10(label, statistics): \n",
    "    class_statistics = statistics[label]\n",
    "    class_count = class_statistics['count']\n",
    "    print(\"count: \" + str(class_count))\n",
    "    class_frequency = class_statistics['content_distribution']\n",
    "           \n",
    "    sorted_by_value = {k: v for k, v in sorted(class_frequency.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    top_10 = {k:sorted_by_value[k] for k in list(sorted_by_value.keys())[0:10]}\n",
    "    # print(json.dumps(top_10, indent=4))\n",
    "\n",
    "    ## HISTOGRAM\n",
    "\n",
    "    plt.rcdefaults()\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    items = list(top_10.values())\n",
    "    keys = list(top_10.keys())\n",
    "\n",
    "    ax.barh(keys, items , align='center')\n",
    "    ax.set_yticklabels(keys)\n",
    "    ax.invert_yaxis()  # labels read top-to-bottom\n",
    "    ax.set_xlabel('Frequency')\n",
    "    ax.set_title(label +' annotation values top terms')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def aggregate_by_soft_matching(statistics, aggregated):\n",
    "\n",
    "#     agg_statistics_soft_matching = aggregated[label]\n",
    "#     print(json.dumps(agg_statistics_soft_matching, indent=4))\n",
    "\n",
    "    delete = []\n",
    "    for k, v in aggregated.items(): \n",
    "        if len(v) > 0: \n",
    "            for val in v:\n",
    "                \n",
    "                statistics[k] += statistics[val]\n",
    "                statistics[val] = 0\n",
    "                delete.append(val)\n",
    "\n",
    "    # print(json.dumps(stat, indent=4))\n",
    "    print(\"Aggregating \" + str(len(delete)) + \" elements\")\n",
    "    \n",
    "    return statistics    "
   ],
   "id": "a7c340b1d7ca57ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Analysis labels\n",
   "id": "20cd8a32bcc5a35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import copy\n",
    "def show_top_10(label):\n",
    "    entities_statistics = copy.copy(result['entities_statistics'])\n",
    "    content_distribution = entities_statistics[label]['content_distribution']\n",
    "    aggregated_statistics = result['aggregated_statistics'][label]\n",
    "    entities_statistics[label]['content_distribution'] = aggregate_by_soft_matching(content_distribution, aggregated_statistics)\n",
    "    plot_top_10(label, entities_statistics)\n"
   ],
   "id": "ad03e2929c0e83db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "show_top_10(\"software\")\n",
   "id": "261d0ad602b337d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "show_top_10(\"url\")",
   "id": "be575f2574342129"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "show_top_10(\"version\")",
   "id": "1e7a607ccacd33aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "70615cf68e2bbd33"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
