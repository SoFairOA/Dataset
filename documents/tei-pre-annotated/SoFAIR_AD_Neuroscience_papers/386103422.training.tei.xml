<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-14T14:07+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Background and objective: Convolutional neural networks (CNNs) play an important role in the field of medical image segmentation. Among many kinds of CNNs, the U-net architecture is one of the most famous fully convolutional network architectures for medical semantic segmentation tasks. Recent work shows that the U-net network can be substantially deeper thus resulting in improved performance on segmentation tasks. Though adding more layers directly into network is a popular way to make a network deeper, it may lead to gradient vanishing or redundant computation during training.Background and objective: Convolutional neural networks (CNNs) play an important role in the field of medical image segmentation. Among many kinds of CNNs, the U-net architecture is one of the most famous fully convolutional network architectures for medical semantic segmentation tasks. Recent work shows that the U-net network can be substantially deeper thus resulting in improved performance on segmentation tasks. Though adding more layers directly into network is a popular way to make a network deeper, it may lead to gradient vanishing or redundant computation during training.</p>
        <p>A novel CNN architecture is proposed that integrates the Inception-Res module and densely connecting convolutional module into the U-net architecture. The proposed network model consists of the following parts: firstly, the Inception-Res block is designed to increase the width of the network by replacing the standard convolutional layers; secondly, the Dense-Inception block is designed to extract features and make the network more deep without additional parameters; thirdly, the down-sampling block is adopted to reduce the size of feature maps to accelerate learning and the up-sampling block is used to resize the feature maps.A novel CNN architecture is proposed that integrates the Inception-Res module and densely connecting convolutional module into the U-net architecture. The proposed network model consists of the following parts: firstly, the Inception-Res block is designed to increase the width of the network by replacing the standard convolutional layers; secondly, the Dense-Inception block is designed to extract features and make the network more deep without additional parameters; thirdly, the down-sampling block is adopted to reduce the size of feature maps to accelerate learning and the up-sampling block is used to resize the feature maps.</p>
        <p>The proposed model is tested on images of blood vessel segmentations from retina images, the lung segmentation of CT Data from the benchmark Kaggle datasets and the MRI scan brain tumor segmentation datasets from MICCAI BraTS 2017. The experimental results show that the proposed method can provide better performance on these two tasks compared with the state-of-the-art algorithms.The proposed model is tested on images of blood vessel segmentations from retina images, the lung segmentation of CT Data from the benchmark Kaggle datasets and the MRI scan brain tumor segmentation datasets from MICCAI BraTS 2017. The experimental results show that the proposed method can provide better performance on these two tasks compared with the state-of-the-art algorithms.</p>
        <p>The results reach an average Dice score of 0.9857 in the lung segmentation. For the blood vessel segmentation, the results reach an average Dice score of 0.9582. For the brain tumor segmentation, the results reach an average Dice score of 0.9867.The results reach an average Dice score of 0.9857 in the lung segmentation. For the blood vessel segmentation, the results reach an average Dice score of 0.9582. For the brain tumor segmentation, the results reach an average Dice score of 0.9867.</p>
        <p>The experiments highlighted that combining the inception module with dense connections in the U-Net architecture is a promising approach for semantic medical image segmentation.The experiments highlighted that combining the inception module with dense connections in the U-Net architecture is a promising approach for semantic medical image segmentation.</p>
        <p>Medical image segmentation has played an important role in the field of medical image analysis and attracted much attention from researchers in image processing [1]. Compared with the classical segmentation methods [2], algorithms based on Deep Learning have provided state-of-art performance and have become very popular [3]. During recent years, with the development of hardware and GPUs, several deep learning models have been proposed, such as AlexNet [4], VGG [5], 
            <rs type="software">DeepLab</rs> [6], 
            <rs type="software">GoogLeNet</rs> [7], Residual network [8] and DenseNet [9]. These network models have achieved good performance in the field of computer vision.
        </p>
        <p>With respect to semantic medical image segmentation, the Fully Convolutional Networks (FCN) [10] provide superior performance when compared with other deep learning models. Among the various FCN architectures, the U-net [11] model is one of the most popular fully convolutional network models and it is widely used in the medical image processing field. The U-net model is a pixel-to-pixel, end-to-end fully convolutional network with skip layers between analysis path and synthesis path [11]. The U-Net needs less training resources and can reserve more important feature information. However, the standard U-Net architecture contains only a few layers and therefore it is not currently deep enough to gain 3 improved performance over other existing networks. To solve the problem, adding more layers directly to the network can enlarge the parameter space and make the network deeper, which may lead to gradient vanishing and redundant computation during training [12]. Gradient vanishing means if the network contains too many hidden layers, the learning rate will decrease with forward propagation, which may decrease the overall network learning. Therefore, the novel contribution of this paper is to make the U-Net deeper and wider without redundant computation and gradient vanishing to improve medical image segmentation.With respect to semantic medical image segmentation, the Fully Convolutional Networks (FCN) [10] provide superior performance when compared with other deep learning models. Among the various FCN architectures, the U-net [11] model is one of the most popular fully convolutional network models and it is widely used in the medical image processing field. The U-net model is a pixel-to-pixel, end-to-end fully convolutional network with skip layers between analysis path and synthesis path [11]. The U-Net needs less training resources and can reserve more important feature information. However, the standard U-Net architecture contains only a few layers and therefore it is not currently deep enough to gain 3 improved performance over other existing networks. To solve the problem, adding more layers directly to the network can enlarge the parameter space and make the network deeper, which may lead to gradient vanishing and redundant computation during training [12]. Gradient vanishing means if the network contains too many hidden layers, the learning rate will decrease with forward propagation, which may decrease the overall network learning. Therefore, the novel contribution of this paper is to make the U-Net deeper and wider without redundant computation and gradient vanishing to improve medical image segmentation.</p>
        <p>In order to solve the aforementioned problems, two steps are considered: firstly, reduce redundant computation when making the network wider; secondly, avoid gradient vanishing while making the network deeper. With respect to reducing redundant computation, existing research indicates that sparse matrices can be clustered into dense sub-matrices to improve calculation and performance [13]. Network models like AlexNet or VGG demonstrate good experimental performance but they are computationally expensive. Compared with the models above, the GoogLeNet architecture proposed the concept of an "Inception module" to build a sparse, high performance computing network architecture [7]. The main advantage of this module is improving the utilization rate of computing resources by increasing the depth and width of network with the inception module while keeping the computational budget constant [14].In order to solve the aforementioned problems, two steps are considered: firstly, reduce redundant computation when making the network wider; secondly, avoid gradient vanishing while making the network deeper. With respect to reducing redundant computation, existing research indicates that sparse matrices can be clustered into dense sub-matrices to improve calculation and performance [13]. Network models like AlexNet or VGG demonstrate good experimental performance but they are computationally expensive. Compared with the models above, the GoogLeNet architecture proposed the concept of an "Inception module" to build a sparse, high performance computing network architecture [7]. The main advantage of this module is improving the utilization rate of computing resources by increasing the depth and width of network with the inception module while keeping the computational budget constant [14].</p>
        <p>In the second step, recent research shows that the neural network does not have to be a progressive architecture and remains converging even though some layers are dropped randomly [15]. Inspired by this idea, a deep network model called DenseNet was proposed as a kind of convolutional neural network with dense connections. The DenseNet architecture has various advantages: parameter simplicity, vanishing-gradient reducing and feature reuse [16]. These characteristics make the networks a very good fit for semantic segmentation as they naturally induce skip connections and multi-scale supervision.In the second step, recent research shows that the neural network does not have to be a progressive architecture and remains converging even though some layers are dropped randomly [15]. Inspired by this idea, a deep network model called DenseNet was proposed as a kind of convolutional neural network with dense connections. The DenseNet architecture has various advantages: parameter simplicity, vanishing-gradient reducing and feature reuse [16]. These characteristics make the networks a very good fit for semantic segmentation as they naturally induce skip connections and multi-scale supervision.</p>
        <p>Inspired by the works introduced above, in order to develop a deep learning network appropriate for medical image segmentation tasks, we propose an architecture that combines the inception module with the densely connected convolutions based on the U-Net architecture named Dense-Inception U-net (DIU
            <rs type="software">-Net</rs>). The details are as followed:
        </p>
        <p>(1) Based on the architecture of U-Net, the analysis path and synthesis path are used in a network with skip connections to transmit feature maps directly from the down-sampling process to the upsampling process.(1) Based on the architecture of U-Net, the analysis path and synthesis path are used in a network with skip connections to transmit feature maps directly from the down-sampling process to the upsampling process.</p>
        <p>(2) Based on GoogLeNet, in order to make the network wider without gradient vanishing, every convolutional layer is replaced by an Inception-Res module with different sizes of convolutional kernels, and residual connection is used in each module.(2) Based on GoogLeNet, in order to make the network wider without gradient vanishing, every convolutional layer is replaced by an Inception-Res module with different sizes of convolutional kernels, and residual connection is used in each module.</p>
        <p>(3) In the middle of network, dense connecting is used to make the network deeper, and in these dense blocks standard convolutional layers are also replaced by the Inception-Res modules.(3) In the middle of network, dense connecting is used to make the network deeper, and in these dense blocks standard convolutional layers are also replaced by the Inception-Res modules.</p>
        <p>(4) All the convolutional blocks except the bottleneck layers are followed by a batch normalization layer [17] to avoid gradient vanishing.(4) All the convolutional blocks except the bottleneck layers are followed by a batch normalization layer [17] to avoid gradient vanishing.</p>
        <p>The proposed architecture will be evaluated with respect to accuracy and practicality, on three clinical segmentation problems, namely lung segmentation in CT Data from the Kaggle datasets [18] , the blood vessel segmentations from retina images [19] and the MRI brain tumor segmentation from Multimodal Brain Tumor Segmentation Challenge 2017 [20].The proposed architecture will be evaluated with respect to accuracy and practicality, on three clinical segmentation problems, namely lung segmentation in CT Data from the Kaggle datasets [18] , the blood vessel segmentations from retina images [19] and the MRI brain tumor segmentation from Multimodal Brain Tumor Segmentation Challenge 2017 [20].</p>
        <p>In this paper, a novel module is proposed by combining the inception module with dense connection and construct the network architecture based on the U-Net with the module. The rest of the paper is organized as follows: in Section 2, we detail the novel DIU-Net architecture and in Section 3 the performance evaluation is presented and compared with the standard U-Net, residual U-Net, FCN-8s [10] and SegNet [21]. Finally, Section 4 concludes the work.In this paper, a novel module is proposed by combining the inception module with dense connection and construct the network architecture based on the U-Net with the module. The rest of the paper is organized as follows: in Section 2, we detail the novel DIU-Net architecture and in Section 3 the performance evaluation is presented and compared with the standard U-Net, residual U-Net, FCN-8s [10] and SegNet [21]. Finally, Section 4 concludes the work.</p>
        <p>A modified residual inception module is proposed to be used in both the analysis path and the synthesis path. The main purpose is to aggregate feature maps from different branches of kernels of different sizes, which can make the network wider and capable of learning more features [21]. Moreover, the residual connections make the learning easier since a residual inception block learns a function with reference to the input feature maps, instead of learning an unreferenced function [22]. Fig. 2 shows the proposed modified Inception-Res block. Different from the original Inception-Res architecture, each convolutional layer is followed by a batch normalization (BN) layer except for bottleneck layers. Batch normalization layer can avoid gradient vanishing while retaining convolutional layers. size convolution kernel represents the bottleneck layer [23]. Thus, the output of the module from 1 √ó 1 the analysis path is as follows:A modified residual inception module is proposed to be used in both the analysis path and the synthesis path. The main purpose is to aggregate feature maps from different branches of kernels of different sizes, which can make the network wider and capable of learning more features [21]. Moreover, the residual connections make the learning easier since a residual inception block learns a function with reference to the input feature maps, instead of learning an unreferenced function [22]. Fig. 2 shows the proposed modified Inception-Res block. Different from the original Inception-Res architecture, each convolutional layer is followed by a batch normalization (BN) layer except for bottleneck layers. Batch normalization layer can avoid gradient vanishing while retaining convolutional layers. size convolution kernel represents the bottleneck layer [23]. Thus, the output of the module from 1 √ó 1 the analysis path is as follows:</p>
        <p>(1)(1)</p>
        <p>In the Dense-Inception block, the inception module proposed above is inserted into the dense connection block. By setting the padding as "Same" mode in the convolutional layer, the outputs of the inception module will remain the same size of feature map as the inputs. Thus, the inception module can be regarded as a much wider convolutional layer since it is an aggregate of different kernel sizes with pooling layers [22]. The main purpose is to fit the inception module into the dense connection architecture and therefore make the network simultaneously deeper and wider without gradient vanishing or redundant computation. The inception module with residual connection in the dense connection block is different from the standard residual inception module as the batch normalization layer is also used after each convolutional layer. The dense connection's main purpose is to make the network deeper by concatenating former convolution outputs but narrower with the small hyper-parameter of growth rate. Therefore, in order to fit the residual inception module in the dense connection block, the dense connection module is modified to improve the performance in the dense connection block. In this way, the Dense-Inception blocks are set in the middle of the network model, where the size of the feature map is small, and a large kernel can be replaced by two smaller kernels to reduce computational cost [24]. Formally, let's assume ùëî ùëõ √ó ùëõ ( ‚ãÖ ) denotes a kernel convolutional layer, denotes the BN layer, and denotes theIn the Dense-Inception block, the inception module proposed above is inserted into the dense connection block. By setting the padding as "Same" mode in the convolutional layer, the outputs of the inception module will remain the same size of feature map as the inputs. Thus, the inception module can be regarded as a much wider convolutional layer since it is an aggregate of different kernel sizes with pooling layers [22]. The main purpose is to fit the inception module into the dense connection architecture and therefore make the network simultaneously deeper and wider without gradient vanishing or redundant computation. The inception module with residual connection in the dense connection block is different from the standard residual inception module as the batch normalization layer is also used after each convolutional layer. The dense connection's main purpose is to make the network deeper by concatenating former convolution outputs but narrower with the small hyper-parameter of growth rate. Therefore, in order to fit the residual inception module in the dense connection block, the dense connection module is modified to improve the performance in the dense connection block. In this way, the Dense-Inception blocks are set in the middle of the network model, where the size of the feature map is small, and a large kernel can be replaced by two smaller kernels to reduce computational cost [24]. Formally, let's assume ùëî ùëõ √ó ùëõ ( ‚ãÖ ) denotes a kernel convolutional layer, denotes the BN layer, and denotes the</p>
        <p>Max-pooling layer. The sign represents concatenation and represents the function of ‚àò ùëì ùêºùëÖ ( ‚ãÖ )Max-pooling layer. The sign represents concatenation and represents the function of ‚àò ùëì ùêºùëÖ ( ‚ãÖ )</p>
        <p>Bottleneck layer followed by the proposed Inception-Res module. Thus, the output of the proposed Inception-Res module is:Bottleneck layer followed by the proposed Inception-Res module. Thus, the output of the proposed Inception-Res module is:</p>
        <p>(2)(2)</p>
        <p>x g g x g g p x g g g g g x xx g g x g g p x g g g g g x x</p>
        <p>The output of the layer in the Dense-Inception block is as follows:The output of the layer in the Dense-Inception block is as follows:</p>
        <p>Where refers to the concatenation of the feature maps produced in the layers 0, 1, ‚Ä¶, . [ùë• 0 ,ùë• 1 ,‚Ä¶,ùë• ùëô ] ùëô Three Dense-Inception blocks are designed in total: one block is set in the analysis path, one is in the synthesis path, and the last one is set in the middle of network. Each Dense-Inception block except the middle one contains 12 proposed Inception-Res modules, and the middle one has 24 Inception-Res modules. The growth rate is used as the channel input of the residual inception module. Due to the concatenation connection, the size of the feature map will not get changed [25].Where refers to the concatenation of the feature maps produced in the layers 0, 1, ‚Ä¶, . [ùë• 0 ,ùë• 1 ,‚Ä¶,ùë• ùëô ] ùëô Three Dense-Inception blocks are designed in total: one block is set in the analysis path, one is in the synthesis path, and the last one is set in the middle of network. Each Dense-Inception block except the middle one contains 12 proposed Inception-Res modules, and the middle one has 24 Inception-Res modules. The growth rate is used as the channel input of the residual inception module. Due to the concatenation connection, the size of the feature map will not get changed [25].</p>
        <p>The down-sample block and up-sample block are illustrated in Fig. 5 (left) and Fig. 5 (right) respectively.The down-sample block and up-sample block are illustrated in Fig. 5 (left) and Fig. 5 (right) respectively.</p>
        <p>These two blocks have the same architecture except for the convolution and Max-pooling layers in the down-sample block and the deconvolution and up-sampling layers in the up-sample block. They can be viewed as a simplified inception module with three branches. Compared with the standard U-Net architecture, Max-pooling and the Upsampling2D layer are used to reduce and enlarge the size of feature maps respectively, which may lead to feature loss and reduce the accuracy of results. Thus, the main purpose of the proposed down-sampling and up-sampling blocks is to overcome this problem and avoid feature loss.These two blocks have the same architecture except for the convolution and Max-pooling layers in the down-sample block and the deconvolution and up-sampling layers in the up-sample block. They can be viewed as a simplified inception module with three branches. Compared with the standard U-Net architecture, Max-pooling and the Upsampling2D layer are used to reduce and enlarge the size of feature maps respectively, which may lead to feature loss and reduce the accuracy of results. Thus, the main purpose of the proposed down-sampling and up-sampling blocks is to overcome this problem and avoid feature loss.</p>
        <p>Fig. 5 The down-sample block (Left) and the up-sample block (Right)Fig. 5 The down-sample block (Left) and the up-sample block (Right)</p>
        <p>Formally, let denote the convolution layer with 2 strides, denotes the convolution ùëî 2 ùëõ √ó ùëõ ( ‚ãÖ ) ‚Ñé 2 ùëõ √ó ùëõ ( ‚ãÖ ) transposed layer with 2 strides, denotes the max-pooling layer with 2 strides andFormally, let denote the convolution layer with 2 strides, denotes the convolution ùëî 2 ùëõ √ó ùëõ ( ‚ãÖ ) ‚Ñé 2 ùëõ √ó ùëõ ( ‚ãÖ ) transposed layer with 2 strides, denotes the max-pooling layer with 2 strides and</p>
        <p>represents the up-sampling layer with 2 strides. The expression for the down-sample block is:represents the up-sampling layer with 2 strides. The expression for the down-sample block is:</p>
        <p>(4)(4)</p>
        <p>and the up-sample block is:and the up-sample block is:</p>
        <p>(5)(5)</p>
        <p>The Whole network model is designed followed by the concept of Encoding-Decoding architecture with skip connections. The encoding process corresponds the analysis path and the decoding process corresponds the synthesis path. In the encoding process, when the training images are transmitted into the model as the inputs, the channel number will increase by double after each Inception-Res block or Dense-Inception block; and the size of feature maps will reduce by half after down-sampling block. In the decoding process, the channel of feature maps will reduce by half after each Inception-Res block or Dense-Inception block; and the size of feature maps will increase by double after up-sampling block.The Whole network model is designed followed by the concept of Encoding-Decoding architecture with skip connections. The encoding process corresponds the analysis path and the decoding process corresponds the synthesis path. In the encoding process, when the training images are transmitted into the model as the inputs, the channel number will increase by double after each Inception-Res block or Dense-Inception block; and the size of feature maps will reduce by half after down-sampling block. In the decoding process, the channel of feature maps will reduce by half after each Inception-Res block or Dense-Inception block; and the size of feature maps will increase by double after up-sampling block.</p>
        <p>And the training images will return to original as outputs size in final.And the training images will return to original as outputs size in final.</p>
        <p>In order to demonstrate the performance of the DIU-Net model, it is evaluated using three different medical image segmentation tasks. We use Keras programing language, based on 
            <rs type="software">TensorFlow</rs> to demonstrate the proposed model on a Core i7 7700 processor and 16 GB RAM with a NVIDIA TITAN XP GPU.
        </p>
        <p>The datasets contain three different segmentation tasks, including lung segmentation in CT datasets, blood vessel segmentation and MRI brain tumor segmentation task. The lung segmentation dataset is from the "Finding and Measuring Lungs in CT Data" competition in the Kaggle Data Science Bowl in 2017. It is a collection of CT images, manually segmented lung and measurements in 2/3D. In the experiments only 2D images will be used and measured. The blood vessel segmentation task contains three different datasets including the DRIVE [26], 
            <rs type="software">STARE</rs> [27] and CHASH_DB1 [28]. In order to enhance the generality of model, these three datasets are randomly merged to create a new dataset which will be used in the proposed model. The MRI brain tumor segmentation dataset was acquired on a 3T scanner at the UMC Utrecht and contain 88 subjects including patients with diabetes, dementia, Alzheimer and matched controls [20]. To fit the dataset into the network each subject was sliced into 2D images based on axial direction. Example image data from these three datasets are shown in Fig. 6. In this figure, the first row is from lungs dataset, the last row is from brain dataset, and the remaining rows are from the retina dataset. The left column shows the original images, the right column shows the ground truth.
        </p>
        <p>Fig. 6 The lung segmentation datasets, the retina blood vessel segmentation and the brain tumor segmentation datasetsFig. 6 The lung segmentation datasets, the retina blood vessel segmentation and the brain tumor segmentation datasets</p>
        <p>For quantitative performance evaluation, several evaluation measures will be used as follows: DICE coefficient (DC) [29], Jaccard similarity (JS) [30], accuracy (AC), sensitivity, specificity (SP) [31], F1score [32], area under curve (AUC) [33] and the 95% confidence interval of AUC. In order to define these measures, we also use the variables True Positive (TP), True Negative (TN), False Positive (FP),For quantitative performance evaluation, several evaluation measures will be used as follows: DICE coefficient (DC) [29], Jaccard similarity (JS) [30], accuracy (AC), sensitivity, specificity (SP) [31], F1score [32], area under curve (AUC) [33] and the 95% confidence interval of AUC. In order to define these measures, we also use the variables True Positive (TP), True Negative (TN), False Positive (FP),</p>
        <p>False Negative (FN), Ground Truth (GT) and Segmentation Result (SR). The GT stands for the segmented region, and all the imaging datasets have been segmented manually by experienced experts, following the standard annotation protocol. These GT contours are references for further segmentation analysis [34]. The SR stands for the segmentation result from the method to be evaluated. The TP represents the pixels exist in both Ground Truth and proposed segmented region. The TN represents the pixels doesn't exist in neither Ground Truth nor segmented region. The FP represents the pixels exit in segmented region only. The FN represents the pixels exit in Ground Truth only. These expressions areFalse Negative (FN), Ground Truth (GT) and Segmentation Result (SR). The GT stands for the segmented region, and all the imaging datasets have been segmented manually by experienced experts, following the standard annotation protocol. These GT contours are references for further segmentation analysis [34]. The SR stands for the segmentation result from the method to be evaluated. The TP represents the pixels exist in both Ground Truth and proposed segmented region. The TN represents the pixels doesn't exist in neither Ground Truth nor segmented region. The FP represents the pixels exit in segmented region only. The FN represents the pixels exit in Ground Truth only. These expressions are</p>
        <p>shown in Table 1. The area under the curve (AUC) is the size of area under the Receiver Operating Characteristic (ROC); AUC intuitively reflects the classification ability of ROC curve expression. The 95% CI for AUC represents the 95% confidence interval of area under curve.shown in Table 1. The area under the curve (AUC) is the size of area under the Receiver Operating Characteristic (ROC); AUC intuitively reflects the classification ability of ROC curve expression. The 95% CI for AUC represents the 95% confidence interval of area under curve.</p>
        <p>Table 1 Expressions of different evaluation metricsTable 1 Expressions of different evaluation metrics</p>
        <p>In the proposed model, every convolution/deconvolution layer is followed by a batch normalization layer except the bottleneck layer or the last layer. The ReLU function [35] will be used as the activation function. In terms of the loss function, because of the binary labels in the segmentation tasks, a crossentropy function will be used in this model. The Adam [36] method will be used as the optimization algorithm. During the experiment stage, various kinds of optimization algorithms were considered. The Adam optimization algorithm is a combination of algorithm Momentum and Adagrad. In training experiments, it had better performance compared with other algorithms. The parameters of the model will be initialized with the He_normal initializer [37] which can make the initial parameters of network much easier to trian compared with other initializers. In these experiments, the Adam algorithm will get employed and the initial learning rate will be set to , = 0.900 and = 0.999. The 1 √ó 10 -5 beta_1 beta_2 network won't get converged if the learning rate is too large or too small. The batch size of training and validating datasets is 8. In each epoch 8 samples will be transposed into the network as the input, and too large batch size will slow down the training or validating speed. The model requires a total of 120 epochs to train with 300 steps per epoch.In the proposed model, every convolution/deconvolution layer is followed by a batch normalization layer except the bottleneck layer or the last layer. The ReLU function [35] will be used as the activation function. In terms of the loss function, because of the binary labels in the segmentation tasks, a crossentropy function will be used in this model. The Adam [36] method will be used as the optimization algorithm. During the experiment stage, various kinds of optimization algorithms were considered. The Adam optimization algorithm is a combination of algorithm Momentum and Adagrad. In training experiments, it had better performance compared with other algorithms. The parameters of the model will be initialized with the He_normal initializer [37] which can make the initial parameters of network much easier to trian compared with other initializers. In these experiments, the Adam algorithm will get employed and the initial learning rate will be set to , = 0.900 and = 0.999. The 1 √ó 10 -5 beta_1 beta_2 network won't get converged if the learning rate is too large or too small. The batch size of training and validating datasets is 8. In each epoch 8 samples will be transposed into the network as the input, and too large batch size will slow down the training or validating speed. The model requires a total of 120 epochs to train with 300 steps per epoch.</p>
        <p>In order to make a better comparison with the proposed model, another DIU-Net called DIU-Net-1 is also designed with only one Dense-Inception block in the middle of the network. To evaluate the proposed model, the results of the experiment will be compared with the SegNet, FCN-8s, U-Net and ResU-Net [8].In order to make a better comparison with the proposed model, another DIU-Net called DIU-Net-1 is also designed with only one Dense-Inception block in the middle of the network. To evaluate the proposed model, the results of the experiment will be compared with the SegNet, FCN-8s, U-Net and ResU-Net [8].</p>
        <p>The provided lung dataset consists of 267 images and the same number of labels. In this experiment, 90%The provided lung dataset consists of 267 images and the same number of labels. In this experiment, 90%</p>
        <p>of the data will be used for training and validating, and the remaining data will be used for testing. In order to get a reliable and stable model, 5-fold cross-validation will be used in this experiment. The size of each original grayscale image is pixels and will be resized to after image 512 √ó 512 256 √ó 256 2 summarizes the segmentation performance. From Table 2 it shows that 
            <rs type="software">DIU-Net</rs> network shows better performance under each evaluation index. DICE coefficient is the most direct evaluation index of segmentation accuracy. In this experiment, the average DICE coefficient of 
            <rs type="software">DIU-Net</rs> is 0.9857, and it is 0.233% higher than the second segmentation performance from 
            <rs type="software">ResU-</rs>Net's results; And in terms of Jaccard similarity, AC, SE, SP, F1-score and AUC coefficient, the results from 
            <rs type="software">DIU-Net</rs> are all higher than second segmentation performance as follows: 0.9824%, 0.24%, 1.87%, 0.01%, 0.60% and 0.84%.
        </p>
        <p>Thus, with the help of dense connections and inception layers, DIU-Net can be deeper and more effective than the original networks. In order to make the experiment more convincing, the experiments of SegNet, FCN-8s, U-Net and ResU-Net used the same initial method, optimization algorithm, loss function and other initial parameters with 
            <rs type="software">DIU-Net</rs>. The 
            <rs type="software">DIU-</rs>Net with three Dense-Inception modules has better segmentation performance than with just one Dense-Inception block. The results show that the module can effectively deepen the depth of the network without introducing gradient disappearance, so that the network can learn more image features.
        </p>
        <p>Table 2 Experimental results of proposed approaches for lung segmentation and comparison against other networks (The bold font is the best value for each column)Table 2 Experimental results of proposed approaches for lung segmentation and comparison against other networks (The bold font is the best value for each column)</p>
        <p>The original images to be used for blood vessel segmentation are RGB images, and they can provide the clearest features of blood vessel under the green channel. Thus, preprocessing via normalization is necessary before conducting the segmentation experiment. The whole dataset contains only 136 samples of RGB images including labels. Among the samples, 85% of the data will be used for training and validating, and the remaining 15% data will be used for testing. In order to get a reliable and stable model, In terms of quantitative analysis, Table 3 shows the detail of the evaluation results. As the blood vessel data contains three different kinds of datasets including the DRIVE, STARE and CHASH_DB1, the results of traditional methods are calculated by averaging. From Table 3 it can be concluded that the DIU-Net model shows better performance in terms of the DICE coefficient, Jaccard similarity, accuracy, F1-score and the sensitivity. In particular, the DICE coefficient is 0.15% higher than the results of ResU-Net. In terms of Jaccard similarity, AC, SE, SP, F1-score and AUC coefficient, the results from DIU-Net are all higher than second segmentation performance as follows: 0.31%, 0.15%, 2.84%, 0.09%, 2.11% and 0.60%. Compared with the traditional methods [33,35], the DIU-Net also achieves better performance. Therefore, the results demonstrate the effectiveness of the proposed method for medical image segmentation tasks. In particular, the blood vessel segmentation task is more complicated than the lung segmentation task, and the 
            <rs type="software">DIU-</rs>Net with deeper architecture performs better in this task than other networks. The Dense-Inception blocks make the network much deeper and wider in order to gain increased performance.
        </p>
        <p>Table 3 Experimental results of proposed approaches for retina blood vessel and comparison against other networks or traditional methods (The bold font is the best value for each column)Table 3 Experimental results of proposed approaches for retina blood vessel and comparison against other networks or traditional methods (The bold font is the best value for each column)</p>
        <p>The provided MRI scans brain tumor segmentation dataset consists of 88 subjects, and each subject was sliced into 2D images based on axial direction. After deleting the data without label pixels there are 4937 images and the same number of labels in total. In this experiment, 90% of the data will be used for training and validating, and the remaining data will be used for testing. In order to get a reliable and stable model, 5-fold cross-validation will be used in this experiment. The first column shows the original image, the second column shows the ground truth, and the remaining columns show the outputs as follows: SegNet, FCN-8s, U-Net, ResU-Net, DIU-Net-1 and DIU-Net. Fig. 17 shows the training accuracy of each of these models, and Fig. 18 shows the corresponding training loss curves. Fig. 19 shows the validating accuracy curve of each model and Fig. 20 shows the validating loss curve of each model. The experimental outputs from the models using 3 images from the brain tumor segmentation dataset are shown in Fig. 21. From the training curve we can determine that all the networks can get the level of accuracy around 0.9800, and the training loss can reduce to about 0.0100. In addition, the validation accuracy in Fig. 19 demonstrates that all the networks can get the level of accuracy around 0.9700. In Fig. 20, the validating loss curve shows the validating loss can reduce under 0.1000. Testing results from Fig. 21 shows that the results of DIU-Net have better segmentation performance than others.The provided MRI scans brain tumor segmentation dataset consists of 88 subjects, and each subject was sliced into 2D images based on axial direction. After deleting the data without label pixels there are 4937 images and the same number of labels in total. In this experiment, 90% of the data will be used for training and validating, and the remaining data will be used for testing. In order to get a reliable and stable model, 5-fold cross-validation will be used in this experiment. The first column shows the original image, the second column shows the ground truth, and the remaining columns show the outputs as follows: SegNet, FCN-8s, U-Net, ResU-Net, DIU-Net-1 and DIU-Net. Fig. 17 shows the training accuracy of each of these models, and Fig. 18 shows the corresponding training loss curves. Fig. 19 shows the validating accuracy curve of each model and Fig. 20 shows the validating loss curve of each model. The experimental outputs from the models using 3 images from the brain tumor segmentation dataset are shown in Fig. 21. From the training curve we can determine that all the networks can get the level of accuracy around 0.9800, and the training loss can reduce to about 0.0100. In addition, the validation accuracy in Fig. 19 demonstrates that all the networks can get the level of accuracy around 0.9700. In Fig. 20, the validating loss curve shows the validating loss can reduce under 0.1000. Testing results from Fig. 21 shows that the results of DIU-Net have better segmentation performance than others.</p>
        <p>In terms of quantitative analysis, Table 4 summarizes the segmentation performance. From Table 2 it shows that DIU-Net network shows better performance under each evaluation index. In this experiment, the average DICE coefficient of DIU-Net is 0.9892, and the average Jaccard similarity, AC, SE, SP and F1-score of DIU-Net are as follows 0.9940, 0.9970, 0.9284, 0.9986 and 0.9085. In terms of the AUC coefficient and its 95% confidence interval, the average AUC of DIU-Net is 0.9867 and the 95% CI is (0.9847-0.9889), which is 2.57% higher than second segmentation performance. These results verify the feasibility and effectiveness of DIU-Net.In terms of quantitative analysis, Table 4 summarizes the segmentation performance. From Table 2 it shows that DIU-Net network shows better performance under each evaluation index. In this experiment, the average DICE coefficient of DIU-Net is 0.9892, and the average Jaccard similarity, AC, SE, SP and F1-score of DIU-Net are as follows 0.9940, 0.9970, 0.9284, 0.9986 and 0.9085. In terms of the AUC coefficient and its 95% confidence interval, the average AUC of DIU-Net is 0.9867 and the 95% CI is (0.9847-0.9889), which is 2.57% higher than second segmentation performance. These results verify the feasibility and effectiveness of DIU-Net.</p>
        <p>The overall training and testing time of all the models, in each experiment, are shown in Table 5. From Table 5 it can be seen that the proposed model needs more time to train and test than other networks, andThe overall training and testing time of all the models, in each experiment, are shown in Table 5. From Table 5 it can be seen that the proposed model needs more time to train and test than other networks, and</p>
        <p>This work was supported in part by the National Natural Science Foundation of China under Grant Nos.This work was supported in part by the National Natural Science Foundation of China under Grant Nos.</p>
        <p>U1713216 and the Shenyang Intelligent Robot Laboratory Fund of China under Grant 18-007-0-06.U1713216 and the Shenyang Intelligent Robot Laboratory Fund of China under Grant 18-007-0-06.</p>
        <p>SegNet, FCN-8s, U-Net and ResU-Net models. From the comparison between DIU-Net and DIU-Net-1, the Dense-Inception block does make the network deeper and wider and hence improves the gradient propagation. However, too many Dense-Inception blocks may significantly increase the computational burden and does not provide better results. In this experiment just three blocks are needed for the proposed network model. In a word, it can be concluded that the proposed DIU-Net is a promising approach for semantic medical image segmentation.SegNet, FCN-8s, U-Net and ResU-Net models. From the comparison between DIU-Net and DIU-Net-1, the Dense-Inception block does make the network deeper and wider and hence improves the gradient propagation. However, too many Dense-Inception blocks may significantly increase the computational burden and does not provide better results. In this experiment just three blocks are needed for the proposed network model. In a word, it can be concluded that the proposed DIU-Net is a promising approach for semantic medical image segmentation.</p>
        <p>The limitation of the proposed DIU-Net model is that in the Dense-Inception block increasing the growth rate may lead to too many parameters, making the model more difficult and slower to train. Another issue is that the test datasets did not include ultrasound images, which contain more speckle noise. Therefore, in the future we will aim to simplify the network structure whilst maintaining its accuracy and effectiveness. And we will also test the proposed model on ultrasound datasets and modify it to make the model more effective on various kinds of medical problems.The limitation of the proposed DIU-Net model is that in the Dense-Inception block increasing the growth rate may lead to too many parameters, making the model more difficult and slower to train. Another issue is that the test datasets did not include ultrasound images, which contain more speckle noise. Therefore, in the future we will aim to simplify the network structure whilst maintaining its accuracy and effectiveness. And we will also test the proposed model on ultrasound datasets and modify it to make the model more effective on various kinds of medical problems.</p>
        <p>None.None.</p>
    </text>
</tei>
