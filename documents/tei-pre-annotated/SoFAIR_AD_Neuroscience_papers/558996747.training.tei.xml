<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-14T14:10+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Transformers have dominated the field of natural language processing and have recently made an impact in the area of computer vision. In the field of medical image analysis, transformers have also been successfully used in to full-stack clinical applications, including image synthesis/reconstruction, registration, segmentation, detection, and diagnosis. This paper aimed to promote awareness of the applications of transformers in medical image analysis. Specifically, we first provided an overview of the core concepts of the attention mechanism built into transformers and other basic components. Second, we reviewed various transformer architectures tailored for medical image applications and discuss their limitations. Within this review, we investigated key challenges including the use of transformers in different learning paradigms, improving model efficiency, and coupling with other techniques. We hope this review would provide a comprehensive picture of transformers to readers with an interest in medical image analysis.Transformers have dominated the field of natural language processing and have recently made an impact in the area of computer vision. In the field of medical image analysis, transformers have also been successfully used in to full-stack clinical applications, including image synthesis/reconstruction, registration, segmentation, detection, and diagnosis. This paper aimed to promote awareness of the applications of transformers in medical image analysis. Specifically, we first provided an overview of the core concepts of the attention mechanism built into transformers and other basic components. Second, we reviewed various transformer architectures tailored for medical image applications and discuss their limitations. Within this review, we investigated key challenges including the use of transformers in different learning paradigms, improving model efficiency, and coupling with other techniques. We hope this review would provide a comprehensive picture of transformers to readers with an interest in medical image analysis.</p>
        <p>Transformers [1] have dominated the field of natural language processing (NLP), with applications in areas including speech recognition [2] , synthesis [3] , text to speech translation [4] , and natural language generation [5] . As an instance of deep learning architectures, the first transformer was introduced to handle sequential inference tasks in NLP. Whereas recurrent neural networks [6] ( e.g. , long short-term memory network [7] ) explicitly use a sequence of inference processes, transformers capture long-term dependencies of sequential data with stacked self-attention layers. Thus, transformers are both efficient, as they solve a sequential learning problem in one-shot, and effective, owing to the stacking of very deep models. Several transformer architectures trained on large-scale architectures have become popular for solving NLP tasks; these include Bidirectional Encoder Representations from Transformers, BERT [8] and GPT-3 [9][10] , to name just two.Transformers [1] have dominated the field of natural language processing (NLP), with applications in areas including speech recognition [2] , synthesis [3] , text to speech translation [4] , and natural language generation [5] . As an instance of deep learning architectures, the first transformer was introduced to handle sequential inference tasks in NLP. Whereas recurrent neural networks [6] ( e.g. , long short-term memory network [7] ) explicitly use a sequence of inference processes, transformers capture long-term dependencies of sequential data with stacked self-attention layers. Thus, transformers are both efficient, as they solve a sequential learning problem in one-shot, and effective, owing to the stacking of very deep models. Several transformer architectures trained on large-scale architectures have become popular for solving NLP tasks; these include Bidirectional Encoder Representations from Transformers, BERT [8] and GPT-3 [9][10] , to name just two.</p>
        <p>Convolutional neural networks (CNNs) and their variants have achieved state-of-the-art (SOTA) performance in several computer vi- other clinical purposes. For instance, the work described in [22][23] used transformers to distinguish COVID-19 from other types of pneumonia using computed tomography (CT) or X-ray images, meeting the urgent need to treat COVID-19 patients fast and effectively. Transformers have also been successfully applied to image segmentation [24] , detection [25] , and synthesis [26] , achieving SOTA results. Figure 1 displays the chronological adaptation of transformers to different medical image applications, which will be further discussed in Section 3 .Convolutional neural networks (CNNs) and their variants have achieved state-of-the-art (SOTA) performance in several computer vi- other clinical purposes. For instance, the work described in [22][23] used transformers to distinguish COVID-19 from other types of pneumonia using computed tomography (CT) or X-ray images, meeting the urgent need to treat COVID-19 patients fast and effectively. Transformers have also been successfully applied to image segmentation [24] , detection [25] , and synthesis [26] , achieving SOTA results. Figure 1 displays the chronological adaptation of transformers to different medical image applications, which will be further discussed in Section 3 .</p>
        <p>Although many studies have been devoted to customizing transformers for medical image analysis tasks, this customization raised new challenges that remain unsolved. To encourage and facilitate the development of transformer-based applications in medical image analysis, we extensively review more than 170 existing transformer-based methods in the field, providing solutions for medical applications, and showing how transformers have been adopted in various clinical settings. Moreover, we present in-depth discussions on the design of transformerbased methods to solve complex real-world tasks, including weaklysupervised/multi-task/multi-modal learning paradigms. This paper also includes comparisons between transformers and CNNs and discusses new ways of improving the efficiency and interpretation of transformer networks.Although many studies have been devoted to customizing transformers for medical image analysis tasks, this customization raised new challenges that remain unsolved. To encourage and facilitate the development of transformer-based applications in medical image analysis, we extensively review more than 170 existing transformer-based methods in the field, providing solutions for medical applications, and showing how transformers have been adopted in various clinical settings. Moreover, we present in-depth discussions on the design of transformerbased methods to solve complex real-world tasks, including weaklysupervised/multi-task/multi-modal learning paradigms. This paper also includes comparisons between transformers and CNNs and discusses new ways of improving the efficiency and interpretation of transformer networks.</p>
        <p>The remainder of the paper is organized as follows. Section 2 introduces the preliminaries of transformers and their development in vision. Section 3 reviews recent applications of transformers in medical image analysis, and Section 4 discusses the potential future directions of transformers. Section 5 concludes the paper.The remainder of the paper is organized as follows. Section 2 introduces the preliminaries of transformers and their development in vision. Section 3 reviews recent applications of transformers in medical image analysis, and Section 4 discusses the potential future directions of transformers. Section 5 concludes the paper.</p>
        <p>A typical transformer leverages the attention mechanism in neural networks. Hence, we start by introducing the core principle of the attention mechanism, followed by a detailed description of how the transformer works.A typical transformer leverages the attention mechanism in neural networks. Hence, we start by introducing the core principle of the attention mechanism, followed by a detailed description of how the transformer works.</p>
        <p>For information exploration, human beings usually leverage their "attention mechanism " to filter out irrelevant information while focusing on the meaningful parts of the data encountered in daily life. Inspired by this observation, researchers have designed attention mechanisms for deep learning that sift through homogeneous data while paying attention to the most significant components or elements.For information exploration, human beings usually leverage their "attention mechanism " to filter out irrelevant information while focusing on the meaningful parts of the data encountered in daily life. Inspired by this observation, researchers have designed attention mechanisms for deep learning that sift through homogeneous data while paying attention to the most significant components or elements.</p>
        <p>Bahdanau attention. An attention mechanism was initially proposed in [27] for a language translation task, namely Bahdanau attention. This attention mechanism is calculated as the weighted sum of all annotations ( i.e. , the results of each input generated by the encoder) and the previous decoder.Bahdanau attention. An attention mechanism was initially proposed in [27] for a language translation task, namely Bahdanau attention. This attention mechanism is calculated as the weighted sum of all annotations ( i.e. , the results of each input generated by the encoder) and the previous decoder.</p>
        <p>Similar concepts have been developed in the field of CV. For example, Hu et al. [28] introduced a novel attention mechanism, i.e., Squeeze-and-Excitation , to execute feature re-calibration , in which informative features for a particular visual task are emphasized, and the remaining features are regarded as less important.Similar concepts have been developed in the field of CV. For example, Hu et al. [28] introduced a novel attention mechanism, i.e., Squeeze-and-Excitation , to execute feature re-calibration , in which informative features for a particular visual task are emphasized, and the remaining features are regarded as less important.</p>
        <p>Self-attention. In [1] , the attention mechanism was re-defined as a function working with queries, keys, and values derived from the input vectors of the module, in contrast to Bahdanau attention. The output is defined as a weighted sum of values, where the weight of each value is calculated as the attention between queries and keys.Self-attention. In [1] , the attention mechanism was re-defined as a function working with queries, keys, and values derived from the input vectors of the module, in contrast to Bahdanau attention. The output is defined as a weighted sum of values, where the weight of each value is calculated as the attention between queries and keys.</p>
        <p>The self-attention operation is usually performed in matrix form to accelerate calculation in parallel. To briefly illustrate the concept of self-attention, we first describe it in an element-wise form.The self-attention operation is usually performed in matrix form to accelerate calculation in parallel. To briefly illustrate the concept of self-attention, we first describe it in an element-wise form.</p>
        <p>For each input ğ‘¥ ğ‘– âˆˆ â„ ğ‘ , ğ‘– = 1 , .., ğ‘› , the corresponding query ğ‘ ğ‘– âˆˆ â„ ğ‘‘ ğ‘ , key ğ‘˜ ğ‘– âˆˆ â„ ğ‘‘ ğ‘˜ , and value ğ‘£ ğ‘– âˆˆ â„ ğ‘‘ ğ‘£ vectors are generated through the parameters ğ‘Š ğ‘ , ğ‘Š ğ‘˜ , and ğ‘Š ğ‘£ , respectively. ğ‘‘ ğ‘ , ğ‘‘ ğ‘˜ , ğ‘‘ ğ‘£ are the sizes of ğ‘ ğ‘– , ğ‘˜ ğ‘– , ğ‘£ ğ‘– and also the number of features that are learned from ğ‘¥ ğ‘– .For each input ğ‘¥ ğ‘– âˆˆ â„ ğ‘ , ğ‘– = 1 , .., ğ‘› , the corresponding query ğ‘ ğ‘– âˆˆ â„ ğ‘‘ ğ‘ , key ğ‘˜ ğ‘– âˆˆ â„ ğ‘‘ ğ‘˜ , and value ğ‘£ ğ‘– âˆˆ â„ ğ‘‘ ğ‘£ vectors are generated through the parameters ğ‘Š ğ‘ , ğ‘Š ğ‘˜ , and ğ‘Š ğ‘£ , respectively. ğ‘‘ ğ‘ , ğ‘‘ ğ‘˜ , ğ‘‘ ğ‘£ are the sizes of ğ‘ ğ‘– , ğ‘˜ ğ‘– , ğ‘£ ğ‘– and also the number of features that are learned from ğ‘¥ ğ‘– .</p>
        <p>ğ‘ ğ‘– = ğ‘¥ ğ‘– Ã— ğ‘Š ğ‘ , ğ‘Š ğ‘ âˆˆ â„ ğ‘Ã—ğ‘‘ ğ‘ , ğ‘˜ ğ‘– = ğ‘¥ ğ‘– Ã— ğ‘Š ğ‘˜ , ğ‘Š ğ‘˜ âˆˆ â„ ğ‘Ã—ğ‘‘ ğ‘˜ , ğ‘£ ğ‘– = ğ‘¥ ğ‘– Ã— ğ‘Š ğ‘£ , ğ‘Š ğ‘£ âˆˆ â„ ğ‘Ã—ğ‘‘ ğ‘£ , ğ‘‘ ğ‘ = ğ‘‘ ğ‘˜ .ğ‘ ğ‘– = ğ‘¥ ğ‘– Ã— ğ‘Š ğ‘ , ğ‘Š ğ‘ âˆˆ â„ ğ‘Ã—ğ‘‘ ğ‘ , ğ‘˜ ğ‘– = ğ‘¥ ğ‘– Ã— ğ‘Š ğ‘˜ , ğ‘Š ğ‘˜ âˆˆ â„ ğ‘Ã—ğ‘‘ ğ‘˜ , ğ‘£ ğ‘– = ğ‘¥ ğ‘– Ã— ğ‘Š ğ‘£ , ğ‘Š ğ‘£ âˆˆ â„ ğ‘Ã—ğ‘‘ ğ‘£ , ğ‘‘ ğ‘ = ğ‘‘ ğ‘˜ .</p>
        <p>(1)(1)</p>
        <p>The output is also a probability calculated as the weighted sum of the calculated weighting values:The output is also a probability calculated as the weighted sum of the calculated weighting values:</p>
        <p>where ğ›¼ â€² ğ‘–ğ‘— measures the contribution of the ğ‘— ğ‘¡â„ element of the input to the ğ‘– ğ‘¡â„ element of the output. Through this operation, ğ›¼ â€² ğ‘–ğ‘— can be regarded as the attention assigned to the element ğ‘£ ğ‘– . Thereby the final output attentions can be computed as a weighted sum of all values as follows:where ğ›¼ â€² ğ‘–ğ‘— measures the contribution of the ğ‘— ğ‘¡â„ element of the input to the ğ‘– ğ‘¡â„ element of the output. Through this operation, ğ›¼ â€² ğ‘–ğ‘— can be regarded as the attention assigned to the element ğ‘£ ğ‘– . Thereby the final output attentions can be computed as a weighted sum of all values as follows:</p>
        <p>The element-wise self-attention can be feasibly extended to matrices. In most cases, the query ğ‘ ğ‘– , key ğ‘˜ ğ‘– and value ğ‘£ ğ‘– for each input ğ‘¥ ğ‘– are generated using parallel matrix computation. ğ‘¥ ğ‘– , ğ‘ ğ‘– , ğ‘˜ ğ‘– , ğ‘£ ğ‘– can be stacked together to matrices, respectively. Let ğ‘‹ âˆˆ â„ ğ‘  Ã—ğ‘ denote the input matrix, ğ‘„ denote the query matrix, ğ¾ denote the key matrix, and ğ‘‰ denote the value matrix, where ğ‘  is the number of the samples and each matrix is consisted of the elements, i.e. , ğ‘‹ = [ ğ‘¥ 1 ; ğ‘¥ 2 ; â‹¯ ; ğ‘¥ ğ‘  ] ğ‘‡ . Similarly, we compute the attention matrix ğ´ and output matrix ğ‘ as follows:The element-wise self-attention can be feasibly extended to matrices. In most cases, the query ğ‘ ğ‘– , key ğ‘˜ ğ‘– and value ğ‘£ ğ‘– for each input ğ‘¥ ğ‘– are generated using parallel matrix computation. ğ‘¥ ğ‘– , ğ‘ ğ‘– , ğ‘˜ ğ‘– , ğ‘£ ğ‘– can be stacked together to matrices, respectively. Let ğ‘‹ âˆˆ â„ ğ‘  Ã—ğ‘ denote the input matrix, ğ‘„ denote the query matrix, ğ¾ denote the key matrix, and ğ‘‰ denote the value matrix, where ğ‘  is the number of the samples and each matrix is consisted of the elements, i.e. , ğ‘‹ = [ ğ‘¥ 1 ; ğ‘¥ 2 ; â‹¯ ; ğ‘¥ ğ‘  ] ğ‘‡ . Similarly, we compute the attention matrix ğ´ and output matrix ğ‘ as follows:</p>
        <p>Figure 2. A brief illustration of a self-attention mechanism.Figure 2. A brief illustration of a self-attention mechanism.</p>
        <p>Multi-head self-attention. It was shown in [1] that applying multiple self-attentions to the same input could better capture hierarchical features. These self-attention layers work similarly to multiple kernels in convolution layers. Given â„ self-attentions (heads), the module outputs the final result by concatenating the calculated attentions:Multi-head self-attention. It was shown in [1] that applying multiple self-attentions to the same input could better capture hierarchical features. These self-attention layers work similarly to multiple kernels in convolution layers. Given â„ self-attentions (heads), the module outputs the final result by concatenating the calculated attentions:</p>
        <p>where ğ‘Š ğ‘„ ğ‘– , ğ‘Š ğ½ ğ‘– , ğ‘Š ğ‘‰ ğ‘– denote linear projection matrices that map matrices ğ‘„, ğ¾, ğ‘‰ into different subspaces, respectively. ğ‘Š ğ‘‚ is an output projection matrix that concatenates self-attention outputs of all attention heads ( Figure 2 ).where ğ‘Š ğ‘„ ğ‘– , ğ‘Š ğ½ ğ‘– , ğ‘Š ğ‘‰ ğ‘– denote linear projection matrices that map matrices ğ‘„, ğ¾, ğ‘‰ into different subspaces, respectively. ğ‘Š ğ‘‚ is an output projection matrix that concatenates self-attention outputs of all attention heads ( Figure 2 ).</p>
        <p>In [1] , the authors proposed a typical transformer network with an encoder-decoder structure. The encoder maps an input sequence { ğ‘¥ 1 , â€¦ , ğ‘¥ ğ‘› } to an output sequence { ğ‘§ 1 , â‹¯ , ğ‘§ ğ‘› } of the same length. The decoder generates the output { ğ‘¦ 1 , â‹¯ , ğ‘¦ ğ‘š } from the encoded representation ğ‘§ in an element-wise manner and takes the previous output as an additional input. A typical transformer architecture is shown in Figure 3 and described below.In [1] , the authors proposed a typical transformer network with an encoder-decoder structure. The encoder maps an input sequence { ğ‘¥ 1 , â€¦ , ğ‘¥ ğ‘› } to an output sequence { ğ‘§ 1 , â‹¯ , ğ‘§ ğ‘› } of the same length. The decoder generates the output { ğ‘¦ 1 , â‹¯ , ğ‘¦ ğ‘š } from the encoded representation ğ‘§ in an element-wise manner and takes the previous output as an additional input. A typical transformer architecture is shown in Figure 3 and described below.</p>
        <p>The encoder in a typical transformer has ğ‘› = 6 stacked blocks consisting of two types of layers, i.e. , the multi-head attention layer and the feed-forward layer. Residual connections and layer normalization layers are combined with the aforementioned layers. Concretely, in each block, the multi-head attention is first calculated, followed by a layerwise normalization, calculating the sum of the input and output of the multi-head attention. This is followed by a feed-forward layer, then a layer-wise normalization of the sum of the feed-forward layer's input and output.The encoder in a typical transformer has ğ‘› = 6 stacked blocks consisting of two types of layers, i.e. , the multi-head attention layer and the feed-forward layer. Residual connections and layer normalization layers are combined with the aforementioned layers. Concretely, in each block, the multi-head attention is first calculated, followed by a layerwise normalization, calculating the sum of the input and output of the multi-head attention. This is followed by a feed-forward layer, then a layer-wise normalization of the sum of the feed-forward layer's input and output.</p>
        <p>The decoder also has ğ‘› = 6 blocks, similar to the encoder, with some minor modifications. Specifically, an additional self-attention layer is inserted on top of the encoded output. Masking is employed in the first self-attention layer to block subsequent contributions to the state of the previous position, as the prediction is based on a known state. A linear layer and a Softmax layer are inserted after the output of the decoder to generate the final output.The decoder also has ğ‘› = 6 blocks, similar to the encoder, with some minor modifications. Specifically, an additional self-attention layer is inserted on top of the encoded output. Masking is employed in the first self-attention layer to block subsequent contributions to the state of the previous position, as the prediction is based on a known state. A linear layer and a Softmax layer are inserted after the output of the decoder to generate the final output.</p>
        <p>The success of transformers in NLP propagated to the CV research community, where several efforts have been made to adapt transformers to vision tasks. Transformer-based models in vision have been developed at an unprecedented pace; the most representative such models are detection transformer (DETR) [14] , ViT [13] , data-efficient image transformer (DeiT) [30] , and Swin-Transformer [31] .The success of transformers in NLP propagated to the CV research community, where several efforts have been made to adapt transformers to vision tasks. Transformer-based models in vision have been developed at an unprecedented pace; the most representative such models are detection transformer (DETR) [14] , ViT [13] , data-efficient image transformer (DeiT) [30] , and Swin-Transformer [31] .</p>
        <p>DETR. DETR, proposed by Carion et al. [14] , was the first application of transformers to a CV task, specifically the task of object detection. Unlike conventional object detection methods that involve hand-crafted processes, DETR is an end-to-end detection model that uses a transformer encoder to model the relation between image features extracted by a CNN backbone, a transformer decoder to generate object queries, and a feed-forward network to assign labels and bound the boxes around the objects.DETR. DETR, proposed by Carion et al. [14] , was the first application of transformers to a CV task, specifically the task of object detection. Unlike conventional object detection methods that involve hand-crafted processes, DETR is an end-to-end detection model that uses a transformer encoder to model the relation between image features extracted by a CNN backbone, a transformer decoder to generate object queries, and a feed-forward network to assign labels and bound the boxes around the objects.</p>
        <p>ViT. Following DETR, Dosovitskiy et al. [13] proposed the ViT, as shown in Figure 4 . ViT is an image classification model that adopts the basic architecture of the conventional transformer. In ViT, the input image is converted to a series of patches, each coupled with a positional encoding method that encodes the spatial positions of each patch to provide spatial information. The patches, along with a class token, are then fed into the transformer to calculate the MHSA and output the learned embeddings of patches. The state of the class token from the output of the ViT serves as the image representation. Last, a multi-layer perceptron (MLP) is used to classify the learned image representation. In addition to raw images, feature maps from CNNs can be fed into a ViT for relational mapping.ViT. Following DETR, Dosovitskiy et al. [13] proposed the ViT, as shown in Figure 4 . ViT is an image classification model that adopts the basic architecture of the conventional transformer. In ViT, the input image is converted to a series of patches, each coupled with a positional encoding method that encodes the spatial positions of each patch to provide spatial information. The patches, along with a class token, are then fed into the transformer to calculate the MHSA and output the learned embeddings of patches. The state of the class token from the output of the ViT serves as the image representation. Last, a multi-layer perceptron (MLP) is used to classify the learned image representation. In addition to raw images, feature maps from CNNs can be fed into a ViT for relational mapping.</p>
        <p>DeiT. In order to solve the problem of large-scale training data being required by ViT, Touvron et al. [30] proposed DeiT to ensure performance on small-scale data. They adopted a knowledge distillation framework with a teacher-student formulation and attached a distillation token (this is terminology for transformers) after the input sequence to learn from the output of the teacher model. In addition, they argued that using a CNN as the teacher model could facilitate training of the transformer as the student network to inherit inductive bias.DeiT. In order to solve the problem of large-scale training data being required by ViT, Touvron et al. [30] proposed DeiT to ensure performance on small-scale data. They adopted a knowledge distillation framework with a teacher-student formulation and attached a distillation token (this is terminology for transformers) after the input sequence to learn from the output of the teacher model. In addition, they argued that using a CNN as the teacher model could facilitate training of the transformer as the student network to inherit inductive bias.</p>
        <p>Swin-Transformer. To reduce the cost of calculating the attention of high-resolution images and deal with the varied patch sizes in sceneunderstanding tasks ( .e.g. , segmentation), Liu et al. [31] proposed the Swin-Transformer. They introduced a window self-attention to reduce the computational complexity and used the shifted window attention to model cross-window relationships. Moreover, they connected these attention blocks with patch merging blocks, which were used to merge neighboring patches to produce a hierarchical representation for handling variations in the scale of visual entities.Swin-Transformer. To reduce the cost of calculating the attention of high-resolution images and deal with the varied patch sizes in sceneunderstanding tasks ( .e.g. , segmentation), Liu et al. [31] proposed the Swin-Transformer. They introduced a window self-attention to reduce the computational complexity and used the shifted window attention to model cross-window relationships. Moreover, they connected these attention blocks with patch merging blocks, which were used to merge neighboring patches to produce a hierarchical representation for handling variations in the scale of visual entities.</p>
        <p>Recent studies have also validated MLP-based models and examined the effectiveness of attention mechanism, convolution, and other modules in CNNs or ViTs. Although CNNs and ViTs have been dominant for some time, the success of certain MLP-based models has had great repercussions. A representative example is MLP-Mixer, proposed by Tolstikhin et al. [32] in May 2021, which used a simple pure deep MLP architecture but showed competitive performance. MLP-Mixer uses per-patch flattening instead of the full flattening, and positional encoding and class token are not added to the patch sequence as in ViT. Following patch embedding learning, the Mixer MLP block is composed of a token-mixing MLP and a channel-mixing MLP, where the former is used to aggregate inter-patch features and the latter is used to integrate intra-patch features. The final class is predicted based on the features obtained following global average pooling.Recent studies have also validated MLP-based models and examined the effectiveness of attention mechanism, convolution, and other modules in CNNs or ViTs. Although CNNs and ViTs have been dominant for some time, the success of certain MLP-based models has had great repercussions. A representative example is MLP-Mixer, proposed by Tolstikhin et al. [32] in May 2021, which used a simple pure deep MLP architecture but showed competitive performance. MLP-Mixer uses per-patch flattening instead of the full flattening, and positional encoding and class token are not added to the patch sequence as in ViT. Following patch embedding learning, the Mixer MLP block is composed of a token-mixing MLP and a channel-mixing MLP, where the former is used to aggregate inter-patch features and the latter is used to integrate intra-patch features. The final class is predicted based on the features obtained following global average pooling.</p>
        <p>Simultaneously with or following MLP-Mixer, many other MLPbased models have been proposed, .e.g. , gMLP [33] , ResMLP [34] , ASMLP [35] , and CycleMLP [36] . MLP-Mixer not only inspired further exploration of MLP-based models but also led to further development of neural architectures in CV. As transformers, CNNs, and MLPs have shown competitive performance against each other, there is still no evidence as to which architecture is more suitable for particular CV learning tasks. In the case of medical image analysis, we provide a comparison of CNN and transformer models in part C of Section 4 .Simultaneously with or following MLP-Mixer, many other MLPbased models have been proposed, .e.g. , gMLP [33] , ResMLP [34] , ASMLP [35] , and CycleMLP [36] . MLP-Mixer not only inspired further exploration of MLP-based models but also led to further development of neural architectures in CV. As transformers, CNNs, and MLPs have shown competitive performance against each other, there is still no evidence as to which architecture is more suitable for particular CV learning tasks. In the case of medical image analysis, we provide a comparison of CNN and transformer models in part C of Section 4 .</p>
        <p>Transformers have been widely used in full-stack clinical applications. In this section, we first introduce transformer-based medical image analysis applications, including classification, segmentation, imageto-image translation, detection, registration, and video-based applica-Figure 4. Architecture of ViT, as proposed in [13] . Sequential image patches are used as the input and processed with the transformer encoder, and the class prediction is output by an MLP head. The transformer encoder is constructed using ğ‘ transformer blocks. tions. We categorize these applications according to their learning tasks as illustrated in Figure 5 .Transformers have been widely used in full-stack clinical applications. In this section, we first introduce transformer-based medical image analysis applications, including classification, segmentation, imageto-image translation, detection, registration, and video-based applica-Figure 4. Architecture of ViT, as proposed in [13] . Sequential image patches are used as the input and processed with the transformer encoder, and the class prediction is output by an MLP head. The transformer encoder is constructed using ğ‘ transformer blocks. tions. We categorize these applications according to their learning tasks as illustrated in Figure 5 .</p>
        <p>Methods using transformers for both disease diagnosis and prognosis are formulated as classification tasks, which can be divided into the following three categories:Methods using transformers for both disease diagnosis and prognosis are formulated as classification tasks, which can be divided into the following three categories:</p>
        <p>(1) applying ViTs directly to medical images;(1) applying ViTs directly to medical images;</p>
        <p>(2) combining ViTs with convolutions for more representative local feature learning;(2) combining ViTs with convolutions for more representative local feature learning;</p>
        <p>(3) combining ViTs with graph representations to better handle complex data.(3) combining ViTs with graph representations to better handle complex data.</p>
        <p>This section gives a comprehensive overview of the aforementioned three transformer categories used for classification tasks on medical images ( Table 1 ).This section gives a comprehensive overview of the aforementioned three transformer categories used for classification tasks on medical images ( Table 1 ).</p>
        <p>We call ViTs that are similar to the originally proposed one [13] p ure transformers. These methods usually do not contain significant structural changes compared with the original method. We introduce the literature of pure transformers by image modality, e.g. , X-ray [44,48] , CT [19][20] , magnetic resonance imaging (MRI) [21] , ultrasound [61] , and optical coherence tomography (OCT) [71] .We call ViTs that are similar to the originally proposed one [13] p ure transformers. These methods usually do not contain significant structural changes compared with the original method. We introduce the literature of pure transformers by image modality, e.g. , X-ray [44,48] , CT [19][20] , magnetic resonance imaging (MRI) [21] , ultrasound [61] , and optical coherence tomography (OCT) [71] .</p>
        <p>X-ray. X-ray is an inexpensive and convenient imaging technique that is widely used in screening and diagnosis of diseases including, breast cancer, pneumonia, and fracture. During the COVID-19 pandemic in particular, X-ray has played a very important part in disease screening and is thus a popular modality for AI researchers to use when designing transformer-based methods. Liu et al. [48] developed the vision outlooker (VOLO), a ViT model that replaced the original attention mechanism with the outlooker attention, as proposed in [72] . Their model achieved SOTA performance for the diagnosis of COVID-19 without pretraining on ImageNet. Shome et al. [50] proposed a ViT-based model for COVID-19 diagnosis that was trained on a self-collected large dataset of COVID-19 chest X-ray images. They also used Grad-CAM [73] to show the progression of COVID-19. Krishnan et al. [51] applied an ImageNetpretrained ViT-B/32 network to distinguish COVID-19, using patches from chest X-ray images as inputs. Given the effectiveness of ViTs for COVID-19 diagnosis, Tanzi et al. [44] used a ViT model to classify femur fracture. Their work used clustering methods to validate the ability of the ViT model to extract features and compared its performance against that of CNNs. The aforementioned models demonstrate the importance of large-scale datasets, which enhance the performance of transformers. Therefore, as the scale of the dataset for COVID-19-related tasks [48,[50][51]73] was larger than that used for the femur fracture task [44] , the performance on the COVID-19-related task was also higher.X-ray. X-ray is an inexpensive and convenient imaging technique that is widely used in screening and diagnosis of diseases including, breast cancer, pneumonia, and fracture. During the COVID-19 pandemic in particular, X-ray has played a very important part in disease screening and is thus a popular modality for AI researchers to use when designing transformer-based methods. Liu et al. [48] developed the vision outlooker (VOLO), a ViT model that replaced the original attention mechanism with the outlooker attention, as proposed in [72] . Their model achieved SOTA performance for the diagnosis of COVID-19 without pretraining on ImageNet. Shome et al. [50] proposed a ViT-based model for COVID-19 diagnosis that was trained on a self-collected large dataset of COVID-19 chest X-ray images. They also used Grad-CAM [73] to show the progression of COVID-19. Krishnan et al. [51] applied an ImageNetpretrained ViT-B/32 network to distinguish COVID-19, using patches from chest X-ray images as inputs. Given the effectiveness of ViTs for COVID-19 diagnosis, Tanzi et al. [44] used a ViT model to classify femur fracture. Their work used clustering methods to validate the ability of the ViT model to extract features and compared its performance against that of CNNs. The aforementioned models demonstrate the importance of large-scale datasets, which enhance the performance of transformers. Therefore, as the scale of the dataset for COVID-19-related tasks [48,[50][51]73] was larger than that used for the femur fracture task [44] , the performance on the COVID-19-related task was also higher.</p>
        <p>Computed tomography. CT is based on the high contrast between gas and tissue and is commonly used for thoracic disease diagnosis. Thus, the applications of pure transformers to CT images have mainly focused on thoracic diseases. For example, Than et al. [40] studied the effect of patch size when using ViT for COVID-19 and diseased lung classification tasks. They found that the performance dropped with larger patch sizes, revealing a trade off between local and global information. The 32 Ã— 32 patch resulted in the best accuracy. Costa et al. [22] used ViT and its variants to distinguish COVID-19 pneumonia and other pneumonia from normal cases. By comparing the performance of several models, they showed that pretrained models including DeiT [30] achieved competitive results. The conventional ViT and its variants using performer encoder also achieved good results, even without pretraining. Li et al. [39] designed a platform for COVID-19 diagnosis based on ViT. They converted CT images into a series of flattened patches to fit the input of ViT for diagnosis. They also adopted a teacher-student model to distill knowledge from a CNN pretrained on natural images. Gao et al. [19] applied ViT to both two-dimensional (2D) and 3D CT scans to diagnose COVID-19. They constructed an image sub-volume by extracting a fixed number of slices, thereby 'normalizing' imaging sequences with a varying number of slices. They also proved that the performance of ViT was better than that of DenseNet, which is a competitive CNN model. Zhang et al. [20] trained the popular Swin-Transformer on CT images. Specifically, the framework first segments the lung via a Unet and then feeds the lung region to the feature extractor. This strategy helped to reduce the computation burden of the transformer framework. The aforementioned works show the importance of pretraining for CT image classification tasks, as CT images are much harder to acquire than X-ray images. Also, methods that reduce computational complexity using attention mechanism are crucial to classification of CT images, owing to the large volume of the images.Computed tomography. CT is based on the high contrast between gas and tissue and is commonly used for thoracic disease diagnosis. Thus, the applications of pure transformers to CT images have mainly focused on thoracic diseases. For example, Than et al. [40] studied the effect of patch size when using ViT for COVID-19 and diseased lung classification tasks. They found that the performance dropped with larger patch sizes, revealing a trade off between local and global information. The 32 Ã— 32 patch resulted in the best accuracy. Costa et al. [22] used ViT and its variants to distinguish COVID-19 pneumonia and other pneumonia from normal cases. By comparing the performance of several models, they showed that pretrained models including DeiT [30] achieved competitive results. The conventional ViT and its variants using performer encoder also achieved good results, even without pretraining. Li et al. [39] designed a platform for COVID-19 diagnosis based on ViT. They converted CT images into a series of flattened patches to fit the input of ViT for diagnosis. They also adopted a teacher-student model to distill knowledge from a CNN pretrained on natural images. Gao et al. [19] applied ViT to both two-dimensional (2D) and 3D CT scans to diagnose COVID-19. They constructed an image sub-volume by extracting a fixed number of slices, thereby 'normalizing' imaging sequences with a varying number of slices. They also proved that the performance of ViT was better than that of DenseNet, which is a competitive CNN model. Zhang et al. [20] trained the popular Swin-Transformer on CT images. Specifically, the framework first segments the lung via a Unet and then feeds the lung region to the feature extractor. This strategy helped to reduce the computation burden of the transformer framework. The aforementioned works show the importance of pretraining for CT image classification tasks, as CT images are much harder to acquire than X-ray images. Also, methods that reduce computational complexity using attention mechanism are crucial to classification of CT images, owing to the large volume of the images.</p>
        <p>Magnetic resonance imaging. MRI has a better imaging quality, particularly for subtle anatomical structures including vessels and nerves; however, acquisition of MRI images is time-consuming. As MRI is a powerful non-invasive imaging technology for soft tissues, it is commonly used in neuroimaging studies. For instance, He et al. [21] proposed a two-pathway network for brain age estimation. A global pathway was designed to capture the global contextual information from the brain MRI, whereas a local pathway was responsible for capturing finegrained information from local patches. The local and global contextual representations were then fused by a global-local attention mechanism. Next, the concatenation of fused features and local patches was fed into a revised global-local transformer. MRI also has a wide spectrum of clinical applications, e.g., cancer diagnosis, which makes it a strong candidate modality for training ViTs.Magnetic resonance imaging. MRI has a better imaging quality, particularly for subtle anatomical structures including vessels and nerves; however, acquisition of MRI images is time-consuming. As MRI is a powerful non-invasive imaging technology for soft tissues, it is commonly used in neuroimaging studies. For instance, He et al. [21] proposed a two-pathway network for brain age estimation. A global pathway was designed to capture the global contextual information from the brain MRI, whereas a local pathway was responsible for capturing finegrained information from local patches. The local and global contextual representations were then fused by a global-local attention mechanism. Next, the concatenation of fused features and local patches was fed into a revised global-local transformer. MRI also has a wide spectrum of clinical applications, e.g., cancer diagnosis, which makes it a strong candidate modality for training ViTs.</p>
        <p>Ultrasound. Ultrasound at point of care has expanded the range of applications of ultrasound, as specific positions are not necessary to acquire images. Perera et al. [61] developed a transformer-based architecture to diagnose COVID-19 based on ultrasound clips. To ensure memory and time efficiencies, they replaced the standard ViTs with Linformer, reducing the space time complexity from ğ‘‚( ğ‘› 2 ) for the conventional self-attention mechanism to ğ‘‚( ğ‘› ) . Moreover, ultrasound has became a prominent modality for imaging of breast cancer owing to its ease of use, low cost, and safety. Gheflati et al. [62] used ViTs to classify normal, malignant, and benign breast tissues based on ultrasound images. They also compared the performance of ViTs of various configurations against CNNs to demonstrate their efficiency.Ultrasound. Ultrasound at point of care has expanded the range of applications of ultrasound, as specific positions are not necessary to acquire images. Perera et al. [61] developed a transformer-based architecture to diagnose COVID-19 based on ultrasound clips. To ensure memory and time efficiencies, they replaced the standard ViTs with Linformer, reducing the space time complexity from ğ‘‚( ğ‘› 2 ) for the conventional self-attention mechanism to ğ‘‚( ğ‘› ) . Moreover, ultrasound has became a prominent modality for imaging of breast cancer owing to its ease of use, low cost, and safety. Gheflati et al. [62] used ViTs to classify normal, malignant, and benign breast tissues based on ultrasound images. They also compared the performance of ViTs of various configurations against CNNs to demonstrate their efficiency.</p>
        <p>Others. In addition to the above-mentioned imaging modalities, other imaging technologies have been adopted for the examination and diagnosis of specific diseases, e.g. , using dermoscopy images [66] ,Others. In addition to the above-mentioned imaging modalities, other imaging technologies have been adopted for the examination and diagnosis of specific diseases, e.g. , using dermoscopy images [66] ,</p>
        <p>fundus images [74] , or histopathology images [59] . For instance, Xie et al. [66] aimed to detect melanoma using dermoscopy images. They combined the Swin-Transformer with a parameter-free attention module, 
            <rs type="software">SimAM</rs>, to learn better features for the target classification task. As the features fed into the classifier contained rich semantic information but lacked detailed information, they designed the output of the first three Swin-Transformer blocks as three SimAM blocks input separately; then, all 
            <rs type="software">SimAM</rs> block outputs including the final feature map were concatenated together to form the new final feature map, which served as the input to the final classification layer. Li et al. [67] evaluated the performance of transformers on out-of-distribution (OOD) detection tasks in medical image analysis. The original ViT and the DeiT with multihead, soft distillation, and hard distillation are included in their work. The performance of these models on skin lesion datasets HAM10000 and DermNet showed the limited performance and safety critical problems of transformers on the OOD detection task. Ikromjanov et al. [59] used ViT to assist pathologists to grade prostate cancer according to the Gleason grading system on whole-slide histopathology images and reported promising results.
        </p>
        <p>As shown in Table 1 , despite the excellent performance of pure transformers in certain cases, e.g. , analysis of COVID-19 X-ray images, further development is necessary for other tasks.As shown in Table 1 , despite the excellent performance of pure transformers in certain cases, e.g. , analysis of COVID-19 X-ray images, further development is necessary for other tasks.</p>
        <p>Although pure ViTs can achieve promising results without much modification, there has been extensive exploration of the possibilities of combining ViTs with other learning components to better capture complex data distributions or achieve better performance. Typical cases include combinations of transformers with (1) convolutional layers and (2) graph representations. We next introduce both categories.Although pure ViTs can achieve promising results without much modification, there has been extensive exploration of the possibilities of combining ViTs with other learning components to better capture complex data distributions or achieve better performance. Typical cases include combinations of transformers with (1) convolutional layers and (2) graph representations. We next introduce both categories.</p>
        <p>Transformers with convolutions. ViTs focus more on modeling the global relationship within the data, whereas conventional CNNs pay more attention to the local texture. These differences have inspired researchers to combine the advantages of ViTs and CNNs. In addition, the analysis of medical images involves not only the correlation of regions in the image but also subtle textures. Hence, many studies have explored CNN-ViT combinations.Transformers with convolutions. ViTs focus more on modeling the global relationship within the data, whereas conventional CNNs pay more attention to the local texture. These differences have inspired researchers to combine the advantages of ViTs and CNNs. In addition, the analysis of medical images involves not only the correlation of regions in the image but also subtle textures. Hence, many studies have explored CNN-ViT combinations.</p>
        <p>Most applications have focused on the diagnosis of thoracic diseases, especially COVID-19 and related diseases. Benefiting from ViT's power of feature integration, Van et al. [23] used a transformer to conduct multi-view analysis of unregistered medical images in order to classify chest X-rays. They proposed a transformer-based approach that considered spatial information across different views at the feature-level by virtue of a trainable attention mechanism. They applied the transformer to intermediate feature maps produced by CNNs to retrieve features from one view and transfer them to another view. Thus, additional context was added to the original view without requiring pixel-wise correspondence. Their approach also contributed to a reduction in computational complexity by substituting a smaller number of visual tokens for the source pixels. Verenich et al. [45] introduced global spatial information from ViTs to CNNs for pulmonary disease classification, while preserving spatial invariance and equivariance. Liang et al. [37] used a CNN to mine effective features and a transformer for feature aggregation. In addition, an effective data sampling strategy can be used to reduce the size of the inputs while preserving sufficient information for diagnosis. Park et al. [43] designed a pretrained CNN backbone followed by a ViT for COVID-19 diagnosis. A large-scale public dataset for CXR classification was used for model pretraining. For the simple task of classifying thoracic diseases, existing methods are simple yet effective, with a CNN used to extract the features, followed by capture of high-level information with a transformer.Most applications have focused on the diagnosis of thoracic diseases, especially COVID-19 and related diseases. Benefiting from ViT's power of feature integration, Van et al. [23] used a transformer to conduct multi-view analysis of unregistered medical images in order to classify chest X-rays. They proposed a transformer-based approach that considered spatial information across different views at the feature-level by virtue of a trainable attention mechanism. They applied the transformer to intermediate feature maps produced by CNNs to retrieve features from one view and transfer them to another view. Thus, additional context was added to the original view without requiring pixel-wise correspondence. Their approach also contributed to a reduction in computational complexity by substituting a smaller number of visual tokens for the source pixels. Verenich et al. [45] introduced global spatial information from ViTs to CNNs for pulmonary disease classification, while preserving spatial invariance and equivariance. Liang et al. [37] used a CNN to mine effective features and a transformer for feature aggregation. In addition, an effective data sampling strategy can be used to reduce the size of the inputs while preserving sufficient information for diagnosis. Park et al. [43] designed a pretrained CNN backbone followed by a ViT for COVID-19 diagnosis. A large-scale public dataset for CXR classification was used for model pretraining. For the simple task of classifying thoracic diseases, existing methods are simple yet effective, with a CNN used to extract the features, followed by capture of high-level information with a transformer.</p>
        <p>For applications other than COVID-19 diagnosis, Yassine et al. [38] combined several CNNs with a ViT by feeding extracted features into the ViT. They compared the number of CNNs as well as their pretraining configurations against the hybrid CNN-ViT model. Notably, they pretrained the CNN on images generated from the ImageNet dataset [12] using a generative adversarial network (GAN) pretrained on brain CT images. They claimed that further pretraining on the generated images would lead to a better inductive bias for the target CT dataset as the dissimilarities of the two domains would be reduced. Zhao et al. [53] used a combination of CNNs and transformers to conduct multi-index quantification of hepatocellular carcinoma using multi-phase contrast-enhanced MRI (CEMRI). They proposed mrTrans-Net, which involves three parallel encoders, each followed by a non-local transformer that extracts features from the arterial phase, PV phase, and delay phase. Next, a phase-aware transformer is used to quantify the relevance of each phase for the target multi-phase CEMRI information fusion and selection. Quantification is conducted not only after the phase-aware transformer but also after the nonlocal transformers to form an enhanced loss function to constrain the quantification task. Jiang et al. [65] explored the effectiveness of ensemble learning by treating ViTs and CNNs as base learners to diagnose acute lymphoblastic leukemia based on microscopic images of B-lymphoid precursors and leukemic B-lymphoblast cells. They proposed an ensemble model based on the ViT and EfficientNet. As the two base models were complementary, the ensemble results showed some improvement. They also proposed a data enhancement method to handle the imbalance between normal and cancer cells in each image. Chen et al. [56] proposed the multi-scale ViT model shown in Figure 6 , called 
            <rs type="software">GasHis-Transformer</rs>, for classification of gastric histopathological images. They designed a global information module (GIM) and local information module (LIM) (based on CNNs) to extract features. Moreover, they borrowed the parallel structure from Inception-V3 to learn multi-scale local representations. Their model was robust to ten different adversarial attacks or conventional noises and was generalizable to classification tasks of histopathological images of other cancers. Gao et al. [55] proposed the instance-based ViT (i-ViT) for papillary renal cell carcinoma subtyping. The i-ViT first extracts and selects instance features from instance-level patches, which include a nucleus with parts of the surrounding background and the nuclei grade. Next, it aggregates these features to further capture cell-level and cell-layer-level features. Last, the model encodes both types of fine-gained features into the final image-level representation, where grades and positions are embedded for subtyping. Wang et al. [54] proposed a 3D transformer that could outperform 3D CNNs. They used a 3D convolutional layer to extract features of 3D blocks and a teacherstudent network to learn transformer weights from a CNN teacher. Xia et al. [42] proposed anatomy-aware transformers for pancreatic cancer screening, and showed to win the radiologists. Zeid et al. [57] validated ViTs and their variants compact convolutional transformers on a multi-class colorectal cancer (CRC) histology image classification task using a public CRC histology dataset. Zhao et al. [60] combined taming transformers with T2T-ViT to handle unbalanced samples with inconsistent image quality for a cervical cancer classification task. Yu et al. [68] adopted the transformer encoder to model dependency among features of skin lesions to detect the ugly duckling sign for melanoma identification. Yang et al. [70] proposed the transformer eye (TransEye) for fine-grained fundus disease image classification by combining CNN and transformer models. Wu et al. [69] proposed ScAT-Net to model inter-patch and inter-scale representations at multiple input scales to diagnose melanocytic lesions in biopsy images. These hybrid transformers for various applications contain rich innovations, including structural improvements, novel ViT modules, CNN modules, and learning strategies for pretraining and ensembling.
        </p>
        <p>Transformers with graphs. Learning with graphs is a common practice in MIA. The core concept of graph learning is learning a compact representation of each sample (e.g., embeddings) while preserving the intrinsic inter-sample relationships via a data graph [75] . As an attention-based network, transformer is suitable for operations on graph data, including aggregation of node features and calculation of node relationships.Transformers with graphs. Learning with graphs is a common practice in MIA. The core concept of graph learning is learning a compact representation of each sample (e.g., embeddings) while preserving the intrinsic inter-sample relationships via a data graph [75] . As an attention-based network, transformer is suitable for operations on graph data, including aggregation of node features and calculation of node relationships.</p>
        <p>In the field of network neuroscience, a brain network is modeled as a graph where each node denotes an anatomical region of interest (ROI) and the edge connecting two nodes encodes their interaction (e.g., neural firing). Brain graphs play an important part in advancing our understanding of the brain as a highly interconnected system in both health and disease [76][77] . Kim et al. [52] leveraged the dynamic characteristics of a functional connectivity network by incorporating dynamic features into a compact brain graph representation. Specifically, they proposed the spatio-temporal attention graph isomorphism network (
            <rs type="software">STAGIN</rs>) for learning a dynamic graph representation of the brain connectome with spatio-temporal attention. In 
            <rs type="software">STAGIN</rs>, the GNN is used to extract graph-level representations for the functional brain connectome at each timestep, and a transformer encoder is used to obtain the final representation of a sequence of dynamic graphs. In detail, encoded timestamps are concatenated with node features to embed temporal information. The authors claim that the use of the transformer not only improved the classification performance of the model but also improved its spatial-temporal interpretability. Such methods have validated the power of transformers for mining both features and relationships of complex graphs, attracting more attention to this methodology.
        </p>
        <p>We draw the following conclusions regarding the use of transformers for medical image classification tasks.We draw the following conclusions regarding the use of transformers for medical image classification tasks.</p>
        <p>â€¢ Transformers have achieved performance comparable with or better than that of CNNs on most tasks. â€¢ Transformers perform best on large-scale datasets, which somewhat limits their applicability, especially in the medical image analysis field. Pretraining could alleviate this problem. â€¢ The computational burden of training transformers on large images is high. Hence, reducing model complexity and developing lightweight models are key factors to improve efficiency. â€¢ Hybrid transformers have attracted increasing attention as they have the advantages of both conventional networks ( i.e. , CNNs and GNNs) and transformers.â€¢ Transformers have achieved performance comparable with or better than that of CNNs on most tasks. â€¢ Transformers perform best on large-scale datasets, which somewhat limits their applicability, especially in the medical image analysis field. Pretraining could alleviate this problem. â€¢ The computational burden of training transformers on large images is high. Hence, reducing model complexity and developing lightweight models are key factors to improve efficiency. â€¢ Hybrid transformers have attracted increasing attention as they have the advantages of both conventional networks ( i.e. , CNNs and GNNs) and transformers.</p>
        <p>Transformer-based methods have also been applied to a variety of segmentation tasks, including abdominal multi-organ segmentation [25,[78][79][80]82,86,[93][94][96][97]106,[113][114]119,121] , thoracic multi-organ segmentation [114] , cardiac segmentation [78,80,84,[86][87]94,[96][97]106,113,119,121,127] , Pancreas segmentation [81,118] , brain tumor/tissue segmentation [82,86,88,100,[107][108][117][118][128][129][130][131][132][133][134] , polyp segmentation [91,103,120,[135][136] , liver and hepatic lesion segmentation [86,93,117,[137][138][139][140] , kidney tumor segmentation [86,138] , skin lesion segmentation [91,103,109,117,120,136,141] , prostate segmentation [91,140] , gland segmentation [24,95,100,109,120] , nucleus segmentation [24,95,100,109,120] , cell segmentation [103,[142][143] , spleen segmentation [107] , lung field/COVID-19 pneumonia lesion segmentation [109] , retinal vessel segmentation [144] , and hyperspectral pathology image segmentation [145] . Several notable methods are listed and detailed in Table 2 .Transformer-based methods have also been applied to a variety of segmentation tasks, including abdominal multi-organ segmentation [25,[78][79][80]82,86,[93][94][96][97]106,[113][114]119,121] , thoracic multi-organ segmentation [114] , cardiac segmentation [78,80,84,[86][87]94,[96][97]106,113,119,121,127] , Pancreas segmentation [81,118] , brain tumor/tissue segmentation [82,86,88,100,[107][108][117][118][128][129][130][131][132][133][134] , polyp segmentation [91,103,120,[135][136] , liver and hepatic lesion segmentation [86,93,117,[137][138][139][140] , kidney tumor segmentation [86,138] , skin lesion segmentation [91,103,109,117,120,136,141] , prostate segmentation [91,140] , gland segmentation [24,95,100,109,120] , nucleus segmentation [24,95,100,109,120] , cell segmentation [103,[142][143] , spleen segmentation [107] , lung field/COVID-19 pneumonia lesion segmentation [109] , retinal vessel segmentation [144] , and hyperspectral pathology image segmentation [145] . Several notable methods are listed and detailed in Table 2 .</p>
        <p>The U-shaped convolutional neural network architecture known as Unet has achieved tremendous success on most medical image segmentation tasks. However, owing to the use of convolution operations, Unet is also limited in its ability to model long-term dependencies. To overcome this limitation, researchers have designed robust hybrid transformers combined with the Unet architecture; these will be introduced in the first part of this section. Several methods also apply pure transformers to segmentation tasks; these will be introduced in the second part of this section.The U-shaped convolutional neural network architecture known as Unet has achieved tremendous success on most medical image segmentation tasks. However, owing to the use of convolution operations, Unet is also limited in its ability to model long-term dependencies. To overcome this limitation, researchers have designed robust hybrid transformers combined with the Unet architecture; these will be introduced in the first part of this section. Several methods also apply pure transformers to segmentation tasks; these will be introduced in the second part of this section.</p>
        <p>Most existing research on coupling transformers with the popular U-shaped architecture focuses on the following three aspects:Most existing research on coupling transformers with the popular U-shaped architecture focuses on the following three aspects:</p>
        <p>(1) inserting transformer layers at different levels of the U-shaped architecture;(1) inserting transformer layers at different levels of the U-shaped architecture;</p>
        <p>(2) combining transformers and CNNs using different strategies;(2) combining transformers and CNNs using different strategies;</p>
        <p>(3) using multi-scale features or attention mechanisms. We detail below each of these three categories.(3) using multi-scale features or attention mechanisms. We detail below each of these three categories.</p>
        <p>An intuitive way to insert transformer layers into a U-shaped architecture is to insert a whole transformer between the encoder and decoder blocks to build long-term dependencies between high-level vision concepts. Based on this idea, Chen et al. [78] proposed 
            <rs type="software">TransUNet</rs>, shown in Figure 7 , which extracts high-resolution spatial features using a CNN and then encodes the global context using a transformer. The self-attention features encoded by the transformer are then upsampled and combined with features at multiple scales extracted from the encoding path using skip connections for precise localization. Tran-sUNet achieved superior performance compared with V-Net, 
            <rs type="software">AttnUNet</rs>, and ViT on multi-organ and cardiac segmentation tasks. Similarly, Yao et al. [79] combined a transformer network with a Claw Unet architecture; the resulting model outperformed 
            <rs type="software">TransUnet</rs> for synapse multiorgan segmentation. In another instance, Xu et al. [80] proposed 
            <rs type="software">LeViT-</rs>UNet, which integrates a LeViT Transformer into the Unet architecture. Sha et al. [81] designed a transformer-Unet by adding Transformer modules to Unet; the resulting model outperformed 
            <rs type="software">TransUnet</rs>.
        </p>
        <p>In contrast to the above approaches, in which the transformer was inserted immediately after the encoder block, Li et al. [82] added an attention upsampling component to the decoder. They also proposed a window attention decoder and window attention upsampling, working on local windows, to reduce memory and computation costs. Gao et al. [84] presented a UTNet in which self-attention modules are applied to both encoder and decoder blocks to capture long-range dependencies at multiple scales with minimal overhead. They proposed an efficient self-attention mechanism along with relative position encoding, which reduced the complexity of the self-attention operation significantly from ğ‘‚( ğ‘› 2 ) to approximate ğ‘‚( ğ‘› ) . In an upgrade of their work, i.e. , UTNetV2 [87] , they further proposed an efficient bidirectional attention. Fu et al. [86] proposed TF-Unet, which is built on the intertwined backbone of convolution and transformers at multiple scales. Several studies report improved strategies for feature concatenation [93,127] .In contrast to the above approaches, in which the transformer was inserted immediately after the encoder block, Li et al. [82] added an attention upsampling component to the decoder. They also proposed a window attention decoder and window attention upsampling, working on local windows, to reduce memory and computation costs. Gao et al. [84] presented a UTNet in which self-attention modules are applied to both encoder and decoder blocks to capture long-range dependencies at multiple scales with minimal overhead. They proposed an efficient self-attention mechanism along with relative position encoding, which reduced the complexity of the self-attention operation significantly from ğ‘‚( ğ‘› 2 ) to approximate ğ‘‚( ğ‘› ) . In an upgrade of their work, i.e. , UTNetV2 [87] , they further proposed an efficient bidirectional attention. Fu et al. [86] proposed TF-Unet, which is built on the intertwined backbone of convolution and transformers at multiple scales. Several studies report improved strategies for feature concatenation [93,127] .</p>
        <p>Unlike the aforementioned methods that combine transformers and U-shaped architectures within a single inference path, some studies have explored different transformer-CNN coupling strategies. Sun et al. [88] used Unet and transformer encoders to generate representations independently and then integrated their representations for subsequent decoding. Similarly, Li et al. [170] proposed X-Net, which used a CNN and a transformer to extract local and global features simultaneously. Zhang et al. [91] proposed 
            <rs type="software">TransFuse</rs>, which also combines transformers and Unet in a parallel style. In an improvement on the above-mentioned work, a novel fusion technique, i.e. , BiFusion module, was proposed to efficiently fuse multi-level features from both branches. Luo et al. [95] also used bidirectional cross-attention to fuse local information extracted by the convolution operations and global information learned by the self-attention mechanisms. Liu et al. [96] proposed PHTrans, which introduces a parallel hybrid module in deep stages, where convolution blocks and the modified 3D Swin-Transformer learn local features and global dependencies separately; then, a sequence-tovolume operation unifies the dimensions of the outputs to achieve feature aggregation ( Figure 8 ).
        </p>
        <p>Zhou et al. [97] claimed that most of the recently proposed transformer-based segmentation approaches simply treat transformers as assisted modules to help encode global context in convolutional [122] , CVC-ClinicDB [123] , CVC-ColonDB [124] , EndoScene [125] , and ETIS [126] . representations, without investigating how to optimally combine selfattention with convolution. To address this issue, they introduced the 
            <rs type="software">nnFormer</rs>, which has an interleaved architecture based on empirical combination of self-attention and convolution. Xu et al. [94] proposed the ECT-NAS method to search for efficient CNN-transformer architectures for medical image segmentation based on a multi-scale space search.
        </p>
        <p>The multi-scale strategy for transformers in MIA uses features in a multi-scale manner or takes multi-scale images as inputs.The multi-scale strategy for transformers in MIA uses features in a multi-scale manner or takes multi-scale images as inputs.</p>
        <p>(1) Multi-resolution images. Zhang et al. [24] proposed a pyramidal network architecture, namely pyramid medical transformer (PMTrans), which captures multi-range relations by working on multi-resolution images. Valanarasu et al. [100] added gated axial transformer layers in the encoder, which contains the basic building block of both height-and width-gated multi-head attention blocks. The whole image and patches were used to learn global and local features, respectively, and a localglobal training strategy was proposed to further boost the overall performance.(1) Multi-resolution images. Zhang et al. [24] proposed a pyramidal network architecture, namely pyramid medical transformer (PMTrans), which captures multi-range relations by working on multi-resolution images. Valanarasu et al. [100] added gated axial transformer layers in the encoder, which contains the basic building block of both height-and width-gated multi-head attention blocks. The whole image and patches were used to learn global and local features, respectively, and a localglobal training strategy was proposed to further boost the overall performance.</p>
        <p>(2) Multi-scale features. In contrast to 
            <rs type="software">TransUNet</rs>, which only uses a transformer to process the low-resolution feature maps learned from the previous layer, Xie et al. [25] proposed a deformable transformer to process multi-scale and high-resolution feature maps. Ji et al. [103] proposed a multi-compound transformer (MCTrans), which embeds multiscale convolutional features as a sequence of tokens and performs intra-and inter-scale self-attention. In contrast to models that use CNNs to extract features, Hatamizadeh et al. [107] introduced Unet transformers (UNETR), which use a pure transformer as an encoder to learn sequence representations of the input volume. The transformer encoder is directly connected to a decoder via skip connections at different resolutions to compute the final semantic segmentation output. Zhang et al. [117] proposed 
            <rs type="software">S 2 WinTOUnet</rs>, which uses a star-shaped window self-attention to obtain fine-grained details and coarse-grained semantic information.
        </p>
        <p>(3) Multi-level attention. Chen et al. [109] proposed 
            <rs type="software">TransAttUnet</rs>, in which a multi-level guided attention and multi-scale skip connection are jointly designed to effectively enhance traditional U-shaped architectures. Both transformer self attention and global spatial attention are incorporated into 
            <rs type="software">TransAttUnet</rs> to effectively learn non-local interactions between encoded features. Wang et al. [113] proposed the mixed transformer module, which calculates self-affinities through welldesigned local-global Gaussian-weighted self-attention and then mines interconnections between data samples through external attention. Wu et al. [106] proposed the dilated transformer, which conducts selfattention for pairwise patch relations that are captured alternately in local and global scopes.
        </p>
        <p>(4) Multi-axial fusion. Yan et al. [114] applied an axial fusion transformer to fuse inter-slice and intra-slice information, which reduced the computational complexity of calculating self-attention in 3D space.(4) Multi-axial fusion. Yan et al. [114] applied an axial fusion transformer to fuse inter-slice and intra-slice information, which reduced the computational complexity of calculating self-attention in 3D space.</p>
        <p>To conclude, the aforementioned methods all leverage additional features learned using a feature fusion strategy for more effective learning.To conclude, the aforementioned methods all leverage additional features learned using a feature fusion strategy for more effective learning.</p>
        <p>In addition to the aforementioned variants of the Unet architecture that combine a transformer with convolutions, Karimi et al. [118] used simple self-attention between adjacent image patches without convolution operations. A 3D image is divided into ğ‘› 3 3D patches ( ğ‘› = 3 or 5), and a 1D embedding is learned for each patch. Through the selfattention between patch embeddings, the network outputs the segmentation result of the center patch. Methods using this assumption can be easily recognized as pure transformers.In addition to the aforementioned variants of the Unet architecture that combine a transformer with convolutions, Karimi et al. [118] used simple self-attention between adjacent image patches without convolution operations. A 3D image is divided into ğ‘› 3 3D patches ( ğ‘› = 3 or 5), and a 1D embedding is learned for each patch. Through the selfattention between patch embeddings, the network outputs the segmentation result of the center patch. Methods using this assumption can be easily recognized as pure transformers.</p>
        <p>Cao et al. [119] developed an Unet-like pure transformer for medical image segmentation by feeding tokenized image patches into the a transformer-like U-shaped encoder-decoder architecture with skip connections for semantic feature learning in a local-global manner. Lin et al. [120] went a step further and proposed DS-TransUNet, which first adopts dual-scale encoder subnetworks based on Swin-Transformer to extract coarse-and fine-grained feature representations on different semantic scales. A well-designed transformer interactive fusion module was also proposed to effectively establish global dependencies between features of different scales through the self-attention mechanism. To better leverage the natural multi-scale feature hierarchies of transformers, Huang et al. [121] proposed MISSFormer, which has two appealing design features: (1) an enhanced transformer block as a feed-forward network with better feature consistency, long-range dependencies, and local context; and (2) an enhanced transformer context bridge to model long-range dependencies and local context of multi-scale features generated by the hierarchical transformer encoder.Cao et al. [119] developed an Unet-like pure transformer for medical image segmentation by feeding tokenized image patches into the a transformer-like U-shaped encoder-decoder architecture with skip connections for semantic feature learning in a local-global manner. Lin et al. [120] went a step further and proposed DS-TransUNet, which first adopts dual-scale encoder subnetworks based on Swin-Transformer to extract coarse-and fine-grained feature representations on different semantic scales. A well-designed transformer interactive fusion module was also proposed to effectively establish global dependencies between features of different scales through the self-attention mechanism. To better leverage the natural multi-scale feature hierarchies of transformers, Huang et al. [121] proposed MISSFormer, which has two appealing design features: (1) an enhanced transformer block as a feed-forward network with better feature consistency, long-range dependencies, and local context; and (2) an enhanced transformer context bridge to model long-range dependencies and local context of multi-scale features generated by the hierarchical transformer encoder.</p>
        <p>Transformer models also have been shown to have strong learning ability in many image-to-image translation applications including image synthesis [16] , reconstruction [171] , and super-resolution [172] . However, in the field of medical image analysis, studies ( e.g. , [26,147] ) on image-to-image translation have recently started to emerge. We list existing transformer-based image-to-image translation methods in Table 3 , as well as the corresponding evaluation metrics.Transformer models also have been shown to have strong learning ability in many image-to-image translation applications including image synthesis [16] , reconstruction [171] , and super-resolution [172] . However, in the field of medical image analysis, studies ( e.g. , [26,147] ) on image-to-image translation have recently started to emerge. We list existing transformer-based image-to-image translation methods in Table 3 , as well as the corresponding evaluation metrics.</p>
        <p>In the medical field, image synthesis remains very challenging owing to inter-subject variability and the fact that anatomical hallucinations (e.g . , hallucinating a white spot in a brain MRI) might be detrimental to diagnostic tasks. In recent years, generative adversarial learning has been widely used to tackle image synthesis tasks. Therefore, transformers have been combined with a generative adversarial learning paradigm for image synthesis. For example, Hu et al. [150] intro-duced a double-scale discriminator GAN for cross-modal medical image synthesis, consisting of a transformer-based global discriminator and a CNN-based local discriminator. Watanabe et al. [26] proposed a generative model architecture based on a transformer decoder block, owing to its powerful ability in modeling time series. During data processing, they normalized the pixel values of single photon emission CT (SPECT) images by the specific/nonspecific binding ratio. During the training process, they used a transformer decoder to construct an auto-regression model and trained the model on [ 123 ğ¼] FP-CIT SPECT images from the Parkinson's Progressive Marker Initiative database in an unpaired manner. The trained model could generate SPECT images that had characteristics of Parkinson's disease patients. Kamran et al. [147] proposed a transformer-based conditional GAN, shown in Figure 9 , that could simultaneously perform semi-supervised image synthesis from fundus photographs to fluorescein angiography (FA) for diagnosis of retinal disease.In the medical field, image synthesis remains very challenging owing to inter-subject variability and the fact that anatomical hallucinations (e.g . , hallucinating a white spot in a brain MRI) might be detrimental to diagnostic tasks. In recent years, generative adversarial learning has been widely used to tackle image synthesis tasks. Therefore, transformers have been combined with a generative adversarial learning paradigm for image synthesis. For example, Hu et al. [150] intro-duced a double-scale discriminator GAN for cross-modal medical image synthesis, consisting of a transformer-based global discriminator and a CNN-based local discriminator. Watanabe et al. [26] proposed a generative model architecture based on a transformer decoder block, owing to its powerful ability in modeling time series. During data processing, they normalized the pixel values of single photon emission CT (SPECT) images by the specific/nonspecific binding ratio. During the training process, they used a transformer decoder to construct an auto-regression model and trained the model on [ 123 ğ¼] FP-CIT SPECT images from the Parkinson's Progressive Marker Initiative database in an unpaired manner. The trained model could generate SPECT images that had characteristics of Parkinson's disease patients. Kamran et al. [147] proposed a transformer-based conditional GAN, shown in Figure 9 , that could simultaneously perform semi-supervised image synthesis from fundus photographs to fluorescein angiography (FA) for diagnosis of retinal disease.</p>
        <p>To tackle the problem of the intensity range of positron emission tomography (PET) often being wide and dense and even heavily biased toward zero, Shin et al. [149] built a GAN utilizing BERT, namely 
            <rs type="software">GAN-BERT</rs>, to generate PET images from MRI images. Luo et al. [173] proposed a 3D transformer GAN to reconstruct high-quality PET image at a low dose. In order to overcome the limitation of scarce access to large medical datasets, Korkmaz et al. [151] introduced an unsupervised reconstruction method based on zero-Shot Learned Adversarial TransformERs (SLATER) to perform MRI synthesis. SLATER is an unconditional adversarial architecture consisting of a synthesizer, a discriminator, and a mapper. The synthesizer uses cross-attention transformer blocks to capture long-range relationships, and the mapper maps noise and latent variables onto MR images. Ristea et al. [153] proposed an architecture named 
            <rs type="software">CyTran</rs>, which is based on generative adversarial convolutional transformers and integrates the cycle-consistency loss for translation of unpaired CT images between contrast and non-contrast CT scans.
        </p>
        <p>In addition to their applications to image synthesis between two modalities, transformer models have been used successfully in multimodal medical image synthesis. For example, Dalmaz et al. [154] proposed a generative adversarial approach, 
            <rs type="software">ResViT</rs>, for multi-modal medical image synthesis. The generator in 
            <rs type="software">ResViT</rs> is based on encoderdecoder architecture, with a central bottleneck that comprises aggregated residual transformer blocks capable of synergistically preserving local and global contexts.
        </p>
        <p>Super-resolution imaging comprises a class of techniques that enhance the resolution of an imaging system. It is also a popular sub-Figure 9. Overview of the architecture of 
            <rs type="software">VTGAN</rs>, which uses coarse and fine generators ğº ğ‘“ and ğº ğ‘ , and ViTs ğ‘‰ ğ‘‡ ğ‘“ , ğ‘‰ ğ‘‡ ğ‘ as discriminators [147] . field of image synthesis. Outstanding contributions have been made by transformer models on super-resolution tasks in medical image analysis. For instance, Feng et al. [159] introduced a task transformer network (T 2 Net) to jointly learn image reconstruction and super-resolution tasks in MRI. This multi-task framework included a super-resolution branch and a common resolution branch, and the authors designed the transformer module to embed the similarity and align the gap between the two branches. Zhang et al. [160] proposed a high-resolution synthesizer based on pyramid transformer (PTNet) and used it for MRI synthesis of images of infant brains. PTNet consists of a performer encoder, a performer decoder, and a transformer bottleneck that inherits U-structures as well as multi-resolution pyramid structures.
        </p>
        <p>Image denoising is the task of removing noise from an image. It is a fundamental step in several clinical applications. For example, Wang et al. [162] used a transformer for low-dose CT (LDCT) denoising for the first time. They developed an encoder-decoder dilation network based on token-to-token (T2T) ViT, namely TED-net. TED-net is a Ustructure model that uses the dilation in the T2T stage to enlarge the receptive field. Luthra et al. [164] proposed an edge-enhancementbased transformer (Eformer) that uses transformer blocks to construct an encoder-decoder architecture for medical image denoising. Transformer models and their applications in the task of LDCT denoising remain scarce.Image denoising is the task of removing noise from an image. It is a fundamental step in several clinical applications. For example, Wang et al. [162] used a transformer for low-dose CT (LDCT) denoising for the first time. They developed an encoder-decoder dilation network based on token-to-token (T2T) ViT, namely TED-net. TED-net is a Ustructure model that uses the dilation in the T2T stage to enlarge the receptive field. Luthra et al. [164] proposed an edge-enhancementbased transformer (Eformer) that uses transformer blocks to construct an encoder-decoder architecture for medical image denoising. Transformer models and their applications in the task of LDCT denoising remain scarce.</p>
        <p>The meaning and terminology of 'detection' varies across technical and clinical fields. In technical areas, it often refers to checking for the existence of diseases or lesions, whereas in clinical practice it often means diagnosis or disease classification, as discussed above. In computer vision, detection aims to identify the location of objects in an input image and predict their categories/classes. In this section, detection refers to object detection.The meaning and terminology of 'detection' varies across technical and clinical fields. In technical areas, it often refers to checking for the existence of diseases or lesions, whereas in clinical practice it often means diagnosis or disease classification, as discussed above. In computer vision, detection aims to identify the location of objects in an input image and predict their categories/classes. In this section, detection refers to object detection.</p>
        <p>Transformers dealing with detection tasks using medical images are often combined with CNN blocks, where a CNN is used to extract features from medical images, and the transformer architecture is used to enhance the extracted features for downstream detection. Shen et al. [166] proposed a DETR-based model, namely COTR, for the detection of polyps in the colon. DETR [14] is a primer method for object detection in computer vision. COTR is composed of a CNN for feature extraction, transformer encoder layers interleaved with convolutional layers for feature encoding and recalibration, transformer decoder layers for object querying, and a feed-forward network for detection prediction. They inserted convolutional layers into the transformer encoder for high-level image feature reconstruction and convergence acceleration. Ma et al. [167] proposed a TR-Net that combines CNN and transformer nets to detect significant stenosis in multiplanar reformatted images. Their model employs a shallow 3D-CNN to extract local semantic features of coronary regions while ensuring the model's efficiency. Next, transformer encoders are used to learn correlations between different regions of the local stenosis at each position of a coronary artery. Thus, TR-Net can accurately detect stenosis after aggregating information from local semantic features and global semantic features. Jiang et al. [165] constructed a YOLOv5s-based transformer for the detection of caries, called 
            <rs type="software">RDFNet</rs>. The model uses the FReLU activation function to activate complex visual-spatial information of images for efficiency boosting. Kong et al. [168] proposed CT-CAD, a context-aware hybrid [166] Polyp Lesion Colon&amp; rectum ETIS-LARIB CVC-ColonDB Convolutions Ã— Transformer TR-Net [167] Coronary arteries significant stenosis Coronary arteries -CT-CAD [168] Chest abnormality detection Chest Vinbig chest Chest Det 10 Context-aware feature extractor Tao et al. [169] Vertebrae detection Spine VerSe 2019 challenge MICCAI-CSI 2014 challenge Inscribed sphere-based object detector transformer for end-to-end detection of chest abnormalities on on X-ray images. Tao et al. [169] designed a spine-transformer to address automatic detection and localization of vertebrae in arbitrary field-of-view spine CT ( Table 4 ). They formulated the detection as an one-to-one set prediction problem.
        </p>
        <p>Transformers have several advantages in image registration tasks owing to their self-attention mechanism, which enables precise spatial mapping between moving and fixed images. Chen et al.pioneered the use of transformers for image registration. Inspired by the architecture of 
            <rs type="software">TransUnet</rs> [78] , they proposed 
            <rs type="software">ViT-V-Net</rs> [29] , which combines ViT and V-Net by simply altering the network architecture of 
            <rs type="software">Vox-elMorph</rs> (a conventional registration network) [174] . 
            <rs type="software">ViT-V-Net</rs> produced superior performance against benchmark methods. In an extension of their work, they developed 
            <rs type="software">TransMorph</rs> [175] for volumetric medical image registration. In this method, the Swin-Transformer [31] was used as the encoder network to capture the spatial correspondence between input moving and fixed images, and a ConvNet decoder was used to map the information provided by the transformer encoder onto a dense displacement field. Long skip connections were deployed to maintain the flow of local information between the encoder and decoder stages. Transformers-based registration methods remain rare and need further exploration and research in the future.
        </p>
        <p>Because of a limited receptive field, CNNs cannot fully utilize the global temporal and spatial information in continuous video frames; however, transformers can overcome this defect. Ji et al. [176] proposed PNS-Net (Progressively Normalized Self-attention Network) for accurate polyp segmentation from colonoscopy videos. Kondo et al. [177] proposed 
            <rs type="software">LapFormer</rs> to detect surgical tools in laparoscopic surgery videos. Czempiel et al. [178] introduced OperA to predict surgical phases from long video sequences. Reynaud et al. [179] adopted a transformer architecture, which contained a residual autoencoder Network and a BERT model, to analyze videos of arbitrary length. Long et al. [180] applied transformers to estimate surgical scene depth.
        </p>
        <p>Transformers have been successfully applied to many applications in almost all fields of medical image analysis. However, the deployment of machine learning methods in real clinical applications can lead to poor performance owing to several challenges. Among them, the most urgent challenge is label scarcity, especially in scene-understanding tasks, e.g., segmentation and detection, which usually need pixelwise precise labeling. Learning from noisy labels presents a bigger challenge. In addition, building advanced computer aided diagnosis (CADx) methods requires the use of multi-modality clinical data in a multi-task manner -a versatile learning approach that is difficult in design.Transformers have been successfully applied to many applications in almost all fields of medical image analysis. However, the deployment of machine learning methods in real clinical applications can lead to poor performance owing to several challenges. Among them, the most urgent challenge is label scarcity, especially in scene-understanding tasks, e.g., segmentation and detection, which usually need pixelwise precise labeling. Learning from noisy labels presents a bigger challenge. In addition, building advanced computer aided diagnosis (CADx) methods requires the use of multi-modality clinical data in a multi-task manner -a versatile learning approach that is difficult in design.</p>
        <p>Building models with multiple tasks helps to improve their generalizability, for which there is high demand in the field of medical image analysis. A frequently used framework unifies classification and segmentation in one model [187][188] . For instance, Chen et al. [188] proposed Multi-Task 
            <rs type="software">TransUNet</rs> (MT-TransUNet) to jointly learn segmentation and classification of skin lesions. With local details (e.g., skin color, texture) and long-range context (e.g., skin lesion shape, physical size) extracted by CNNs and ViTs, the method achieved SOTA performance and efficiency improvements in model parameters and inference speed. Sui et al. [189] combined detection with segmentation tasks to develop a novel transfer learning method, CST, with a transformer-based framework for joint CRC region detection and tumor segmentation. For detection, the generated region proposals of the input images, as well as the position features obtained by the encoder-decoder module, were used as the input to a DETR network. For segmentation, the model used image patches as inputs, which were projected into a sequence of embeddings.
        </p>
        <p>Using multiple modality data provides complementary evidence for diagnosis. For example, researchers have explored the use of combinations of OCT and visual field (VF) testing to aid in the diagnosis of eye diseases. Song et al. [71] used transformers for glaucoma diagnosis. Their model used an attention mechanism to model the pairwise relations between OCT features and VF features. Next, the attention mechanism was applied again to calculate the regional relations of features between the VF areas and the quadrants of the retinal nerve fiber layer. The complementary information was passed from one modality to another by a transformer model.Using multiple modality data provides complementary evidence for diagnosis. For example, researchers have explored the use of combinations of OCT and visual field (VF) testing to aid in the diagnosis of eye diseases. Song et al. [71] used transformers for glaucoma diagnosis. Their model used an attention mechanism to model the pairwise relations between OCT features and VF features. Next, the attention mechanism was applied again to calculate the regional relations of features between the VF areas and the quadrants of the retinal nerve fiber layer. The complementary information was passed from one modality to another by a transformer model.</p>
        <p>Monajatipoor et al. [184] developed a transformer-based vision-andlanguage model that combined the efficient PixelHop++ model with the BERT model. Specifically, the BERT model was pretrained using indomain knowledge. The model was proved to be effective when trained on small-scale datasets. The extracted vision features and the word embeddings were fed into the transformer for final diagnosis. Although the model decreased the need for massive annotations of medical images, the pretraining of the language model still needed a large amount of clinical report data. JacenkÃ³w et al. [186] combined text with CXR for disease classification. They observed that the interpretation and reporting of an image was affected by the scan request text, which served as the indication field in the radiology report. Zheng et al. [182] focused on feature fusion of multi-modal information, considering the latent inter-modal correlation. They proposed a transformer-like modalattentional feature fusion approach (MaFF) to extract rich information from each modality while mining the inter-modal relationships. Next, an adaptive graph learning mechanism was utilized to construct latent robust graphs for downstream tasks based on the fused features. The method achieved significant improvements in the prediction of AD and autism. Dai et al. [185] proposed 
            <rs type="software">TransMed</rs> for the diagnosis of parotid gland tumor. TransMed combines the advantages of CNN and transformer networks to capture both low-level textures and cross-modality high-level relationships. The model first processes multi-modal images as sequences by chaining and sending them to a CNN for feature extraction. The feature sequences are then fed into the transformers to learn the relationships between sequences as well as conducting feature fusion. Their work leveraged transformers to capture mutual information from images of different modalities, resulting in better performance and efficiency. Nguyen et al. [181] attempted to mimic the interaction between a radiologist and a general practitioner in the diagnosis of knee osteoarthritis and prediction of prognosis. They proposed a clinically-inspired multi-agent transformers (CLIMAT) framework with a tri-transformer architecture ( Table 5 ). In this framework, first, a feature extractor with a combination of transformer and CNN is used to predict the current state of a disease. Next, the non-image auxiliary information is fed into another transformer to extract context embedding. Finally, an additional transformer-based general practitioner module forecasts disease trajectory based on the current state and context embedding.
        </p>
        <p>To conclude, transformers are regarded as a promising approach to bridge CV and NLP tasks [190] . Under this assumption, Radford et al. [191] built a multi-modal transformer, CLIP, that provided zero-shot ability for recognizing images from text descriptions without image labeling. These strength of this approach also indicates a potential way of building more robust and accurate CADx methods for real clinical applications, where multiple data types, e.g. , clinical, laboratory, and imaging data, can be used as diverse source of information.To conclude, transformers are regarded as a promising approach to bridge CV and NLP tasks [190] . Under this assumption, Radford et al. [191] built a multi-modal transformer, CLIP, that provided zero-shot ability for recognizing images from text descriptions without image labeling. These strength of this approach also indicates a potential way of building more robust and accurate CADx methods for real clinical applications, where multiple data types, e.g. , clinical, laboratory, and imaging data, can be used as diverse source of information.</p>
        <p>One of the weakly supervised conditions in medical images is that the ROI for a certain disease is relatively small in the image, and only imagelevel labeling is available. Multiple instance learning (MIL) was adopted as a solution to this problem. In MIL, the training samples include sets of instances, called bags. The supervision is provided only for bags, and individual labels of the instances contained in the bags are not provided [200] .One of the weakly supervised conditions in medical images is that the ROI for a certain disease is relatively small in the image, and only imagelevel labeling is available. Multiple instance learning (MIL) was adopted as a solution to this problem. In MIL, the training samples include sets of instances, called bags. The supervision is provided only for bags, and individual labels of the instances contained in the bags are not provided [200] .</p>
        <p>Although many existing MIL methods assume that positive and negative instances are sampled independently from a positive and a nega-tive distribution [200] , instances in a bag are relational, especially in medical image analysis. The learning scenario of MIL does not follow the independent and identically distributed assumption, as the relationships between instances are not neglected. In such situations, ViTs can be leveraged to build correlations between instances to achieve better high-level representations. Li et al. [192] proposed a transformerbased MIL framework with an induced attention block, which calculates the attention while bypassing the quadratic computational complexity caused by the pairwise dot product. The feature aggregator of the framework is also based on multi-head attentions. It merges the previously mentioned features into bag representations. Yang et al. [194] treated multiple pulmonary nodules of a patient as a bag and each nodule as an instance. Unlike conventional MIL methods that use a pooling operation to get bag-level representations, they used a 3D DenseNet to learn solitary-nodule-level representations at the voxel level. Next, the generated representations were fed into the transformer to learn the nodule relationships from the same patient. To reduce the computational burden, they applied in-group scaled-dot-production attention, extracted from split channel features. Shao et al. [196] focused on the correlations between different instances as opposed to simply assuming that instances are independent and identically distributed. To this end, they proposed a transformer-based MIL framework to deal with the whole-slide image classification problem. Their framework used transformer layers to aggregate morphological information and a pyramid position encoding generator to extract spatial information. They also used the Nystrom method to calculate approximated selfattentions, which reduced computational complexity from ğ‘‚( ğ‘› 2 ) to ğ‘‚( ğ‘› ) . Rymarczyk et al. [193] focused on the attention mechanism and revised attention-based MIL pooling (AbMILP), which aggregates information from a varying number of instances. They developed self-attention AbMILP (SA-AbMILP) to model the dependencies between different instances within a bag. They also extended the calculation of attentions by introducing different kernels, which played the same part as the dot product. They evaluated their work on histological, microbiological, and retinal datasets. Yu et al. [195] explored the applicability of ViTs to retinal disease classification in fundus images ( Table 6 ). They developed a MIL-enhanced ViT (MIL-VT) by adding a plug-and-play MIL learning head to the ViT to exploit the features extracted from individual patches.Although many existing MIL methods assume that positive and negative instances are sampled independently from a positive and a nega-tive distribution [200] , instances in a bag are relational, especially in medical image analysis. The learning scenario of MIL does not follow the independent and identically distributed assumption, as the relationships between instances are not neglected. In such situations, ViTs can be leveraged to build correlations between instances to achieve better high-level representations. Li et al. [192] proposed a transformerbased MIL framework with an induced attention block, which calculates the attention while bypassing the quadratic computational complexity caused by the pairwise dot product. The feature aggregator of the framework is also based on multi-head attentions. It merges the previously mentioned features into bag representations. Yang et al. [194] treated multiple pulmonary nodules of a patient as a bag and each nodule as an instance. Unlike conventional MIL methods that use a pooling operation to get bag-level representations, they used a 3D DenseNet to learn solitary-nodule-level representations at the voxel level. Next, the generated representations were fed into the transformer to learn the nodule relationships from the same patient. To reduce the computational burden, they applied in-group scaled-dot-production attention, extracted from split channel features. Shao et al. [196] focused on the correlations between different instances as opposed to simply assuming that instances are independent and identically distributed. To this end, they proposed a transformer-based MIL framework to deal with the whole-slide image classification problem. Their framework used transformer layers to aggregate morphological information and a pyramid position encoding generator to extract spatial information. They also used the Nystrom method to calculate approximated selfattentions, which reduced computational complexity from ğ‘‚( ğ‘› 2 ) to ğ‘‚( ğ‘› ) . Rymarczyk et al. [193] focused on the attention mechanism and revised attention-based MIL pooling (AbMILP), which aggregates information from a varying number of instances. They developed self-attention AbMILP (SA-AbMILP) to model the dependencies between different instances within a bag. They also extended the calculation of attentions by introducing different kernels, which played the same part as the dot product. They evaluated their work on histological, microbiological, and retinal datasets. Yu et al. [195] explored the applicability of ViTs to retinal disease classification in fundus images ( Table 6 ). They developed a MIL-enhanced ViT (MIL-VT) by adding a plug-and-play MIL learning head to the ViT to exploit the features extracted from individual patches.</p>
        <p>Another weakly supervised example is semi-supervised learning, which requires only a small amount of labeled data to exploit knowledge from a large amount of unlabeled data. Luo et al. [201] first combined a CNN and transformer for semi-supervised medical image segmentation. They introduced cross-teaching between the CNN and the transformer, with the prediction of each network used as a pseudo label to supervise the other network. Zhao et al. [202] proposed a context-aware network called 
            <rs type="software">CA-Net</rs> for semi-supervised LA segmentation from 3D MRI. 
            <rs type="software">CA-Net</rs> contains two main modules, a trans-V module that combines a transformer and V-net to learn contextual information, and a discriminator to calculate an adversarial loss for learning the unlabeled data. Xiao et al. [203] used a dual teacher structure involving a CNN and a transformer to guide a student segmentation model.
        </p>
        <p>Successful training of a transformer model relies on large-scale annotated data, which are rarely available in real clinical facilities. The SSL paradigm was created to handle such issue. SSL aims to improve the performance of downstream tasks (e.g., classification, detection, and segmentation) by transferring knowledge from a related unsupervised upstream task ( i.e. , learning of vision concepts), and pretrains the model using self-contained information in the unlabeled data [204] . In practice, training of SLL ViTs generally involves pretraining the model on ImageNet, followed by a fine-tuning step on the target medical image dataset. This can boost the performance of ViTs in comparison with CNNs and enable SOTA accuracy to be achieved [205][206][207][208] .Successful training of a transformer model relies on large-scale annotated data, which are rarely available in real clinical facilities. The SSL paradigm was created to handle such issue. SSL aims to improve the performance of downstream tasks (e.g., classification, detection, and segmentation) by transferring knowledge from a related unsupervised upstream task ( i.e. , learning of vision concepts), and pretrains the model using self-contained information in the unlabeled data [204] . In practice, training of SLL ViTs generally involves pretraining the model on ImageNet, followed by a fine-tuning step on the target medical image dataset. This can boost the performance of ViTs in comparison with CNNs and enable SOTA accuracy to be achieved [205][206][207][208] .</p>
        <p>Truong et al. [198] evaluated the transferability of self-supervised features in medical images. They pretrained features using DINO, a selfsupervised ViT. They used the ViT as a backbone and demonstrated that it could outperform 
            <rs type="software">SimCLR</rs> and SwAV. Park et al. [43] used a public large-scale CXR classification dataset to pretrain the backbone network. The features extracted by the pretrained backbone model were then fed into a ViT to diagnose COVID-19. Jun et al. [129] proposed a selfsupervised transfer learning framework that could better represent the spatial relationships in 3D volumetric images to facilitate downstream tasks. They converted 3D volumetric images into sequences of 2D image slices from three views and fed them into the pretrained backbone network, which consisted of a convolutional encoder and a transformer. The pretraining of the transformer was implemented using masked encoding vectors, which served as a proxy task for SSL. The downstream tasks included brain disease diagnosis, brain age prediction, and brain tumor segmentation, using 3D volumetric images. They also explored a parameter-efficient transfer learning framework for 3D medical images. Wang et al. [197] collected a large public histopathological image dataset to pretrain their proposed hybrid CNN-transformer framework. Moreover, they designed a token-aggregating and excitation module to further enhance global weight attention by taking all tokens into consideration. Sriram et al. [199] explored the applications of transformers for COVID-19 prognosis. They proposed a multiple image prediction model that could take a sequence of images along with the corresponding scanning time as input. To deal with missing COVID-19 images, they used momentum contrast learning, a self-supervised method, to pretrain the feature extractor network. In addition to the features extracted from X-rays, they used continuous positional embedding to add information based on the time-step. The concatenation of features and continuous positional embeddings was fed into the transformer to predict the possibility of an adverse event. Chen et al. [209] showed that ViT using DINO-based knowledge distillation could learn data-efficient and interpretable features in histology images by training various self-supervised models with validation on different weakly supervised tissue phenotyping tasks. Notably, they achieved excellent performance on different attention heads in the ViT while learning distinct morphological phenotypes.
        </p>
        <p>Several studies have focused on model efficiency within the medical imaging field. A natural idea is to simplify the attention mechanism, which demands the largest workload in transformers. Gao et al. [84] proposed an efficient self-attention mechanism and position encoding, which significantly reduced the complexity of the selfattention operation from ğ‘‚ ( ğ‘› 2 ) to approximately ğ‘‚ ( ğ‘› ) . This circumvented the hurdle of transformers requiring huge amounts of data to learn vision inductive bias. The hybrid-layer design could initialize transformers as convolutional networks without the need for pretraining. The aforementioned VOLO proposed by Liu et al. [48] replaced the standard ViTs with Linformer, which performs an internal self-attention mechanism, reducing the original space time complexity of ğ‘‚ ( ğ‘› 2 ) to a smaller complexity of ğ‘‚ ( ğ‘› ) . Li et al. [210] redesigned the transformer block in their TransBTSV2 model, resulting in a shallower but wider architecture compared with conventional transformer-based methods. Inspired by dilated convolution kernels, Wu et al. [106] conducted global self-attention in a dilated manner, enlarging the receptive fields without increasing the patches and thus reducing computational costs. Xu et al. [94] built a multi-scale searching space composed of a multibranch parallel searching block, which connected a CNN and transformer in parallel. They also proposed an efficient resource-constrained search strategy to simultaneously optimize accuracy and costs (e.g., params. and FLOPs) of the model.Several studies have focused on model efficiency within the medical imaging field. A natural idea is to simplify the attention mechanism, which demands the largest workload in transformers. Gao et al. [84] proposed an efficient self-attention mechanism and position encoding, which significantly reduced the complexity of the selfattention operation from ğ‘‚ ( ğ‘› 2 ) to approximately ğ‘‚ ( ğ‘› ) . This circumvented the hurdle of transformers requiring huge amounts of data to learn vision inductive bias. The hybrid-layer design could initialize transformers as convolutional networks without the need for pretraining. The aforementioned VOLO proposed by Liu et al. [48] replaced the standard ViTs with Linformer, which performs an internal self-attention mechanism, reducing the original space time complexity of ğ‘‚ ( ğ‘› 2 ) to a smaller complexity of ğ‘‚ ( ğ‘› ) . Li et al. [210] redesigned the transformer block in their TransBTSV2 model, resulting in a shallower but wider architecture compared with conventional transformer-based methods. Inspired by dilated convolution kernels, Wu et al. [106] conducted global self-attention in a dilated manner, enlarging the receptive fields without increasing the patches and thus reducing computational costs. Xu et al. [94] built a multi-scale searching space composed of a multibranch parallel searching block, which connected a CNN and transformer in parallel. They also proposed an efficient resource-constrained search strategy to simultaneously optimize accuracy and costs (e.g., params. and FLOPs) of the model.</p>
        <p>There have been fewer studies attempting to solve the model efficiency problem in MIA rather than in CV. However, as medical images generally come in large sizes and small quantities, there is an urgent need to solve this problem. Thus, we would like to see more work in this specific research direction.There have been fewer studies attempting to solve the model efficiency problem in MIA rather than in CV. However, as medical images generally come in large sizes and small quantities, there is an urgent need to solve this problem. Thus, we would like to see more work in this specific research direction.</p>
        <p>CNNs were dominant in CV prior to the emergence of ViTs, including in the field of medical image analysis. Much effort has been invested in improving the performance of CNN-based classifiers for both natural and medical images. Several studies have investigated whether CNNbased methods could work on ViTs. Moreover, as ViTs have ranked top among several benchmarks, many studies have focused on performance comparisons between ViTs and CNNs.CNNs were dominant in CV prior to the emergence of ViTs, including in the field of medical image analysis. Much effort has been invested in improving the performance of CNN-based classifiers for both natural and medical images. Several studies have investigated whether CNNbased methods could work on ViTs. Moreover, as ViTs have ranked top among several benchmarks, many studies have focused on performance comparisons between ViTs and CNNs.</p>
        <p>Large-scale datasets are required for to obtain desirable performance with transformers. However, in the medical image analysis field, available images and annotations are limited. To alleviate this problem, many methods have adopted convolutional layers in ViTs to boost performance with limited medical images and have also leveraged the power of transfer learning and SSL. Matsoukas et al. [205] explored whether transfer learning and SSL regimes could benefit ViTs. They conducted several experiments to compare the performance of a CNN ( i.e. , ResNet50) and a ViT ( i.e. , DEIT-S) using different initialization strategies: (1) randomly initialized weights, (2) transfer learning using Ima-geNet pretrained weights, and (3) self-supervised pretraining on the target dataset with the same initialization as in (2). They evaluated these methods on the APTOS 2019, ISIC 2019, and CBIS-DDSM datasets. It can be concluded that standard procedures, e.g., initialization using Im-ageNet pretrained weights and leveraging SSL, can bridge the performance gap between CNN and ViT. Krishnamurthy et al. [211] adopted a transfer learning scheme in both CNNs and ViTs for Pneumonitis diagnosis. They first pretrained their models on ImageNet and fine-tuned the classifier on their private dataset. However, their comparison was based on fine-tuning with frozen backbone layers, which limited the performance of feature extraction when adapted to the target domain. Truong et al. [198] assessed the transferability of self-supervised features in medical imaging tasks. They chose ResNet-50 as the backbone and pretrained it using three self-supervised methods: 
            <rs type="software">SimCLR</rs>, SwAV, and DINO. DINO used ViT as the backbone and consistently outperformed other self-supervised techniques as well as the supervised baseline by a large margin. They proposed a model-agnostic technique, i.e. , dynamic visual meta-embeddings, to combine pretrained features from multiple SSL methods with self-attention.
        </p>
        <p>For the task of multi-scale cell image classification, Liu et al. [212] developed an experimental platform to compare multiple deep learning methods, including CNNs and ViTs. They validated the performance of deep learning models on standard and scaled data by changing the cell aspect ratios of the images. The results suggested that deep learning models, including ViTs, are robust to changes in the cell aspect ratio in cervical cytopathological images. For shoulder implant X-ray manufacturer classification, Zhou et al. [213] compared the performance of various models, including traditional machine learning methods, CNN-based deep learning methods, and ViTs. The results showed that ViTs achieved the best performance in these tasks, and that transfer learning improved ViT by a large margin. Altay et al. [214] aimed to achieve early pre-clinical prediction of AD using MRI. They compared transformers against a baseline 3D CNN model and 3D recurrent visual attention model, and showed that transformers achieved the best accuracy and F1 scores. Adjei-Mensah et al. [215] showed that CNNs outperformed ViTs on low-resolution medical image recognition. Galdran et al. [216] also showed that CNNs outperformed ViTs on diabetic foot ulcer classification in a few-data regime.For the task of multi-scale cell image classification, Liu et al. [212] developed an experimental platform to compare multiple deep learning methods, including CNNs and ViTs. They validated the performance of deep learning models on standard and scaled data by changing the cell aspect ratios of the images. The results suggested that deep learning models, including ViTs, are robust to changes in the cell aspect ratio in cervical cytopathological images. For shoulder implant X-ray manufacturer classification, Zhou et al. [213] compared the performance of various models, including traditional machine learning methods, CNN-based deep learning methods, and ViTs. The results showed that ViTs achieved the best performance in these tasks, and that transfer learning improved ViT by a large margin. Altay et al. [214] aimed to achieve early pre-clinical prediction of AD using MRI. They compared transformers against a baseline 3D CNN model and 3D recurrent visual attention model, and showed that transformers achieved the best accuracy and F1 scores. Adjei-Mensah et al. [215] showed that CNNs outperformed ViTs on low-resolution medical image recognition. Galdran et al. [216] also showed that CNNs outperformed ViTs on diabetic foot ulcer classification in a few-data regime.</p>
        <p>In summary, existing studies have not shown that ViTs can outperform CNNs in all scenarios, particularly in both few-shot and low-resolution medical image analysis. Thus, similar to the case for CV methods, most recent studies have built hybrid models with convolutions.In summary, existing studies have not shown that ViTs can outperform CNNs in all scenarios, particularly in both few-shot and low-resolution medical image analysis. Thus, similar to the case for CV methods, most recent studies have built hybrid models with convolutions.</p>
        <p>Transformers are now transforming the field of computer vision. Also, research using transformers is undergoing rapid growth in the field of medical image analysis. However, most of the current transformerbased methods can be naturally and simply applied to medical imaging problems without drastic changes. Thus, advanced methodologies, e.g. , weakly supervised learning, multi-modal learning, multi-task learning, and model improvement, are rarely explored. Also, only a few studies have focused on general problems of the model, e.g. , parallelization, interpretability, quantification, and safety. These indicate future directions of for medical transformer research.Transformers are now transforming the field of computer vision. Also, research using transformers is undergoing rapid growth in the field of medical image analysis. However, most of the current transformerbased methods can be naturally and simply applied to medical imaging problems without drastic changes. Thus, advanced methodologies, e.g. , weakly supervised learning, multi-modal learning, multi-task learning, and model improvement, are rarely explored. Also, only a few studies have focused on general problems of the model, e.g. , parallelization, interpretability, quantification, and safety. These indicate future directions of for medical transformer research.</p>
        <p>ACT-MOS: abdominal CT multi-organ segmentation; COPLE: COVID-19 pneumonia lesion segmentation; ACDC: automated cardiac diagnosis challenge; MSD-01: Medical Segmentation Decathlon dataset Task01; KCCEE: KvasirACT-MOS: abdominal CT multi-organ segmentation; COPLE: COVID-19 pneumonia lesion segmentation; ACDC: automated cardiac diagnosis challenge; MSD-01: Medical Segmentation Decathlon dataset Task01; KCCEE: Kvasir</p>
        <p>This work was supported by the National Natural Science Foundation of China (Grant No. 62106101), the Natural Science Foundation of Jiangsu Province (Grant No. BK20210180).This work was supported by the National Natural Science Foundation of China (Grant No. 62106101), the Natural Science Foundation of Jiangsu Province (Grant No. BK20210180).</p>
        <p>The authors declare no competing interests.The authors declare no competing interests.</p>
        <p>Kelei He, Junfeng Zhang, and Dinggang Shen led the project. Kelei He, Chen Gan, Zhuoyuan Li, Islem Rekik, Zihao Yin, and Wen Ji wrote the paper. Dinggang Shen, Qian Wang, Yang Gao, and Junfeng Zhang revised the paper.Kelei He, Junfeng Zhang, and Dinggang Shen led the project. Kelei He, Chen Gan, Zhuoyuan Li, Islem Rekik, Zihao Yin, and Wen Ji wrote the paper. Dinggang Shen, Qian Wang, Yang Gao, and Junfeng Zhang revised the paper.</p>
    </text>
</tei>
