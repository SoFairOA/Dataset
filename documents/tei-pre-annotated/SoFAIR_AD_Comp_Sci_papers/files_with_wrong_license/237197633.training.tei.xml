<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T11:37+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>This is a PDF file of an article that has undergone enhancements after acceptance, such as the addition of a cover page and metadata, and formatting for readability, but it is not yet the definitive version of record. This version will undergo additional copyediting, typesetting and review before it is published in its final form, but we are providing this version to give early visibility of the article. Please note that, during the production process, errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.</p>
        <p>Rapid advancements in Technology-Enhanced Learning platforms have shown a tremendous increase in online educational data to yield ample educational repositories (Treasure-Jones et al., 2019) -demonstrating significant impact on Higher Educational Institutions. This rapid increase in educational data is also providing opportunities to optimize users' engagement with technological platforms to enhance the learning experience (Shorfuzzaman et al., 2019). The progression of the accumulated educational data has stimulated the emergence of several research communities, such as learning analytics to predict learners' behavior and providing indicators for optimized policy formulations (Azcona &amp; Smeaton, 2017;Viberg et al., 2018;Capuano &amp; Toti 2019). Educational data, a by-product of the interaction between learners and instructors, has been substantiated as a multidisciplinary field of study, involving researchers from various research disciplines (Xu et al., 2019). This has generated the inclusion of numerous terms associated with the exploration of educational data, such as academic analytics, predictive Journal Pre-proof analytics, and learning analytics. A more recent term formulated is 'educational data science,' which cohesively combines various researchers from different disciplines and backgrounds, bringing them together to work collaboratively on research interests related to educational data (Piety et al., 2014).</p>
        <p>The Artificial Neural Networks (ANNs) are the most prominent practice utilized in the Educational Data Mining (EDM) domain (Coelho &amp; Silveira, 2017). Although there have been issues associated with ANNs, especially when extracting human-interpretable patterns from the predicted results, most of these concerns were resolved in the last decade, with the emergence of Deep ANNs (Coelho &amp; Silveira, 2017;LeCun et al., 2015). Deep learning, evolved from machine learning, and characterized by numerous computational layers, enable the model to learn from examples, patterns (Wang et al., 2011;Nawaz et al., 2012) or events (Ananiadou et al., 2013;Nawaz et al., 2012), superseding the traditional techniques of hand-engineering the features (Poplin et al., 2018).</p>
        <p>Contrary to the increasing rise in deep learning techniques, sufficient evidence of Deep ANNs in EDM and learning analytics literature is not available. Coelho &amp; Silveira (2017) conducted a systematic literature review to investigate the evidence found in learning analytics and deep learning studies. They identified student performance (Guo et al., 2015;Okubo et al., 2017;Wang et al., 2017), student assessment (Li et al., 2016) and hand writing recognition (Gross et al., 2015), as being some of the areas where deep learning was deployed, proving better than the baseline models. Numerous models have been explored in the learning analytics research paradigm, however identifying the significance of deep learning in the learning analytics domain is still in its infancy, with studies on the adoption of this technique emerging in the last few years.</p>
        <p>Learning analytics comprises of several facets with the inclusion of gathering, assembling, examining and analysing students' information to enhance understanding of the learning environment, resulting in optimized learners' and instructors' performance (Siemens, 2010;Siemens &amp; Long, 2011). It is also interpreted as emphasizing on individually assessing the learners' performance and its corresponding impact on the institutions performance (Baker &amp; Inventado, 2014;Daniel, 2017;Romero &amp; Ventura, 2010). Additionally, it deals with formulating policies and developing strategies in order to elevate the ability of institutions at an academician level, stimulating effective decision-making (Elias, 2011;Leitner et al., 2017). It aids in inferring patterns from the educational data to not only optimize students' performance, but also to provide supplementary support in teaching mechanisms by tailoring teaching methods, holistically improving the learning environment (Khalil &amp; Ebner, 2015). In higher education, learning analytics is defined consistently with notions of academic analytics, aiding institutions in their financial strength by reducing attrition rates, improving learning outcomes by considering learner's behavior, recommending corrective policies for instructors that eventually yield in establishing a stable institute and exercising a suitable resource allocation method. Student retention has become a standard strategic imperative for institutions, and the learning analytics phenomena cumulatively aids in retaining students, consequently resulting in accumulated graduation rates (Palmer, 2013). Academic and learning analytics consistently overlap in formulating the 'Educational Analytics' paradigm -learning analytics is associated with the learner's experience and academic analytics implicitly incorporates the overall institute and its performance (Waheed et al., 2018). Moreover, a semantic mapping of deep learning with educational data science is presented in Fig. 1 Multiple facets of learning analytics, encompassing learning excellence and monitoring operational services, enable the integration of educational repositories, affording higher education to achieve effective decision making, eventually aiding an institute in maintaining a stable performance (Van Barneveld et al., 2012). Another dimension of this domain is the prediction of student performance, monitoring their actions to gain insight into their patterns of accessing the system and identifying potentially weak students or those at risk of failing. Aljohani et al., (2018) discussed the need of learning analytics applications, tailored for each course, in order to have a better understanding of a learners cognition, pedagogy intention and online behavior. Learning analytics in mobile learning is a relatively new emergent area, formulating policies for assessing the behavior of mobile learners (Aljohani et al., 2012). Aljohani &amp; Davis (2013) developed an app to record feedback from mobile learners after each lecture, which consequently assisted in examining the learner's behavior.</p>
        <p>With the growth of internet, online education has become one of the rising key phenomena providing sufficient repositories for student and learner information. This allows analysis for predicting patterns in an educational setting, defining association between the stakeholders involved and optimizing the learning environment. Online systems included, but not limited to, Learning Management Systems (LMS), Course Management Systems (CMS), Massive Open Online Courses (MOOCS), Virtual Learning Environments (VLE), Intelligent Tutoring systems (ITS) and other web-based educational systems contribute in generating digital footprints that can be examined to assess the prospective behavior of learners, analysing activities of successful and at-risk students, providing corrective strategies based on learner's performances, consequently assisting instructors in improving the pedagogical methods (Casquero et al., 2016;Fidalgo-Blanco et al., 2015).</p>
        <p>Learning analytics assists in providing automated, real time opinions and recommendations for constructive pedagogical strategies through multiple learning analytics platforms (such as learning analytics 
            <rs type="software">dashboard</rs>, LMS visualization systems). These feedback systems aid in constructing a more robust platform to measure, examine and predict student involvement, contributing self-regulated learning and driving motivational factors to regulate successful goal achievement (Corrin &amp; de Barba, 2014;Mah, 2016;Siemens et al., 2011;Siemens &amp; Long, 2011). Corrin &amp; de Barba (2014) initiated a study to interpret the influence that feedback provided via learning analytics 
            <rs type="software">dashboards</rs> had on students' performance, motivational drive and strategy development for course work. These applications induce a positive drive in students learning, consequently impacting performance (Arnold &amp; Pistilli, 2012).
        </p>
        <p>The overall research agenda for this study is to measure the effectiveness of VLEs in predicting students' performance, for timely intervention by the instructors, providing suitable pedagogical support. Moreover, determining learners' behavior and various patterns associated with them, the performance of the students can be assessed by the extent of their interaction with the virtual environment. More specifically, students' interactions with the deployed learning management tools generate an abundance of clickstream data, consequently reflecting their participation with the learning environment. The current students' performance is indicative of their legacy data, such as their past performance in assessments and quizzes. The objectives to be addressed in this study are as follows:</p>
        <p>ÔÄ≠ Determining the effectiveness of deep learning models in predicting students' academic performance for the following categories: 'withdrawn-pass', 'pass-fail', 'distinction-pass' and 'distinction-fail' using VLE dataset. ÔÄ≠ Exploiting clickstream data generated through students interaction with the virtual learning management system to assess their performance. ÔÄ≠ Ascertaining the effectiveness of deep learning models for the early prediction of students' performance.</p>
        <p>The proposed contribution of this study, as derived from the above research objectives, is essential in identifying the effectiveness of the learner's digital footprints in the VLE, including their legacy data and ascertaining interaction patterns of students' at-risk of failure or withdrawal. In this study, we explored a well-known freely available dataset, called Open University Learning Analytics (OULA) provided by Open University, to analyse student behavior and the impact of students' interaction with the VLE on their performance. The rest of the paper is organized as follows. Section 2 briefly describes existing educational data science literature in predicting students' performance in an online environment. Section 3 describes the dataset and discusses the methodology. Section 4 presents the experimental results and discussion. Finally, section 5 summarizes the paper by providing concluding remarks and proposing future directions.</p>
        <p>The research area related to students' performance prediction is multidimensional and can be explored and analysed via multiple perspectives, including early prediction of dropouts and withdrawals in an on-going course, analysing the intrinsic factors impacting their performance and deploying statistical techniques to measure the performance of students. Various data mining techniques are deployed on educational datasets to predict students' performance, assessing slow learners and dropouts (Abu-Oda &amp; El-Halees, 2015;Kaur, 2015;Hardman et al., 2013;Yadav, 2012). The techniques employed on these learning analytics datasets aid in data-driven decision making (Waheed et al., 2018). Early prediction is a new phenomenon in this domain, encompassing methods to timely assess the students in order to retain them, by suggesting suitable corrective strategies and policies, subsequently managing and reducing attrition rates. In the literature, there has been substantial debate on the subject of student retention in MOOCs.</p>
        <p>Numerous studies have emphasized the identification of the factors contributing to students' dropout rates (Aulck et al., 2016;Fei &amp; Yeung, 2015;Tan &amp; Shao, 2015;Xing, 2016). Various factors, such as the attribute of time, motivational factors, non-existent interactivity between students-instructors, lack of knowledge of the course pre-requisites and effectiveness of the course content, are associated with the early withdrawal of students in MOOCs (Hone &amp; Said, 2016;Khalil &amp; Ebner, 2014). Jaggers &amp; Xu (2016) conducted a study identifying the major Journal Pre-proof factors influencing student performance in an online environment and concluded studentinstructor interaction to be the foremost attribute positively impacting one's performance.</p>
        <p>Different attributes are associated with the prediction of students' performance. Shahiri &amp; Husain (2015) conducted a systematic literature review to observe the characteristics substantially contributing to the prediction of class-room performances. Cumulative grade point (CGPA) and assessments (such as assignments and quiz marks) were considered to be the two primary attributes to assess students' performance (Elakia &amp; Aarthi, 2014;Mayilvaganan &amp; Kalpanadevi, 2014;Papamitsiou et al., 2014;Tucker et al., 2014). Another perspective encompasses factors of students' legacy data (such as past performances in previous assessments/entry test etc.) and demographics in being significant contributors to assessing performances (Leitner et al., 2017). Furthermore, another study employed family characteristics, such as family expenditure, income and students personal information, to assess the impact on their performance (Daud et al., 2017). They concluded that excessive rental expenditures and health expenses impacted the overall environment ultimately influencing student's performance.</p>
        <p>Emotional stability, a student's inherent attribute, was considered to be an important predictor of their performance.</p>
        <p>Several studies deploy machine learning techniques to analyse student behavior and predict students at-risk of a failure (Costa et al., 2017;Hassan et al., 2019;Wasif et al., 2019). In the existing literature, another array of studies follow a sequential approach to convert the course duration into a week-wise format and assess student performance according to their interaction with the learning environment. Marbouti &amp; Diefes-Dux (2015) utilised machine learning techniques to predict students at-risk of a failure in the 2 nd , 4 th and 9 th week of their first year of engineering. Their dataset comprised of attendances, quizzes and assignments, with the inclusion of an additional attribute of mid-term exams in the 9 th week. Deploying a logistic regression model, they achieved an accuracy of 98% by the 9 th week. Additionally, in a study, at-risk students were predicted by deploying various data mining techniques, including Support Vector Machines (SVM), Na√Øve Bayes Classifier, Decision Tree, K-Nearest Neighbor and Multi-Layer Perceptron, to identify the best prediction modeling method. Logistic Regression was employed as the baseline model (Chui et al., 2018;Marbouti et al., 2016). Assessment engagement pattern is considered to be another parameter that effectively captures the behavior of students and induces a positive impact on their performance (Hussain et al., 2018;Jung &amp; Lee, 2018). In contrast to the clickstream behavior that only captures 10% variance of student learning dynamics, assessment related activities enable to tap a much broader variation of student behavior (Tempelaar et al., 2015). Similarly, the involvement in the discussion forums was found to be positively associated with student success (Bonafini et al., 2017).</p>
        <p>Deploying deep learning techniques on learning analytics to predict successful and at-risk students is rather a new area of research. Deep learning employs techniques that encompass constructing a model comprising of multiple layers to learn representations from raw data. This representation learning consists of multiple layers, where each layer transforms the representation to a more abstract form for the next layer (LeCun et al., 2015). Corrigan &amp; Smeaton (2017) predicted student success via a VLE by including the number of times a student interacts with the environment. They deployed Recurrent Neural Network's (RNN) variation Long Short Term Memory (LSTM), to predict the success of students based on their interaction with the 
            <rs type="software">Moodle</rs> based learning environment. The results were evaluated using Random Forests, and LSTM outperformed it by 13.3% of the variance of the model, as opposed to 8.1%. Okubo et al. (2017) predicted student success by utilizing various features extracted from Kyushu University's learning system named M2B. They collected 108 student learning logs for an 'Information Science' course. The learning logs comprised of a feature set of weekly quiz, attendance, notes on the lectures delivered, sides view and book markers. They deployed the RNN model with hidden layers of LSTM to predict students grades based on their interactions and defined features. The results were compared with multiple regression analysis and the proposed model surpassed the regression models through early prediction of the grades. Fei &amp; Yeung (2015) employed different machine learning techniques to predict student dropout rates in two courses from Massive Open Online Courses (MOOCS). Since the data is time dependent, they viewed it as a sequence classification problem. MOOCS are subjected to high attrition rates; hence apart from baseline models such as SVM and Logistic regression, they implemented Input Output Hidden Markov Model (IOHMM), RNN and LSTM to identify the best technique among these.The feature set incorporated lecture view, lectures downloaded, quiz attempts, numbers of access, forum activities, forum views and number of times students commented on forums. After deploying the above mentioned techniques, they found that a combination of RNN and LSTM had the highest accuracy. Recently, in the existing literature many studies have explored the OULA dataset to leverage the power of machine learning in analysing and predicting student performance. Table 1 presents a conclusive comparison of the studies using OULA dataset in performance and courses evaluation. Overall, in the existing literature several studies leverage the ability of machine learning techniques to predict student performance through various parameters of student engagement. However, such studies either emphasize on predicting at-risk students or analyse the dropouts. In this study, we intend to analyse student performance through the initiation of different performance-related categories. The investigation of such categories will assist in identifying behavioral patterns of various performance categories, providing opportunities for educational stakeholders to support students in need.
        </p>
        <p>This section presents an overview of dataset, pre-processing techniques and details of deep learning model employed.</p>
        <p>The dataset is retrieved from the openly accessible 
            <rs type="software">OULA</rs> which provides demographic, clickstream behavior and assessment performance of 32,593 students over a course period of 9 months, from 2014-2015 (Kuzilek et al., 2017). It is comprised of 7 courses, referred to as 2). Similarly, to predict students with distinctions, they are employed on the model with 'pass' and 'fail'. Overall four categories of the dataset are computed with respect to the class labels. Each category represents a binary classification system, to address the objectives of the study.
        </p>
        <p>An array of analytical techniques are employed on this dataset to predict students' performance by identifying students at-risk of a course failure, early prediction of at-risk and withdrawal students and identifying patterns of students passing with distinction. A two-fold approach is utilized to represent the OULA dataset to encompass the objectives of our study. Firstly, demographics, assignments and total clicks of each student are computed to address the objectives of this study. Secondly, each module is divided into quartiles, to provide early intervention of the students at-risk of failure or withdrawal. A detailed description of all the features employed in the two approaches is described below in the preprocessing section.</p>
        <p>The study follows a two-fold analysis: a) mining student activity with the VLE portal and static demographics data; b) mining quarterly clickstream data for each student in each new course. In order to evaluate students' performance, the OULA dataset is analysed as follows to obtain the objectives of this study.</p>
        <p>The first approach mines the data that includes the students' demographics and VLE portal information, such as the number of clicks for each activity type provided in the dataset.</p>
        <p>Moreover, students are able to access the VLE a few weeks before the start of the module; hence the number of clicks for each activity type, before the module started, is considered another range of features. Similarly, the data relevant to the assessments and number of late assignments submitted are also included in the dataset. A list of these 54 features is provided in of the data points. is the computed low rank estimation of the data matrix implying ùëã that the data points in 'X' matrix are compressed into a r-dimensional sub-space. In terms of centered data SVD corresponds to Principle Component Analysis (PCA), which can be defined as reducing the estimation error (Jolliffe &amp; Cadima, 2016). However, in terms of non-centered data, such as in this case where the data is sparse, SVD instead of the covariance matrix, is expressed as the Eigen-decomposition of the matrix X T X.</p>
        <p>A list of the 30 features (F1-F30) selected by the sparse reduction technique (SVD) is provided in Table A-1 in Appendix A, where each feature is listed with respect to its significance on the student's performance. The demographics data is available in the OULA dataset. However, the VLE portal information, constituting of the overall total clicks on each activity, is computed through the available raw OULA dataset.</p>
        <p>The second approach in this study is the early intervention for all the categories (as provided in Table 2). To address early intervention, modules are divided into quarters. The duration of a module is nine months, so each module is divided into 4 quartiles and for each quartile the clickstream data of each activity is computed. Clickstream data refers to the interaction behavior of the students with the online learning platform. For each quarter temporal features are computed, that are updated with respect to each quarter. This forms another array of derived Journal Pre-proof features, in order to assess the most significant time in a module affecting a student's performance, which can further be utilized for early intervention by the academia.</p>
        <p>Each module quarter contains the computed clickstream data of the temporal longitudinal activities, which vary with respect to each quarter. Each new quarter is computed by adding clickstream data of its previous quarters, such that Q2 will contain the clickstream activities of Q1 and Q2 cumulatively. Similarly, Q3 and Q4 are computed, with each of them including the clickstream information of their previous quarters. The activities in OULA VLE have been discussed previously in the dataset section. The number of features in each quarter corresponds to the number of activities in the VLE; however, the clickstream information of each quarter varies.</p>
        <p>Finally, the Deep Artificial Neural Network or Deep ANN classification model is employed to learn the predictive function of predicting students' at-risk of failure, those likely to withdraw from their courses, early prediction of withdrawal students, and determining the students who outperform others with distinction.</p>
        <p>Deep learning methods are referred to as representation-learning methods constituting several layers of non-linear modules. This enables the system to be proficient enough to learn complex functions, making it robust enough to be sensitive to intricate and minute specificities. In contrast to the statistical methods, Deep ANNs facilitate generalization, which enables them to correctly infer hidden patterns from the data, assisting in making data driven assumptions (Montavon, Samek, &amp; M√ºller, 2018). The network learns from the examples in the training data, thus increasing the training split leads to a more robust accuracy (Nielsen, 2015). The stack of nonlinear layers between the input and output layers are referred to as hidden layers, weights are adjusted in the layers through stochastic gradient, to calculate the error computed in classifying and predicting correct answers. An ANN with multiple non-linear layers, also referred to as a Multiple Layered Perceptron, is capable of implementing complicated input functions (LeCun et al., 2015).</p>
        <p>An ANN is composed of inter-connected objects referred to as processing units. Each unit takes the weighted sum of inputs and produces an output. An ANN has a layered architecture where neurons are assembled in consecutive layers and output of each layer is fed to its successive Journal Pre-proof layer. A combination of non-linear functions is deployed on the input, hidden and output layers.</p>
        <p>In binary classification the output layer is activated with 'sigmoid' function, because sigmoid squashes the values into 0 or 1. For a supervised learning problem, the weight vector 'w' for the 'n th ' unit at time instant 't' is defined as shown in Eq. 1. that correspond to a robust prediction (Da Silva et al., 2017).</p>
        <p>In this study, Deep ANN is employed to predict the students' performance, in terms of identifying students' at-risk of failure in their modules. After feature selection, min / max scaling is employed to normalize the data. The data is split into train test and is fed to the neural network in the form of a feature vector. A train-test split of 70% is performed with 30% reserved for the validation of the model. Fig. 2 illustrates the proposed architecture level system of the ANN. For each of the four categories defined in Table 2, extensive experimentations were conducted to select the appropriate parameters providing optimal results. After rigorous experiments ANN with three hidden layers is implemented with each hidden layer constituting of 50, 20 and 10 neurons respectively. Moreover, each hidden layer is activated with either 'relu' or 'tanh' function and the output layer with 'sigmoid' function. Further, the experimentation details are provided in the next section.</p>
        <p>The study follows a two-fold analysis: a) over-all data including student activity with the VLE portal and demographics data; b) quarterly clickstream data for each student in each new course.</p>
        <p>For each of these cases different experiments were performed with different parameters, however optimized results were obtained by deploying the deep ANN model with three hidden layers of different neurons, a batch size ranging from 32 to 64 and either 'adam' or 'rmsprop' as optimizers.</p>
        <p>Journal Pre-proof</p>
        <p>To predict students' performance, considering their demographics and overall portal VLE information, 30 features, through sparse reduction technique were selected from a total of 54 features, provided in Appendix A. From these 54 features F1-F30 were selected through sparse reduction technique. Table 3 represents the evaluation results of our Deep ANN with the baseline models Support Vector Machine (SVM) and Logistic Regression (LR) where cross validation was performed on the data. Each method was executed multiple times with a random train-test split; these results were then averaged and reported in Table 3. Due to the consistency in the existing literature, SVM and LR were selected as baseline models.</p>
        <p>The problem of predicting at-risk students was converted to a binary classification problem by defining two classes 'pass' and 'fail'. SVM with a rbf kernel and deep ANN with three hidden layers of 50, 30 and 15 neurons and a batch size of 64 produced optimal results. It can be observed that Deep ANN yields a better accuracy in predicting students' at-risk of failure. Therefore, according to the features provided in Table A-1 in Appendix A, Deep ANN produces better predictions by yielding an accuracy of 84% (see Table 3).The significant features associated with the desired student performance are illustrated in the heat-map (part (a) of Fig. 3). It can be observed that demographic features, previous education history, legacy data of students including assessments submission and the overall activity of a student in a module are significantly impacting student's performance.</p>
        <p>To predict students with distinction, the problem was considered a binary classification problem by defining two sets of categories, a) 'distinction' and 'pass' b) 'distinction' and 'fail'. For 'distinction-pass', a deep ANN with hidden layers of 50, 25, 10 neurons, batch size of 32 and tanh as activation function provided optimal results. As depicted in Table 3, ANNs accuracy in 'distinction-pass' category does not have a significant difference from other baseline models.</p>
        <p>This may be attributed to the class imbalance problem; also another justification can be that both classes do not have significant differences in their patterns. In the 'distinction-fail' category deep ANN yields more accurate results than other baseline models, as shown in Table 3. Parameters were tuned by setting hidden layers of 50, 40 and 20 neurons and 32 as batch size. The significant features associated with the desired student performance are illustrated in the heatmap (parts (b) and (c) of Fig. 3). To predict 'distinction' instances from failures, students' portal information is observed to be positively associated with the performance, whereas demographic features including geographical region and education history are seen to be negatively associated with performance.</p>
        <p>Journal Pre-proof</p>
        <p>To predict 'withdrawal' instances, 'withdrawals' and 'pass' are the two classes that were defined. The pattern of withdrawals will distinguish from 'pass' instances, since withdrawals tend to leave/drop out of their modules. Parameters for deep ANN were tuned with 50, 50 and 20 neurons in the three hidden layers, 64 batch size and relu as activation function. The evaluation results show an accuracy of 94% in predicting the two aforementioned class labels, provided in Table 3. Comparing the deep ANN results with SVM and LR, deep ANN tends to give a higher accuracy in predicting withdrawals. The significant features associated with the desired student performance are depicted in the heat-map (part (d) of Fig. 3). In case of withdrawals, the activities before the initiation of the module are observed to be significantly impacting the performance. Students interested in registering in a particular module are more likely to be active on the portal before the module starts. It can be observed that demographic features, previous education history, legacy data of students including assessments submission and the overall activity of a student in a module, are significantly impacting the performance of students.</p>
        <p>For each quarter, the four categories corresponding to at-risk students, distinction students and withdrawals were predicted through the deep ANN classifier. The detailed analysis for each of these quarters is discussed below and provided in the Table 4.</p>
        <p>The problem of predicting at-risk students was converted to a binary classification problem by defining two classes 'pass' and 'fail'. For each quarter, at-risk students were predicted, in order to analyse the improvement in our model, insinuating early prediction of students' at risk of failure.</p>
        <p>Fig. 4 illustrates the accuracy for each of the quarters. It can be observed that the best accuracy is achieved in the last quarter that cumulatively integrates the overall clicks for each activity.</p>
        <p>However, 2 nd and 3 rd quarters also do not perform poorly; they too reach an accuracy of above 80%. Therefore, our classifier gives an accuracy of 81-86% for early prediction of at-risk students from the initial quarter to the final one, respectively.</p>
        <p>To predict the students outperforming others with distinction, the problem was again converted to a binary classification by defining two sets of this problem; a) 'distinction-pass' b)</p>
        <p>'distinction-fail'. We show that for the category 'distinction-pass', a major change or rise in the accuracy for all the quarters is not observed (see Fig. 4). The accuracy throughout the quarters rather remains stagnant, with a little difference in the last quarter (Q1-4), implying that a distinct decision boundary in the case of 'distinction' and 'pass' is not available. This may be attributed to the class imbalance problem, because the 'distinction' instances are quite scarce as compared to 'pass' instances. If 'distinction' instances are increased, then a distinct pattern between such students may be visualized.</p>
        <p>For early intervention in predicting withdrawals, the problem was considered a binary classification by deploying 'withdrawals' and 'pass' as two defined class labels. A distinct rise in accuracy, with respect to each of the quarters, is observed, with the last quarter reaching an overall accuracy of 93%. Fig. 4 demarcates a discrete pattern in the prediction of 'withdrawal' instances from 'pass' instances. It can be inferred that the model constantly learns the patterns of withdrawals for each new quarter, with accuracy ranging from 78% to 93%. Owing to the class imbalance problem, the 'distinction-pass' category does not yield substantial results and a clear demarcation line, in terms of improved accuracy between quartiles, is thus missing for this case. In order to evaluate the quarterly results, the last quartile encompassing data from Q1-4 was compared with SVM and ANN, details provided in Table 5. As illustrated in Fig. 7, the proposed model yields better accuracy compared to baseline models.</p>
        <p>This research contributes in predicting students' performance through a deep ANN, by ascertaining significant features impacting a student's academic performance. The student performance for four classes 'pass', 'fail', 'distinction' and 'withdrawn' is analyzed by devising each problem into binary classification and converting them to four categories; 'pass-fail', 'distinction-pass', 'distinction-fail' and 'withdrawn-pass'. The study follows a twofold analysis for each of these categories, where firstly the students' demographics and overall VLE portal information is gathered and feature construction is performed. Significant features for each of Pre-proof these categories, having an impact on the student performance, are examined. For students outperforming others with distinction, their age, region and disability are observed to be negatively associated with their performance, implying that students living in the rural areas may have hindrances or connectivity issues in accessing the VLE and hence the negative association with their performance. Personal characteristics impact a student's performance, consequently yielding to significant decisions regarding one's education (Daud et al., 2017). For students atrisk of failure and withdrawal, their overall studied credits, highest education achieved till date and region were found to be significantly impacting their performance. The factor of age is also an important determinant for withdrawals, with an increase in age demonstrating a positive association with their performance, insinuating stability in the mindset of mature students as compared to youngsters. Moreover, for each of these categories the portal information collected after the module initiation was found to be significantly associated with an individual's performance, implying that active participation during a module is positively associated with performance, in contrast to the activities prior the module which are found to be insignificant determinants of performance. Moreover, for both pass-fail and withdrawan-pass categories students actively clicking on the previous week's content, to access previous lectures and material, were found to demonstrate a positive influence on performance. This factor highlights the crucial significance of the learning pattern behavior of students. The model predicts the students at-risk of failure by an accuracy of 88%, proving slightly better than other baseline models. The deep learning model correctly classifies at risk students with an accuracy of 88-89%. Similarly, 'withdrawn' instances are classified accurately with an accuracy of 94-95%.</p>
        <p>Also students with 'distinction' are predicted by the model with an accuracy of 86%, proving slightly better than other baseline models.</p>
        <p>In the second analysis, quarterly clickstream data for each student is computed for early intervention by the university. In this analysis, temporal features are computed to timely predict the students' at-risk of failure, those that are likely to withdraw and students achieving a distinction who outperform others in achievement. For early prediction of students' at-risk of failure, quarterly data shows an accuracy of 77% after the first quartile which gradually rises to 88% with additional quarterly clickstream information. Similarly, in case of 'withdrawn' instances, a rise in accuracy from 78% to 93% is observed, suggesting the effectiveness of the deep learning model for early interventions in order to resolve student issues and motivating them to continue their studies. In case of analysing distinction students, a distinct boundary is not observed that discretely demarcates between distinction and other instances. This can be attributed to the class imbalance problem, since 'distinction' instances are few in number compared to others which hinders the performance of the model. However, in the case of the 'distinction-fail' category, the model's accuracy slightly increases with the data of the first two quarters and an increase is observed in the third quarter that relatively stagnates in the last quarter, suggesting the effectiveness of the model in the third quarter.</p>
        <p>The results achieved by the study can be utilized for constructive and formative pedagogical guidelines. The early prediction of student performance will enable administrative and decision committees to adopt a pragmatic approach for timely interventions with students and influencing them in a positive manner by providing appropriate recommendations and counseling. Such an early prediction, during the academic year, will help identify weak students, offering them additional support in the learning tasks. This study provides emphasis on the efficacy of ANN for devising data-driven decision making policies, addressing issues experienced by students and consequently assisting an institution in maintaining their academic career.</p>
        <p>This study presents a contribution to knowledge in early prediction of students at-risk of low performance, determining students likely to withdraw from modules and ascertaining significant features that enable a student to outperform others. Results reveal demographic characteristics and student's clickstream activity, after the module initiation, as having a significant impact on student performance. The participation of students with the learning environment before the modules begin has no association with their performance. This study also determines the effectiveness of the deep learning model in the early prediction of student performance, enabling timely intervention by the university to implement corrective strategies for students support and counselling. Such studies will facilitate institutes in formulating student support committees for their provision and benefits, thus helping an institute in maintaining its decorum and productivity. Due to the class imbalance problem in 'distinction' instances, a discrete pattern for such students was not observed, a limitation of our study. However, demographic and geographic characteristics tend to significantly impact performance. The performance evaluation model shows a sensitivity of 69%, a precision of 93% and overall accuracy of 88% in predicting at-risk students; a sensitivity of 86%, a precision of 96% and overall accuracy 93% in predicting early withdrawals. Similarly, while ascertaining 'distinction' students from 'fail', a sensitivity of 74%, and precision of 81% and overall accuracy of 85% is achieved.</p>
        <p>Overall, the results demonstrate the effectiveness of the deployed techniques in evaluating the early prediction of students. Such data-driven studies are required to assist higher education in the formulation of a learning analytics framework, contributing in their decision-making process.</p>
        <p>An in-depth study is required to evaluate the significance and impact of all the activities provided in the OULA dataset. In the future we intend to investigate activity-wise importance and determine activities having an influential impact on the performance by mining textual data (Thompson et al, 2017;Shardlow et al., 2018) pertaining to students' feedback by employing natural language processing (Batista-Navarro et al., 2013) and advance deep learning models (Jahangir et al., 2017). This will enable to formulate a discrete pattern of students belonging to a certain performance category, which in the future can facilitate educational stakeholders to develop requisite pedagogical policies and guidelines. Similarly, a more detailed analysis of the day-to-day activities, for each student, will enable a more intensive exploration of such behaviors. Such data-driven studies are required for the formulation of appropriate pedagogic instructional committees to provide support to students, facilitating the higher education in their decision-making process and devising corrective policies for students' retention.</p>
        <p>1: Educational data, tools and technologies</p>
        <p>BC DualPane before course clicks on the information on site and activity related to that information</p>
    </text>
</tei>
