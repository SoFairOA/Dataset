<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-13T15:24+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Clustering algorithms by minimizing an object function share a clear drawback that the number of clusters need to be set manually. Although density peak clustering is able to seek the number of clusters, it suffers from memory overflow when it is used for image segmentation because a moderate-size image usually includes a large number of pixels leading to a huge similarity matrix. To address the issue, here we proposed an automatic fuzzy clustering framework (AFCF) for image segmentation. The proposed framework has threefold contributions. Firstly, the idea of superpixel is used for the density peak (DP) algorithm, which efficiently reduces the size of the similarity matrix and thus improves the computational efficiency of the DP algorithm. Secondly, we employ a density balance algorithm to obtain a more robust decision-graph that helps the DP algorithm to achieve fully automatic clustering. Finally, a fuzzy c-means clustering based on prior entropy is used in the framework to improve image segmentation results. Because the spatial neighboring information of both the pixels and membership are considered, the final segmentation result is improved effectively. Experiments show that the proposed framework is not only able to achieve automatic image segmentation, it also provides better segmentation results than state-of-the-art algorithms.Clustering algorithms by minimizing an object function share a clear drawback that the number of clusters need to be set manually. Although density peak clustering is able to seek the number of clusters, it suffers from memory overflow when it is used for image segmentation because a moderate-size image usually includes a large number of pixels leading to a huge similarity matrix. To address the issue, here we proposed an automatic fuzzy clustering framework (AFCF) for image segmentation. The proposed framework has threefold contributions. Firstly, the idea of superpixel is used for the density peak (DP) algorithm, which efficiently reduces the size of the similarity matrix and thus improves the computational efficiency of the DP algorithm. Secondly, we employ a density balance algorithm to obtain a more robust decision-graph that helps the DP algorithm to achieve fully automatic clustering. Finally, a fuzzy c-means clustering based on prior entropy is used in the framework to improve image segmentation results. Because the spatial neighboring information of both the pixels and membership are considered, the final segmentation result is improved effectively. Experiments show that the proposed framework is not only able to achieve automatic image segmentation, it also provides better segmentation results than state-of-the-art algorithms.</p>
        <p>C LUSTERING, grouping the objects of a dataset into meaningful subclasses, is one of the most popular research topics, since it is a useful tool for data mining [1], machine learning [2], and computer vision [3]. With the rapid development of intelligent technologies, automated knowledge discovery based on clustering becomes more and more important in these years. Although a large number of clustering algorithms have been successfully used in image segmentation and data classification [4], [5], it is still a challenging topic because it is difficult to achieve automatic clustering and to provide fine results for image segmentation. Image segmentation algorithms based on clustering have two advantages. Firstly, they are able to achieve unsupervised image segmentation without labels. Secondly, they have a better robustness than other image segmentation algorithms such as active contour models [6], graph cuts [7], random walkers [8], region merging [9], etc., since they require fewer parameters. Finally, clustering has a clear advantage on multi-channel image segmentation because it is easy to apply clustering algorithms to high-dimensional data classification. Inevitably, clustering has some disadvantages for image segmentation as well. On the one hand, it is sensitive to noise because the local spatial information of pixels is missed. On the other hand, it takes a long running time when it is used for high-resolution images, as repeated calculations and an iterative optimization are required for the same pixels.C LUSTERING, grouping the objects of a dataset into meaningful subclasses, is one of the most popular research topics, since it is a useful tool for data mining [1], machine learning [2], and computer vision [3]. With the rapid development of intelligent technologies, automated knowledge discovery based on clustering becomes more and more important in these years. Although a large number of clustering algorithms have been successfully used in image segmentation and data classification [4], [5], it is still a challenging topic because it is difficult to achieve automatic clustering and to provide fine results for image segmentation. Image segmentation algorithms based on clustering have two advantages. Firstly, they are able to achieve unsupervised image segmentation without labels. Secondly, they have a better robustness than other image segmentation algorithms such as active contour models [6], graph cuts [7], random walkers [8], region merging [9], etc., since they require fewer parameters. Finally, clustering has a clear advantage on multi-channel image segmentation because it is easy to apply clustering algorithms to high-dimensional data classification. Inevitably, clustering has some disadvantages for image segmentation as well. On the one hand, it is sensitive to noise because the local spatial information of pixels is missed. On the other hand, it takes a long running time when it is used for high-resolution images, as repeated calculations and an iterative optimization are required for the same pixels.</p>
        <p>To address the first shortcoming, a simple idea is to incorporate local spatial information into objective functions to improve the robustness of algorithms to noise, such as fuzzy c-means (FCM) clustering algorithm with spatial constraints (FCM S) [10], FCM S1/S2 [11], fuzzy local information c-means clustering algorithm (FLICM) [12], neighborhood weighted FCM clustering algorithm (NWFCM) [13], the FLICM based on kernel metric and weighted fuzzy factor (KWFLICM) [14], and deviation-sparse fuzzy c-means with neighbor information constraint (DSFCM N) [15], etc. Although these improved algorithms can obtain better segmentation of images corrupted by noise, they have two limitations. One is that they need a longer running time than conventional fuzzy c-means clustering algorithm due to the high computational complexity. Moreover, the running time is much worse when these algorithms are used for color image segmentation, for the spatial neighboring information is calculated in each iteration. The other one is that these algorithms employ a fixed neighboring window for each pixel in an image, which leads to a poor segmentation result. For this problem, an instinctive idea is to employ adaptive neighboring information to improve segmentation results. Liu et al. [16] improved FCM algorithm by integrating the distance between different regions obtained by mean-shift [17] and the distance of pixels into its objective function. However, as the algorithm employs adaptive neighboring information, its computational complexity is still very high, which limits its practicability in image segmentation.To address the first shortcoming, a simple idea is to incorporate local spatial information into objective functions to improve the robustness of algorithms to noise, such as fuzzy c-means (FCM) clustering algorithm with spatial constraints (FCM S) [10], FCM S1/S2 [11], fuzzy local information c-means clustering algorithm (FLICM) [12], neighborhood weighted FCM clustering algorithm (NWFCM) [13], the FLICM based on kernel metric and weighted fuzzy factor (KWFLICM) [14], and deviation-sparse fuzzy c-means with neighbor information constraint (DSFCM N) [15], etc. Although these improved algorithms can obtain better segmentation of images corrupted by noise, they have two limitations. One is that they need a longer running time than conventional fuzzy c-means clustering algorithm due to the high computational complexity. Moreover, the running time is much worse when these algorithms are used for color image segmentation, for the spatial neighboring information is calculated in each iteration. The other one is that these algorithms employ a fixed neighboring window for each pixel in an image, which leads to a poor segmentation result. For this problem, an instinctive idea is to employ adaptive neighboring information to improve segmentation results. Liu et al. [16] improved FCM algorithm by integrating the distance between different regions obtained by mean-shift [17] and the distance of pixels into its objective function. However, as the algorithm employs adaptive neighboring information, its computational complexity is still very high, which limits its practicability in image segmentation.</p>
        <p>For the second shortcoming, because the number of gray levels is much smaller than the number of pixels in an image, researchers often perform clustering algorithms on gray levels instead of pixels to avoid the repeated distance computation, which can indeed reduce the execution time of algorithms such as enhanced FCM (EnFCM) [18], fast generalized FCM algorithm (FGFCM) [19], and fast and robust FCM (FRFCM) [20]. These improved FCM algorithms achieve a high computation efficiency by integrating histogram to its objective function.For the second shortcoming, because the number of gray levels is much smaller than the number of pixels in an image, researchers often perform clustering algorithms on gray levels instead of pixels to avoid the repeated distance computation, which can indeed reduce the execution time of algorithms such as enhanced FCM (EnFCM) [18], fast generalized FCM algorithm (FGFCM) [19], and fast and robust FCM (FRFCM) [20]. These improved FCM algorithms achieve a high computation efficiency by integrating histogram to its objective function.</p>
        <p>Unfortunately, it is difficult to extend these algorithms to color images because the histogram of color images is more complex than gray images. Moreover, a new problem is how to reduce the computational complexity of algorithms while efficiently improving the utilization of spatial neighboring information. We have mentioned above that the adaptive neighboring information is better than a fixed neighboring window for image segmentation. To reduce the computational complexity while utilizing the adaptive spatial neighboring information, Gu et al. [21] employed a superpixel approach to obtain adaptive neighboring information and to reduce the number of clustering samples. They proposed a fuzzy double c-means clustering based on sparse self-representation (FDCM SSR).Unfortunately, it is difficult to extend these algorithms to color images because the histogram of color images is more complex than gray images. Moreover, a new problem is how to reduce the computational complexity of algorithms while efficiently improving the utilization of spatial neighboring information. We have mentioned above that the adaptive neighboring information is better than a fixed neighboring window for image segmentation. To reduce the computational complexity while utilizing the adaptive spatial neighboring information, Gu et al. [21] employed a superpixel approach to obtain adaptive neighboring information and to reduce the number of clustering samples. They proposed a fuzzy double c-means clustering based on sparse self-representation (FDCM SSR).</p>
        <p>Unfortunately, FDCM SSR still has a higher computational complexity than most of popular algorithms. Inspired by superpixel technology [22] and EnFCM, Lei et al. [23] proposed a superpixel-based fast FCM algorithm (SFFCM) for color image segmentation. SFFCM has two advantages. One is that the proposed watershed transform based on multiscale morphological gradient reconstruction (MMGR-WT) is able to provide an excellent superpixel result that is useful for improving the final clustering result. The other one is that the color histogram is integrated into the objective function of FCM to speed up the implementation of the algorithm.Unfortunately, FDCM SSR still has a higher computational complexity than most of popular algorithms. Inspired by superpixel technology [22] and EnFCM, Lei et al. [23] proposed a superpixel-based fast FCM algorithm (SFFCM) for color image segmentation. SFFCM has two advantages. One is that the proposed watershed transform based on multiscale morphological gradient reconstruction (MMGR-WT) is able to provide an excellent superpixel result that is useful for improving the final clustering result. The other one is that the color histogram is integrated into the objective function of FCM to speed up the implementation of the algorithm.</p>
        <p>Although SFFCM is excellent for color image segmentation, it requires to set manually the number of clusters. In practical image segmentation, it may be impossible to set the number of clusters for each image in an immense image dataset.Although SFFCM is excellent for color image segmentation, it requires to set manually the number of clusters. In practical image segmentation, it may be impossible to set the number of clusters for each image in an immense image dataset.</p>
        <p>To achieve automatic clustering algorithms, researchers tried to estimate the number of clusters using different algorithms such as eigenvector analysis [24], genetic algorithm [25], the particle swarm optimization [26], the robust learning-based schema [27], etc. Although these algorithms are able to find the number of clusters in any unlabeled data set, they are unsuitable for image segmentation since the spatial information is missed and the corresponding segmentation result is coarse. Moreover, none of them is robust for different kinds of data. Density peaks (DP) algorithm proposed by Rodriguez and Laio [28] addresses the problem; it first finds the local density peaks of data, and then computes the minimal distance between a center and other centers that have higher local density than the center, and finally obtains a decision-graph to achieve fast clustering. However, DP algorithm only provides decisiongraph without giving the number of clusters. Wang et al. [29] proposed a more robust and effective automatic clustering algorithm to overcome the shortcomings of DP algorithm. Though the new algorithm is able to obtain automatically the number of clusters and provides better experimental results, it is still unsuitable for image segmentation since the spatial information of images is missing.To achieve automatic clustering algorithms, researchers tried to estimate the number of clusters using different algorithms such as eigenvector analysis [24], genetic algorithm [25], the particle swarm optimization [26], the robust learning-based schema [27], etc. Although these algorithms are able to find the number of clusters in any unlabeled data set, they are unsuitable for image segmentation since the spatial information is missed and the corresponding segmentation result is coarse. Moreover, none of them is robust for different kinds of data. Density peaks (DP) algorithm proposed by Rodriguez and Laio [28] addresses the problem; it first finds the local density peaks of data, and then computes the minimal distance between a center and other centers that have higher local density than the center, and finally obtains a decision-graph to achieve fast clustering. However, DP algorithm only provides decisiongraph without giving the number of clusters. Wang et al. [29] proposed a more robust and effective automatic clustering algorithm to overcome the shortcomings of DP algorithm. Though the new algorithm is able to obtain automatically the number of clusters and provides better experimental results, it is still unsuitable for image segmentation since the spatial information of images is missing.</p>
        <p>In this paper, we aim to propose an automatic fuzzy clustering framework (AFCF) for image segmentation. The proposed AFCF is inspired by image superpixel, DP algorithm, and prior entropy-based fuzzy clustering. Although the similarity matrix of an image is often huge, which limits the application of DP in image segmentation, we can use a superpixel algorithm to simplify an image to obtain a small similarity matrix that depends on the number of superpixel. Based on the small similarity matrix, we can compute the corresponding decision graph. To obtain automatic clustering algorithms, we need to improve the decision graph to obtain the number of clusters directly without human-computer interaction. Finally, prior entropy is integrated into FCM to improve segmentation results. The proposed AFCF is able to achieve automatic image segmentation with a high precision. Three advantages of the AFCF are presented.In this paper, we aim to propose an automatic fuzzy clustering framework (AFCF) for image segmentation. The proposed AFCF is inspired by image superpixel, DP algorithm, and prior entropy-based fuzzy clustering. Although the similarity matrix of an image is often huge, which limits the application of DP in image segmentation, we can use a superpixel algorithm to simplify an image to obtain a small similarity matrix that depends on the number of superpixel. Based on the small similarity matrix, we can compute the corresponding decision graph. To obtain automatic clustering algorithms, we need to improve the decision graph to obtain the number of clusters directly without human-computer interaction. Finally, prior entropy is integrated into FCM to improve segmentation results. The proposed AFCF is able to achieve automatic image segmentation with a high precision. Three advantages of the AFCF are presented.</p>
        <p>• AFCF is a fully automatic clustering framework for image segmentation, where the number of clusters is not a required parameter compared to existing clustering algorithms.• AFCF is a fully automatic clustering framework for image segmentation, where the number of clusters is not a required parameter compared to existing clustering algorithms.</p>
        <p>• AFCF provides accurate number of clusters and achieves better image segmentation than state-of-the-art algorithms because of the utilization of the spatial information of images and prior entropy. • AFCF has a low memory demand on the experimental environment compared to algorithms connected with DP algorithm because image superpixel addresses the problem of memory overflow. The rest of this paper is organized as follows. In Section II, we illustrate motivations of this work. In Section III, we propose our methodology and analyze its superiority. The experimental results on synthetic and real images are described in Section IV. Finally, we present our conclusion in Section V.• AFCF provides accurate number of clusters and achieves better image segmentation than state-of-the-art algorithms because of the utilization of the spatial information of images and prior entropy. • AFCF has a low memory demand on the experimental environment compared to algorithms connected with DP algorithm because image superpixel addresses the problem of memory overflow. The rest of this paper is organized as follows. In Section II, we illustrate motivations of this work. In Section III, we propose our methodology and analyze its superiority. The experimental results on synthetic and real images are described in Section IV. Finally, we present our conclusion in Section V.</p>
        <p>Image segmentation results provided by clustering always depend on the number of clusters. Although researchers proposed a lot of adaptive clustering algorithms [30], [31] by estimating the number of clusters, these algorithms have a low robustness and practicability for automatic image segmentation. DP algorithm is able to generate a decision-graph that is helpful for finding the number of clusters, but it has a high computational complexity when it is used in image segmentation. To achieve automatic clustering for image segmentation, two problems need to be overcome. The first is to remove redundant information of images to obtain a small similarity matrix used for DP algorithm. The second is to improve DP algorithm to obtain accurate number of clusters and furtherly achieve image segmentation. We employ superpixel algorithms to simplify the computation of DP algorithm, and then utilize a density balance algorithm to decide the number of clusters. Furthermore, we use prior entropy to improve image segmentation.Image segmentation results provided by clustering always depend on the number of clusters. Although researchers proposed a lot of adaptive clustering algorithms [30], [31] by estimating the number of clusters, these algorithms have a low robustness and practicability for automatic image segmentation. DP algorithm is able to generate a decision-graph that is helpful for finding the number of clusters, but it has a high computational complexity when it is used in image segmentation. To achieve automatic clustering for image segmentation, two problems need to be overcome. The first is to remove redundant information of images to obtain a small similarity matrix used for DP algorithm. The second is to improve DP algorithm to obtain accurate number of clusters and furtherly achieve image segmentation. We employ superpixel algorithms to simplify the computation of DP algorithm, and then utilize a density balance algorithm to decide the number of clusters. Furthermore, we use prior entropy to improve image segmentation.</p>
        <p>In the popular clustering algorithms, such as k-means, FCM, and spectral clustering, the number of clusters is set manually. DP algorithm can automatically recognize potential cluster centers to address the issue by making two basic assumptions.In the popular clustering algorithms, such as k-means, FCM, and spectral clustering, the number of clusters is set manually. DP algorithm can automatically recognize potential cluster centers to address the issue by making two basic assumptions.</p>
        <p>One is that a cluster center often has higher density than its surrounding points. The other is that a cluster center often has a relatively large distance from other cluster centers with high density. Two quantities can be computed for each sample x i , i.e., the local density ρ i and the minimal distance δ i . Both ρ i and δ i are obtained from the data. The local density ρ i of the sample x i is presented as followsOne is that a cluster center often has higher density than its surrounding points. The other is that a cluster center often has a relatively large distance from other cluster centers with high density. Two quantities can be computed for each sample x i , i.e., the local density ρ i and the minimal distance δ i . Both ρ i and δ i are obtained from the data. The local density ρ i of the sample x i is presented as follows</p>
        <p>where N is the number of total samples in a data set, 1 i, j N , and d ij denotes the Euclidean distance between x i and x j . The d c is the cutoff distance that is an essential global decay parameter of the weight. The value of d c is usually around 2% of neighbors [28]. According to (1), ρ i describes the density intensity of x i using Gaussian kernel. In general, a large ρ i is considered as a cluster center while a small ρ i is considered as noise or outliers in data sets. δ i indicates the minimal distance between the sample x i and any other samples with higher density. The δ i is defined aswhere N is the number of total samples in a data set, 1 i, j N , and d ij denotes the Euclidean distance between x i and x j . The d c is the cutoff distance that is an essential global decay parameter of the weight. The value of d c is usually around 2% of neighbors [28]. According to (1), ρ i describes the density intensity of x i using Gaussian kernel. In general, a large ρ i is considered as a cluster center while a small ρ i is considered as noise or outliers in data sets. δ i indicates the minimal distance between the sample x i and any other samples with higher density. The δ i is defined as</p>
        <p>Note that δ i = max j (d ij ) is used for the sample with the highest density. The anomalously large value of δ i is helpful for recognizing the hidden cluster centers. By building a decision-graph with horizontal-axis ρ and vertical-axis δ, we can easily choose the samples of high ρ and relatively high δ as cluster centers. However, it is very difficult to select the appropriate cluster centers for users because there is often a series of continuous sparse points in decision-graphs. To simplify the selection of cluster centers, the DP algorithm designs a new decision scheme by individually computing γ i = ρ i δ i sorted in decreasing order. The new scheme can effectively avoid interference of false centers and easily define the potential centers. After finding the cluster centers, each remaining pixel is allocated to the same cluster as its nearest neighbor of higher density.Note that δ i = max j (d ij ) is used for the sample with the highest density. The anomalously large value of δ i is helpful for recognizing the hidden cluster centers. By building a decision-graph with horizontal-axis ρ and vertical-axis δ, we can easily choose the samples of high ρ and relatively high δ as cluster centers. However, it is very difficult to select the appropriate cluster centers for users because there is often a series of continuous sparse points in decision-graphs. To simplify the selection of cluster centers, the DP algorithm designs a new decision scheme by individually computing γ i = ρ i δ i sorted in decreasing order. The new scheme can effectively avoid interference of false centers and easily define the potential centers. After finding the cluster centers, each remaining pixel is allocated to the same cluster as its nearest neighbor of higher density.</p>
        <p>Although the decision-graph used for DP is able to provide the potential centers, it is difficult to extend DP algorithm to image segmentation. In the DP algorithm, the computation of ρ and δ depends on a similarity matrix. However, the similarity matrix corresponding to an image of size P × Q, has a large size, i.e., (P × Q) × (P × Q). The matrix always causes memory overflow if P or Q is large. For the problem, the downsampling approach is often used to reduce the size of the matrix to achieve image segmentation based on DP algorithm [32], [33]. Fig. 1 shows the image segmentation framework using DP algorithm. In Fig. 1, one can obtain cluster centers easily because there are two points with large value of ρ and δ in the decision-graph. However, the final segmentation result is coarse. Furthermore, Fig. 2 shows more results generated by DP algorithm.Although the decision-graph used for DP is able to provide the potential centers, it is difficult to extend DP algorithm to image segmentation. In the DP algorithm, the computation of ρ and δ depends on a similarity matrix. However, the similarity matrix corresponding to an image of size P × Q, has a large size, i.e., (P × Q) × (P × Q). The matrix always causes memory overflow if P or Q is large. For the problem, the downsampling approach is often used to reduce the size of the matrix to achieve image segmentation based on DP algorithm [32], [33]. Fig. 1 shows the image segmentation framework using DP algorithm. In Fig. 1, one can obtain cluster centers easily because there are two points with large value of ρ and δ in the decision-graph. However, the final segmentation result is coarse. Furthermore, Fig. 2 shows more results generated by DP algorithm.</p>
        <p>Figs. 1 and2 show that the DP algorithm is able to achieve roughly automatic image segmentation. The segmentation result is good when the input image (the first image) is simple, but the results are poor when input images (the last three images) are complex as shown in Fig. 2. To improve these segmentation results, two issues need to be addressed.Figs. 1 and2 show that the DP algorithm is able to achieve roughly automatic image segmentation. The segmentation result is good when the input image (the first image) is simple, but the results are poor when input images (the last three images) are complex as shown in Fig. 2. To improve these segmentation results, two issues need to be addressed.</p>
        <p>• The downsampling operation is a rough way used for the size reduction of a similarity matrix. We need to develop a new algorithm that is not only able to reduce the size of a similarity matrix, but also it can preserve the structuring information of images. • The decision-graph needs to be improved to obtain automatically the number of clusters. In this paper, we employ superpixel algorithms instead of the downsampling operation to address the first issue, and then use a density balance algorithm to overcome the second issue. The detailed description will be presented in Section III.• The downsampling operation is a rough way used for the size reduction of a similarity matrix. We need to develop a new algorithm that is not only able to reduce the size of a similarity matrix, but also it can preserve the structuring information of images. • The decision-graph needs to be improved to obtain automatically the number of clusters. In this paper, we employ superpixel algorithms instead of the downsampling operation to address the first issue, and then use a density balance algorithm to overcome the second issue. The detailed description will be presented in Section III.</p>
        <p>It is well-known that the iterative optimization is important for clustering algorithms through minimizing an objectionIt is well-known that the iterative optimization is important for clustering algorithms through minimizing an objection</p>
        <p>Color feature and histogram of superpixel function. Compared with some fast image segmentation algorithms [34], [35], clustering algorithms have a low computational efficiency when they are used to segment an image with high-resolution. Fortunately, superpixel technology [36] plays a key role in improving the execution efficiency of image segmentation algorithms. Superpixel means that an image is divided into a large number of small and independent areas with different sizes and shapes [37]. Based on the superpixel result of an image, one can use a pixel to replace all pixels in a superpixel area to efficiently reduce the number of pixels in an image. Motivated by this, Lei et al. proposed SFFCM [23] for color image segmentation. SFFCM addresses two difficulties existing in clustering algorithms for color image segmentation. One is that SFFCM presents an excellent superpixel approach named MMGR-WT, and the superpixel image obtained by MMGR-WT is helpful for improving segmentation effect because the adaptive neighboring information of pixels is integrated into the objective function of clustering algorithms.Color feature and histogram of superpixel function. Compared with some fast image segmentation algorithms [34], [35], clustering algorithms have a low computational efficiency when they are used to segment an image with high-resolution. Fortunately, superpixel technology [36] plays a key role in improving the execution efficiency of image segmentation algorithms. Superpixel means that an image is divided into a large number of small and independent areas with different sizes and shapes [37]. Based on the superpixel result of an image, one can use a pixel to replace all pixels in a superpixel area to efficiently reduce the number of pixels in an image. Motivated by this, Lei et al. proposed SFFCM [23] for color image segmentation. SFFCM addresses two difficulties existing in clustering algorithms for color image segmentation. One is that SFFCM presents an excellent superpixel approach named MMGR-WT, and the superpixel image obtained by MMGR-WT is helpful for improving segmentation effect because the adaptive neighboring information of pixels is integrated into the objective function of clustering algorithms.</p>
        <p>The other one is that the color histogram is integrated into the objective function to achieve fast clustering due to the fact that the number of different pixels in a color image has been effectively reduced.The other one is that the color histogram is integrated into the objective function to achieve fast clustering due to the fact that the number of different pixels in a color image has been effectively reduced.</p>
        <p>The objective function of SFFCM isThe objective function of SFFCM is</p>
        <p>where ∂ l represents the l-th superpixel area, S l is the total number of pixels in the superpixel area ∂ l , 1 l N . N is the total number of superpixel area in an image f , and c is the number of clusters. The u kl represents the fuzzy membership of the l-th superpixel area with respect to the k-th cluster, v k denotes the k-th clustering center, and x p denotes a pixel in a color image f .where ∂ l represents the l-th superpixel area, S l is the total number of pixels in the superpixel area ∂ l , 1 l N . N is the total number of superpixel area in an image f , and c is the number of clusters. The u kl represents the fuzzy membership of the l-th superpixel area with respect to the k-th cluster, v k denotes the k-th clustering center, and x p denotes a pixel in a color image f .</p>
        <p>According to (3), the membership partition matrix u kl and the cluster center v k of SFFCM are given as followsAccording to (3), the membership partition matrix u kl and the cluster center v k of SFFCM are given as follows</p>
        <p>It can be seen from ( 3)-( 5) that the computation cost of SFFCM is clearly lower than FCM due to N N . Therefore, the SFFCM achieve fasts and effective color image segmentation. The image segmentation framework based on SFFCM is shown in Fig. 3.It can be seen from ( 3)-( 5) that the computation cost of SFFCM is clearly lower than FCM due to N N . Therefore, the SFFCM achieve fasts and effective color image segmentation. The image segmentation framework based on SFFCM is shown in Fig. 3.</p>
        <p>In Fig. 3, the segmentation result is better than the result shown in Fig. 1. The main reason can be attributed to the superpixel image generated by MMR-WT. The result further demonstrates that the advantages of SFFCM. Although SF-FCM achieves fast and robust image segmentation, the number of clusters is still an essential parameter, which limits the application of SFFCM. Moreover, the Euclidean distance is used to measure the similarity between different superpixel areas in SFFCM, which causes erroneous segmentation results as shown in Fig. 4.In Fig. 3, the segmentation result is better than the result shown in Fig. 1. The main reason can be attributed to the superpixel image generated by MMR-WT. The result further demonstrates that the advantages of SFFCM. Although SF-FCM achieves fast and robust image segmentation, the number of clusters is still an essential parameter, which limits the application of SFFCM. Moreover, the Euclidean distance is used to measure the similarity between different superpixel areas in SFFCM, which causes erroneous segmentation results as shown in Fig. 4.</p>
        <p>For the problem shown in Fig. 4, hidden Markov random fields (HMRF) [38], [39] is a popular algorithm used for overcoming the problem. HMRF uses cluster centers and the prior probability of membership to obtain the final membership called posterior probability [40]. Motivated by this, in this work, we employ fuzzy clustering based on prior entropy to achieve image segmentation. The detailed analysis will be presented in Section III.B.For the problem shown in Fig. 4, hidden Markov random fields (HMRF) [38], [39] is a popular algorithm used for overcoming the problem. HMRF uses cluster centers and the prior probability of membership to obtain the final membership called posterior probability [40]. Motivated by this, in this work, we employ fuzzy clustering based on prior entropy to achieve image segmentation. The detailed analysis will be presented in Section III.B.</p>
        <p>In Section II, we presented the motivation of this paper. On the one hand, we employ parameter-free clustering to obtain automatically the number of clusters. On the other hand, we employ the image superpixel and fuzzy clustering based on prior entropy to achieve image segmentation. Based on the two ideas, we present the segmentation framework of images using the proposed AFCF as shown in Fig. 5.In Section II, we presented the motivation of this paper. On the one hand, we employ parameter-free clustering to obtain automatically the number of clusters. On the other hand, we employ the image superpixel and fuzzy clustering based on prior entropy to achieve image segmentation. Based on the two ideas, we present the segmentation framework of images using the proposed AFCF as shown in Fig. 5.</p>
        <p>In Fig. 5, the proposed AFCF firstly employs a superpixel algorithm to obtain pre-segmentation result. Secondly, DP algorithm is performed on the superpixel image to generate a decision-graph. Because the number of areas in the superpixel image is much smaller than the number of pixels in the original image, a small similarity matrix is obtained resulting in a small memory requirement and a low computational complexity for DP algorithm. After that, the density balance algorithm is used to obtain a more robust decision-graph that directly outputs the number of clusters. By computing the maximal interval of adjacent points, the points in the decision-graph are divided into two groups; the first group of points is considered as cluster centers. Finally, a fuzzy clustering based on prior entropy is used for achieving image segmentation.In Fig. 5, the proposed AFCF firstly employs a superpixel algorithm to obtain pre-segmentation result. Secondly, DP algorithm is performed on the superpixel image to generate a decision-graph. Because the number of areas in the superpixel image is much smaller than the number of pixels in the original image, a small similarity matrix is obtained resulting in a small memory requirement and a low computational complexity for DP algorithm. After that, the density balance algorithm is used to obtain a more robust decision-graph that directly outputs the number of clusters. By computing the maximal interval of adjacent points, the points in the decision-graph are divided into two groups; the first group of points is considered as cluster centers. Finally, a fuzzy clustering based on prior entropy is used for achieving image segmentation.</p>
        <p>The DP algorithm achieves semi-automatic clustering because one can choose the number of clusters according to a decision-graph. However, it is difficult to apply DP algorithm to automatic image segmentation because a huge similarity matrix usually causes memory overflow and a high computational cost. For example, a moderate-sized image of size 500 × 500 generates a huge similarity matrix of size (500×500) 2 . Such a huge matrix occupies very large memory and requires very high computational cost. Moreover, initial cluster centers obtained by a decision-graph need to be improved for image segmentation because the spatial information of images is missing.The DP algorithm achieves semi-automatic clustering because one can choose the number of clusters according to a decision-graph. However, it is difficult to apply DP algorithm to automatic image segmentation because a huge similarity matrix usually causes memory overflow and a high computational cost. For example, a moderate-sized image of size 500 × 500 generates a huge similarity matrix of size (500×500) 2 . Such a huge matrix occupies very large memory and requires very high computational cost. Moreover, initial cluster centers obtained by a decision-graph need to be improved for image segmentation because the spatial information of images is missing.</p>
        <p>In this section, we firstly employ a superpixel algorithm to reduce the size of the similarity matrix. Because a superpixel algorithm can smooth the texture details and preserve the structuring information of objects, it is helpful for image pre-segmentation to improve the final segmentation effect. Secondly, we use a density balance algorithm and the maximal interval to obtain automatically the number of clusters. In practical applications, because different superpixel algorithms can be selected, we only propose a framework of automatic fuzzy clustering but not a specific algorithm in this paper. Fig. 6 shows superpixel results provided by different superpixel algorithms such as SLIC [41], DBSCAN [42], LSC [43], GMMSP [44], HS [45], and MMGR-WT [23]. Note that each of SLIC, DBSCAN, LSC, and HS requires one parameter, i.e., the number of superpixel area; GMMSP also requires one parameter that is the size of areas; but MMGR-WT needs two parameters that are the initial structuring element denoted by r 1 and the minimal threshold error denoted by η. Table I shows the number of areas in different superpixel images for an original image size of 321×481. In practical applications, η is usually a constant and η = 10 -4 in [23]. We set η = 0.1 here in order to obtain more superpixel areas for fair comparison.In this section, we firstly employ a superpixel algorithm to reduce the size of the similarity matrix. Because a superpixel algorithm can smooth the texture details and preserve the structuring information of objects, it is helpful for image pre-segmentation to improve the final segmentation effect. Secondly, we use a density balance algorithm and the maximal interval to obtain automatically the number of clusters. In practical applications, because different superpixel algorithms can be selected, we only propose a framework of automatic fuzzy clustering but not a specific algorithm in this paper. Fig. 6 shows superpixel results provided by different superpixel algorithms such as SLIC [41], DBSCAN [42], LSC [43], GMMSP [44], HS [45], and MMGR-WT [23]. Note that each of SLIC, DBSCAN, LSC, and HS requires one parameter, i.e., the number of superpixel area; GMMSP also requires one parameter that is the size of areas; but MMGR-WT needs two parameters that are the initial structuring element denoted by r 1 and the minimal threshold error denoted by η. Table I shows the number of areas in different superpixel images for an original image size of 321×481. In practical applications, η is usually a constant and η = 10 -4 in [23]. We set η = 0.1 here in order to obtain more superpixel areas for fair comparison.</p>
        <p>Table I shows that these superpixel algorithms can efficiently reduce the total number of pixels in an image and thus obtain a small similarity matrix to improve the computational efficiency of DP algorithm. Moreover, a superpixel area integrates both the color features and the spatial structuring features, which is helpful for improving image segmentation results. For instance, the size of the similarity matrix is reduced from (321 × 481) 2 to 200 × 200 for the SLIC as shown in Fig. 6b.Table I shows that these superpixel algorithms can efficiently reduce the total number of pixels in an image and thus obtain a small similarity matrix to improve the computational efficiency of DP algorithm. Moreover, a superpixel area integrates both the color features and the spatial structuring features, which is helpful for improving image segmentation results. For instance, the size of the similarity matrix is reduced from (321 × 481) 2 to 200 × 200 for the SLIC as shown in Fig. 6b.</p>
        <p>According to the aforementioned superpixel algorithms and DP algorithm, the local density denoted by ρ I and the minimal distance denoted by δ I are presented as followsAccording to the aforementioned superpixel algorithms and DP algorithm, the local density denoted by ρ I and the minimal distance denoted by δ I are presented as follows</p>
        <p>where 1 ≤ I, J ≤ N , D IJ denotes the Euclidean distance between ∂ I and ∂ J . S J is the total number of pixels in the J-th superpixel area, d c is the cutoff distance, and δ I indicates the minimal distance between the area ∂ I and any other area with higher density. The δ I is defined aswhere 1 ≤ I, J ≤ N , D IJ denotes the Euclidean distance between ∂ I and ∂ J . S J is the total number of pixels in the J-th superpixel area, d c is the cutoff distance, and δ I indicates the minimal distance between the area ∂ I and any other area with higher density. The δ I is defined as</p>
        <p>where δ I = max J (D IJ ) for the superpixel area with highest density. To speed up the computation, D IJ is defined aswhere δ I = max J (D IJ ) for the superpixel area with highest density. To speed up the computation, D IJ is defined as</p>
        <p>It can be seen that D IJ is different from d ij . According to DP algorithm and γ i = ρ i δ i , we can obtain the initial decisiongraph as shown in Fig. 7. Fig. 7 shows that although one can easily select the number of clusters depending on the decisiongraph, it is difficult to obtain automatically the number of clusters by setting a threshold for the decision-graph. In Fig. 7a, the threshold ranges from 0.15 to 0.56, while it ranges from 0.1 to 0.38 for Fig. 7b.It can be seen that D IJ is different from d ij . According to DP algorithm and γ i = ρ i δ i , we can obtain the initial decisiongraph as shown in Fig. 7. Fig. 7 shows that although one can easily select the number of clusters depending on the decisiongraph, it is difficult to obtain automatically the number of clusters by setting a threshold for the decision-graph. In Fig. 7a, the threshold ranges from 0.15 to 0.56, while it ranges from 0.1 to 0.38 for Fig. 7b.</p>
        <p>To achieve fully automatic clustering, we propose a new decision-graph by using a density balance algorithm. We don't need to select the number of clusters and we only compute the maximal interval in the new decision-graph. The density balance algorithm aims to map the original decision-graph γ j into a new decision-graph φ j that is superior to γ j for finding the best number of clusters, where 1 ≤ j ≤ q, where q denotes the number of superipixel area. Let a denote the number of intervals in the range [0,1] and κ denote the radius of the neighborhood, where χ = {χ 1 , χ 2 , . . . χ a+1 } represents the set of data interval, χ 1 = 0, χ 2 = 1/a, χ 2 = 2/a, and χ a+1 = 1. Generally, a is an empirical value and a = 1000. We define that ξ(χ κ ) is the number of γ j under the constraint condition χ a -γ j ≤ κ , where 1 ≤ e ≤ a + 1 and γ j is the normalized result γ j = (γ j -min(γ j ))/(max(γ j ) -min(γ j )), e, a ∈ N + . ξ(χ e ) can be computed as followsTo achieve fully automatic clustering, we propose a new decision-graph by using a density balance algorithm. We don't need to select the number of clusters and we only compute the maximal interval in the new decision-graph. The density balance algorithm aims to map the original decision-graph γ j into a new decision-graph φ j that is superior to γ j for finding the best number of clusters, where 1 ≤ j ≤ q, where q denotes the number of superipixel area. Let a denote the number of intervals in the range [0,1] and κ denote the radius of the neighborhood, where χ = {χ 1 , χ 2 , . . . χ a+1 } represents the set of data interval, χ 1 = 0, χ 2 = 1/a, χ 2 = 2/a, and χ a+1 = 1. Generally, a is an empirical value and a = 1000. We define that ξ(χ κ ) is the number of γ j under the constraint condition χ a -γ j ≤ κ , where 1 ≤ e ≤ a + 1 and γ j is the normalized result γ j = (γ j -min(γ j ))/(max(γ j ) -min(γ j )), e, a ∈ N + . ξ(χ e ) can be computed as follows</p>
        <p>We define that φ j is the mapping result of γ j , where φ j can be computed as followsWe define that φ j is the mapping result of γ j , where φ j can be computed as follows</p>
        <p>We presented the pseudocode of the algorithm as recorded in Algorithm 1.We presented the pseudocode of the algorithm as recorded in Algorithm 1.</p>
        <p>According to the Algorithm 1, we compute the new decision-graphs corresponding to Fig. 7(a) and Fig. 7(c). Fig. Comparing Fig. 7 and Fig. 8, it is clear that the DB algorithm is useful for finding the best number of clusters. The proposed DB algorithm is able to distinguish efficiently cluster centers and non-cluster centers.According to the Algorithm 1, we compute the new decision-graphs corresponding to Fig. 7(a) and Fig. 7(c). Fig. Comparing Fig. 7 and Fig. 8, it is clear that the DB algorithm is useful for finding the best number of clusters. The proposed DB algorithm is able to distinguish efficiently cluster centers and non-cluster centers.</p>
        <p>According to section III. B, we can obtain the number of clusters and the final clustering result using superpixel-based DP algorithm. However, in [28], Euclidean distance is used to measure the similarity between different superpixel areas, which often leads to poor segmentation results for complex images as shown in Fig. 4. We solve this issue using the knowledge that both the covariance analysis and the Markov random field (MRF) are useful for improving high-dimensional data classification [46], [47]. Thus we integrate the covariance analysis and MRF into FCM algorithm, and propose prior entropy-based fuzzy clustering algorithm (PEFC).According to section III. B, we can obtain the number of clusters and the final clustering result using superpixel-based DP algorithm. However, in [28], Euclidean distance is used to measure the similarity between different superpixel areas, which often leads to poor segmentation results for complex images as shown in Fig. 4. We solve this issue using the knowledge that both the covariance analysis and the Markov random field (MRF) are useful for improving high-dimensional data classification [46], [47]. Thus we integrate the covariance analysis and MRF into FCM algorithm, and propose prior entropy-based fuzzy clustering algorithm (PEFC).</p>
        <p>Based on superpixel results of images and the number of clusters provided by the Algorithm 1, we firstly propose the objective function of PEFC as followsBased on superpixel results of images and the number of clusters provided by the Algorithm 1, we firstly propose the objective function of PEFC as follows</p>
        <p>where 1 S l p∈∂ l x p is the mean value of the superpixel area ∂ l , and it integrates the spatial information of images. The Σ k is the covariance matrix that respects to the correlation of different dimensions. The proportion π k is the prior probability of the superpixel area 1 In (12), Φ( 1 S l p∈∂ l x p |v k , Σ k ) denotes the multivariate Gaussian distribution, which is defined as followswhere 1 S l p∈∂ l x p is the mean value of the superpixel area ∂ l , and it integrates the spatial information of images. The Σ k is the covariance matrix that respects to the correlation of different dimensions. The proportion π k is the prior probability of the superpixel area 1 In (12), Φ( 1 S l p∈∂ l x p |v k , Σ k ) denotes the multivariate Gaussian distribution, which is defined as follows</p>
        <p>In (13), ρ is the Gaussian density function, which is presented as followsIn (13), ρ is the Gaussian density function, which is presented as follows</p>
        <p>where D denotes the dimension of image data or the number of image channel, Σ k is a diagonal matrix of size D × D, and |Σ k | is the determinant of Σ k . Substituting ( 14) into (13), we getwhere D denotes the dimension of image data or the number of image channel, Σ k is a diagonal matrix of size D × D, and |Σ k | is the determinant of Σ k . Substituting ( 14) into (13), we get</p>
        <p>According to c k=1 u kl = 1 and c k=1 π k = 1, we use the Lagrange multiplier approach to compute the optimal value, and we haveAccording to c k=1 u kl = 1 and c k=1 π k = 1, we use the Lagrange multiplier approach to compute the optimal value, and we have</p>
        <p>where λ 1 and λ 2 are Lagrange multipliers. Thus we havewhere λ 1 and λ 2 are Lagrange multipliers. Thus we have</p>
        <p>According to c k=1 u kl = 1, the solution of ∂J ∂u kl = 0 yieldsAccording to c k=1 u kl = 1, the solution of ∂J ∂u kl = 0 yields</p>
        <p>By computing ∂J ∂v k = 0, we haveBy computing ∂J ∂v k = 0, we have</p>
        <p>Similarly, we computeSimilarly, we compute</p>
        <p>..</p>
        <p>The solution of ∂J ∂Σ k = 0 yieldsThe solution of ∂J ∂Σ k = 0 yields</p>
        <p>Finally, by computing ∂J ∂π k = 0, we haveFinally, by computing ∂J ∂π k = 0, we have</p>
        <p>and thusand thus</p>
        <p>According to ( 17)-( 20), we obtain a membership partition matrix D×D) , and the proportion πAccording to ( 17)-( 20), we obtain a membership partition matrix D×D) , and the proportion π</p>
        <p>We can see from ( 17)-( 20) that PEFC integrates the adaptive neighboring information of prior probability distribution, and it considers the distribution characteristic of data. Therefore, it is often used to divide high-dimensional data into different groups. Fig. 9 shows an example where PEFC is used for the Step 2: Implement the DP algorithm on x to obtain γ j ;We can see from ( 17)-( 20) that PEFC integrates the adaptive neighboring information of prior probability distribution, and it considers the distribution characteristic of data. Therefore, it is often used to divide high-dimensional data into different groups. Fig. 9 shows an example where PEFC is used for the Step 2: Implement the DP algorithm on x to obtain γ j ;</p>
        <p>Step 3: Implement the Algorithm 1 to obtain φ j and the number of clusters;Step 3: Implement the Algorithm 1 to obtain φ j and the number of clusters;</p>
        <p>Step 4: Initialize the variables U (0) , V (0) , Σ (0) , and π (0) using FCM algorithm, where the weighting exponent, the convergence condition, and the maximal number of iterations are 2, 10 -5 , and 50, respectively;Step 4: Initialize the variables U (0) , V (0) , Σ (0) , and π (0) using FCM algorithm, where the weighting exponent, the convergence condition, and the maximal number of iterations are 2, 10 -5 , and 50, respectively;</p>
        <p>Step 5: Set the loop counter t = 0;Step 5: Set the loop counter t = 0;</p>
        <p>Step 6: Update the variables U (t) , V (t) , Σ (t) , and π (t) ;Step 6: Update the variables U (t) , V (t) , Σ (t) , and π (t) ;</p>
        <p>(1) Update the membership matrix U (t) using ( 17).(1) Update the membership matrix U (t) using ( 17).</p>
        <p>(2) Update the cluster center V (t) using ( 18).(2) Update the cluster center V (t) using ( 18).</p>
        <p>(3) Update the covariance Σ (t) using ( 19).(3) Update the covariance Σ (t) using ( 19).</p>
        <p>(4) Update the prior probability π (t) using (20).(4) Update the prior probability π (t) using (20).</p>
        <p>Step 7: If max U (t) -U (t+1) &lt; 10 -5 then stop, otherwise, set t=t+1 and go to step 6.Step 7: If max U (t) -U (t+1) &lt; 10 -5 then stop, otherwise, set t=t+1 and go to step 6.</p>
        <p>In this paper, three excellent superpixle algorithms, namely SLIC, LSC, and MMGR-WT, are used for the AFCF to obtain three automatic image segmentation algorithms, i.e., SLIC-AFCF, LSC-AFCF, and MMGR-AFCF. Fig. 10 shows the segmentation results generated by the three algorithms. It can be seen that three algorithms obtain the same number of clusters but different segmentation results. MMGR-AFCF obtains a better result than SLIC-AFCF and LSC-AFCF, for MMGR-WT generates a better superpixle result than SLIC and LSC.In this paper, three excellent superpixle algorithms, namely SLIC, LSC, and MMGR-WT, are used for the AFCF to obtain three automatic image segmentation algorithms, i.e., SLIC-AFCF, LSC-AFCF, and MMGR-AFCF. Fig. 10 shows the segmentation results generated by the three algorithms. It can be seen that three algorithms obtain the same number of clusters but different segmentation results. MMGR-AFCF obtains a better result than SLIC-AFCF and LSC-AFCF, for MMGR-WT generates a better superpixle result than SLIC and LSC.</p>
        <p>We conducted experiments on two types of images: synthetic color images with complex texture information (image size is 256 × 256), as well as real images from the Berkeley segmentation dataset and benchmark (BSDS500) [48] (image size is 481 × 321 or 321 × 481). Two synthetic images include three and four different colors and textures, respectively. The BSDS500 includes 300 training images and 200 testing images. There are 4 to 9 ground truth segmentations for each image in BSDS500, and each ground truth is delineated by one human subject. All algorithms and experimental evaluations are performed on a workstation with an Intel(R) Core (TM) CPU, i7-6700, 3.4GHz, and 16GB RAM.We conducted experiments on two types of images: synthetic color images with complex texture information (image size is 256 × 256), as well as real images from the Berkeley segmentation dataset and benchmark (BSDS500) [48] (image size is 481 × 321 or 321 × 481). Two synthetic images include three and four different colors and textures, respectively. The BSDS500 includes 300 training images and 200 testing images. There are 4 to 9 ground truth segmentations for each image in BSDS500, and each ground truth is delineated by one human subject. All algorithms and experimental evaluations are performed on a workstation with an Intel(R) Core (TM) CPU, i7-6700, 3.4GHz, and 16GB RAM.</p>
        <p>To evaluate the effectiveness and efficiency of the proposed AFCF, ten popular image segmentation algorithms based on clustering are considered for comparisons. These are FCM [49], FGFCM [19], HMRF-FCM [40], FLICM [12], NWFCM [13], Liu's algorithm [16], NDFCM [50], FRFCM [20], DS-FCM N [15], and SFFCM [23]. In addition, because the proposed AFCF is an image segmentation framework, three algorithms SLIC-AFCF, LSC-AFCF, and MMGR-AFCF mentioned in Section III. C, are used in our experiments.To evaluate the effectiveness and efficiency of the proposed AFCF, ten popular image segmentation algorithms based on clustering are considered for comparisons. These are FCM [49], FGFCM [19], HMRF-FCM [40], FLICM [12], NWFCM [13], Liu's algorithm [16], NDFCM [50], FRFCM [20], DS-FCM N [15], and SFFCM [23]. In addition, because the proposed AFCF is an image segmentation framework, three algorithms SLIC-AFCF, LSC-AFCF, and MMGR-AFCF mentioned in Section III. C, are used in our experiments.</p>
        <p>Since both comparative algorithms and the proposed AFCF belong to the class of clustering algorithms through minimizing an objective function. Here three indispensable parameters, i.e., the weighting exponent, the minimal error threshold, and the maximal number of iterations must be set before iterations. In our experiments, the values of these parameters are 2, 10 -5 , and 50, respectively. The parameter setting of comparative algorithms follows the original paper. As all comparative algorithms require a neighboring window except FCM, HMRF-FCM, FLICM, Liu's algorithm and SFFCM, a window of size 3 × 3 is used for those algorithms that require a neighboring window for fair comparison. The spatial scale factor and the gray-level scale factor in FGFCM, are λ s = 3 and λ g = 5, respectively. The NWFCM only refers to the gray-level scale factor, λ g = 5. The three parameters, the spatial bandwidth h s = 10, the range bandwidth h r = 10, and the minimum size of final output regions h k = 100 are used for Liu's algorithm. Except three indispensable parameters mentioned above and the number of the cluster prototypes, FCM, HMRF-FCM, FLICM, and DSFCM N do not require any other parameters. In FRFCM, both the structuring element and the filtering window are a square of size 3 × 3 for fair comparison. For the SFFCM and the proposed MMGR-AFCF, they share two same parameters used for the MMGR-WT, r 1 = 2, and η = 10 -4 . For the proposed SLIC-AFCF and LSC-AFCF, the number of superpixel areas is 400 here.Since both comparative algorithms and the proposed AFCF belong to the class of clustering algorithms through minimizing an objective function. Here three indispensable parameters, i.e., the weighting exponent, the minimal error threshold, and the maximal number of iterations must be set before iterations. In our experiments, the values of these parameters are 2, 10 -5 , and 50, respectively. The parameter setting of comparative algorithms follows the original paper. As all comparative algorithms require a neighboring window except FCM, HMRF-FCM, FLICM, Liu's algorithm and SFFCM, a window of size 3 × 3 is used for those algorithms that require a neighboring window for fair comparison. The spatial scale factor and the gray-level scale factor in FGFCM, are λ s = 3 and λ g = 5, respectively. The NWFCM only refers to the gray-level scale factor, λ g = 5. The three parameters, the spatial bandwidth h s = 10, the range bandwidth h r = 10, and the minimum size of final output regions h k = 100 are used for Liu's algorithm. Except three indispensable parameters mentioned above and the number of the cluster prototypes, FCM, HMRF-FCM, FLICM, and DSFCM N do not require any other parameters. In FRFCM, both the structuring element and the filtering window are a square of size 3 × 3 for fair comparison. For the SFFCM and the proposed MMGR-AFCF, they share two same parameters used for the MMGR-WT, r 1 = 2, and η = 10 -4 . For the proposed SLIC-AFCF and LSC-AFCF, the number of superpixel areas is 400 here.</p>
        <p>As this paper proposes a fully automatic fuzzy clustering framework for image segmentation, we demonstrate that the proposed framework is able to provide accurate number of clusters and achieve better image segmentation than comparative algorithms. Two synthetic images are considered as testing images, where we use the texture information of the Colored Brodatz Texture database (http://multibandtexture.recherche. usherbrooke.ca/colored%20 brodatz.html) to generate complex texture images. Furthermore, these texture images are corrupted by different kinds of noise. Two kinds of different noises -Gaussian noise and Salt &amp; Pepper noise are added to two testing images. The final segmentation results are shown in Figs. 11 and12.As this paper proposes a fully automatic fuzzy clustering framework for image segmentation, we demonstrate that the proposed framework is able to provide accurate number of clusters and achieve better image segmentation than comparative algorithms. Two synthetic images are considered as testing images, where we use the texture information of the Colored Brodatz Texture database (http://multibandtexture.recherche. usherbrooke.ca/colored%20 brodatz.html) to generate complex texture images. Furthermore, these texture images are corrupted by different kinds of noise. Two kinds of different noises -Gaussian noise and Salt &amp; Pepper noise are added to two testing images. The final segmentation results are shown in Figs. 11 and12.</p>
        <p>Note that all comparative algorithms require the number of clusters except the proposed AFCF since it is fully automatic for image segmentation. Figs. 11(c Fig. 11 (d) shows that FCM provides a poor segmentation result since the spatial information of images is missing. Although FRFCM and DSFCM N integrate spatial information of images into their objective function, lots of pixels are classified falsely as shown in Figs. 11 (k andl). FRFCM obtains a poor result, since the multivariate morphological reconstruction is unsuitable for images with complex texture. DSFCM N employs the sparse representation to improve image segmentation results, but it is not effective for images corrupted by the mixture noise. Figs. 11 (e, g, h, andj) provide good segmentation results because the neighboring information employed by FGFCM, FLICM, NWFCM, and NDFCM is effective for improving segmentation results. Furthermore, Liu's algorithm and HRMF-FCM provide better results, which show that they are robust for images corrupted by noise. AFCF has a strong capability of noise suppression, and MMGR-AFCF obtains the best result that looks similar to the result provided by HMRF-FCM.Note that all comparative algorithms require the number of clusters except the proposed AFCF since it is fully automatic for image segmentation. Figs. 11(c Fig. 11 (d) shows that FCM provides a poor segmentation result since the spatial information of images is missing. Although FRFCM and DSFCM N integrate spatial information of images into their objective function, lots of pixels are classified falsely as shown in Figs. 11 (k andl). FRFCM obtains a poor result, since the multivariate morphological reconstruction is unsuitable for images with complex texture. DSFCM N employs the sparse representation to improve image segmentation results, but it is not effective for images corrupted by the mixture noise. Figs. 11 (e, g, h, andj) provide good segmentation results because the neighboring information employed by FGFCM, FLICM, NWFCM, and NDFCM is effective for improving segmentation results. Furthermore, Liu's algorithm and HRMF-FCM provide better results, which show that they are robust for images corrupted by noise. AFCF has a strong capability of noise suppression, and MMGR-AFCF obtains the best result that looks similar to the result provided by HMRF-FCM.</p>
        <p>Fig. 12 (a) has a more complex shape and texture than Fig. 11 (a). In Figs. 12 (d, e, andl), FCM, FGFCM, and DSFCM N generate poor segmentation results because they obtain wrong cluster centers, which indicates that the three algorithms have a weak capability of pixel classification for complex images corrupted by noise. Figs. 12 (f, g, h, i, andj) offters similar results, which means that HMRF-FCM, FLICM, NWFCM, NDFCM, and Liu's algorithm have a limited capability for improving segmentation effect on complex images. Both SF-FCM and AFCF obtain better segmentation results as shown in Figs. 12 (m, n, o, andp), which shows that the superpixel technology and the prior entropy are effective for improving segmentation results on complex images. Similar to Fig. 11 (p), the MMGR-AFCF obtains the best segmentation result as shown in Fig. 12 (p).Fig. 12 (a) has a more complex shape and texture than Fig. 11 (a). In Figs. 12 (d, e, andl), FCM, FGFCM, and DSFCM N generate poor segmentation results because they obtain wrong cluster centers, which indicates that the three algorithms have a weak capability of pixel classification for complex images corrupted by noise. Figs. 12 (f, g, h, i, andj) offters similar results, which means that HMRF-FCM, FLICM, NWFCM, NDFCM, and Liu's algorithm have a limited capability for improving segmentation effect on complex images. Both SF-FCM and AFCF obtain better segmentation results as shown in Figs. 12 (m, n, o, andp), which shows that the superpixel technology and the prior entropy are effective for improving segmentation results on complex images. Similar to Fig. 11 (p), the MMGR-AFCF obtains the best segmentation result as shown in Fig. 12 (p).</p>
        <p>To evaluate the performance of different algorithms on noisy images, two performance indices are employed here. The first is the quantitative index score (S) that is the degree of equality between pixel sets A k and the ground truth C k [20], The second is the optimal segmentation accuracy (SA) that is the sum of the number of the correctly classified pixels divided by the sum of the number of total pixels. They are defined asTo evaluate the performance of different algorithms on noisy images, two performance indices are employed here. The first is the quantitative index score (S) that is the degree of equality between pixel sets A k and the ground truth C k [20], The second is the optimal segmentation accuracy (SA) that is the sum of the number of the correctly classified pixels divided by the sum of the number of total pixels. They are defined as</p>
        <p>where A k is the set of pixels belonging to the kth class found by an algorithm while C k is the set of pixels belonging to the class in the ground truth. All algorithms are performed on the two synthetic images corrupted by different kinds and levels of noise. The experimental results are shown in Tables II-III. As the algorithms used in experiments show different performance for images corrupted by different kinds of noise, we further presented the mean value and the root mean square error (RMSE) of S and SA in Tables II andIII.where A k is the set of pixels belonging to the kth class found by an algorithm while C k is the set of pixels belonging to the class in the ground truth. All algorithms are performed on the two synthetic images corrupted by different kinds and levels of noise. The experimental results are shown in Tables II-III. As the algorithms used in experiments show different performance for images corrupted by different kinds of noise, we further presented the mean value and the root mean square error (RMSE) of S and SA in Tables II andIII.</p>
        <p>In Tables II andIII, FCM obtains low mean value and high RMSE of S and SA because it is sensitive to both Salt &amp; Pepper noise and Gaussian noise. NWFCM, FRFCM, and DSFCM N show better performance than FCM, but they are poor compared to other comparative algorithms, especially when the noise level is high. The DSFCM N is robust for Salt &amp; Pepper noise, but it is sensitive to Gaussian noise, and thus it is difficult to obtain a good segmentation result using the DSFCM N for images corrupted by the mixture noise. FGFCM, HMRF-FCM, NDFCM, Liu's algorithm, and FLICM obtain similar average performance for the first synthetic image as shown in Table II, but the FLICM shows worse performance than other four algorithms for the second synthetic image as shown in Table III. SFFCM shows excellent performance for two synthetic images. We can see that SFFCM is insensitive to both Salt &amp; Pepper noise and Gaussian noise. However, it is sensitive to Gaussian noise when the noise level is high, as shown in Table III. The proposed AFCF shows better performance than comparative algorithms except SFFCM. Especially, MMGR-AFCF obtains the largest mean value and the smallest RMSE of S and SA on two synthetic images, which demonstrates that MMGR-AFCF is robust for different kinds of image corrupted by noise.In Tables II andIII, FCM obtains low mean value and high RMSE of S and SA because it is sensitive to both Salt &amp; Pepper noise and Gaussian noise. NWFCM, FRFCM, and DSFCM N show better performance than FCM, but they are poor compared to other comparative algorithms, especially when the noise level is high. The DSFCM N is robust for Salt &amp; Pepper noise, but it is sensitive to Gaussian noise, and thus it is difficult to obtain a good segmentation result using the DSFCM N for images corrupted by the mixture noise. FGFCM, HMRF-FCM, NDFCM, Liu's algorithm, and FLICM obtain similar average performance for the first synthetic image as shown in Table II, but the FLICM shows worse performance than other four algorithms for the second synthetic image as shown in Table III. SFFCM shows excellent performance for two synthetic images. We can see that SFFCM is insensitive to both Salt &amp; Pepper noise and Gaussian noise. However, it is sensitive to Gaussian noise when the noise level is high, as shown in Table III. The proposed AFCF shows better performance than comparative algorithms except SFFCM. Especially, MMGR-AFCF obtains the largest mean value and the smallest RMSE of S and SA on two synthetic images, which demonstrates that MMGR-AFCF is robust for different kinds of image corrupted by noise.</p>
        <p>The previous experiments demonstrate that the proposed AFCF is useful for automatic image segmentation, and it is robust for images corrupted by different kinds of noise. To demonstrate further the superiority of AFCF on real image segmentation, we conducted experiments on BSDS500. The parameters values in all algorithms are the same as that in Section IV. B except FRFCM and SFFCM. The related parameters of FRFCM and SFFCM follow the original papers [20] and [23]. In addition, the CIE-Lab color space is used for all algorithms for fair comparison.The previous experiments demonstrate that the proposed AFCF is useful for automatic image segmentation, and it is robust for images corrupted by different kinds of noise. To demonstrate further the superiority of AFCF on real image segmentation, we conducted experiments on BSDS500. The parameters values in all algorithms are the same as that in Section IV. B except FRFCM and SFFCM. The related parameters of FRFCM and SFFCM follow the original papers [20] and [23]. In addition, the CIE-Lab color space is used for all algorithms for fair comparison.</p>
        <p>To evaluate the performance of different algorithms for real image segmentation, we consider the probabilistic rand index (PRI), the covering (CV), the variation of information (VI), the global consistency error (GCE), and the boundary displacement error (BDE) [51], as the performance metrics. Both PRI and CV are usually used for the evaluation of the pixelwise classification task, where PRI is the similarity of labels and CV is the overlap of regions between two clustering results. VI is also used for the purpose of clustering comparison, and it is the distance of average conditional entropy between two clustering results. Additionally, GCE and BDE are often used for the evaluation of image segmenta- Firstly, we demonstrate that the proposed AFCF is able to obtain an accurate number of clusters for real images. Because it is difficult to give an accurate number of clusters for each image in BSDS500, we choose five simple images as shown in Fig. 13. According to the proposed AFCF that includes SLIC-AFCF, LSC-AFCF, and MMGR-AFCF, the corresponding decision-graphs and segmentation results are obtained as shown in Fig. 13. We can see that SLIC-AFCF, LSC-AFCF, and MMGR-AFCF obtain the approximate number of clusters, which means that AFCF obtains and provides similar number of clusters for an image independent of the selected superpixel algorithm.To evaluate the performance of different algorithms for real image segmentation, we consider the probabilistic rand index (PRI), the covering (CV), the variation of information (VI), the global consistency error (GCE), and the boundary displacement error (BDE) [51], as the performance metrics. Both PRI and CV are usually used for the evaluation of the pixelwise classification task, where PRI is the similarity of labels and CV is the overlap of regions between two clustering results. VI is also used for the purpose of clustering comparison, and it is the distance of average conditional entropy between two clustering results. Additionally, GCE and BDE are often used for the evaluation of image segmenta- Firstly, we demonstrate that the proposed AFCF is able to obtain an accurate number of clusters for real images. Because it is difficult to give an accurate number of clusters for each image in BSDS500, we choose five simple images as shown in Fig. 13. According to the proposed AFCF that includes SLIC-AFCF, LSC-AFCF, and MMGR-AFCF, the corresponding decision-graphs and segmentation results are obtained as shown in Fig. 13. We can see that SLIC-AFCF, LSC-AFCF, and MMGR-AFCF obtain the approximate number of clusters, which means that AFCF obtains and provides similar number of clusters for an image independent of the selected superpixel algorithm.</p>
        <p>In Fig. 13, SLIC-AFCF and MMGR-AFCF obtain the same number of cluster for each image, but LSC-AFCF obtains different results for images "97010" and "376020". In practical applications, because three superpixel algorithms lead to different pre-segmentation results, it is impossible to obtain same number of clusters for each image in BSDS500 using three algorithms. However, Fig. 13 shows that three suprepixel algorithms can obtain an approaximate number of clusters. Because the MMGR-WT has been able to provide better superpixel results, we consider the number of clusters provided by MMGR-AFCF for comparative algorithms in the following experiments.In Fig. 13, SLIC-AFCF and MMGR-AFCF obtain the same number of cluster for each image, but LSC-AFCF obtains different results for images "97010" and "376020". In practical applications, because three superpixel algorithms lead to different pre-segmentation results, it is impossible to obtain same number of clusters for each image in BSDS500 using three algorithms. However, Fig. 13 shows that three suprepixel algorithms can obtain an approaximate number of clusters. Because the MMGR-WT has been able to provide better superpixel results, we consider the number of clusters provided by MMGR-AFCF for comparative algorithms in the following experiments.</p>
        <p>To demonstrate further that AFCF is able to generate the best number of clusters, Table V shows the performance of the proposed AFCF on real image segmentation. Because our purpose is to demonstrate that the proposed AFCF can obtain the best number of clusters, we present the average values of To demonstrate further that the proposed AFCF is effective for most images in the BSDS500, we performed all algorithms on each image in BSDS500. For fair comparison, the same value of c obtained by the proposed MMGR-AFCF is used for all algorithms. Experimental results are shown in Figs. these algorithms employ fixed-size windows to obtain spatial neighboring information. The Liu's algorithm, SFFCM, and the proposed AFCF obtain better segmentation results due to the employment of superpixel algorithms, which means that the adaptive spatial information is useful for improving segmentations. However, because MMGR-WT always generates better superpixel areas than SLIC and LSC, MMGR-AFCF provides better segmentation results than SLIC-AFCF and LSC-AFCF. Compared to SFFCM that employs Sobel operators, MMGR-AFCF employs structured forests (SE) [52] to generate a gradient image to be used for MMGR. Therefore, MMGR-AFCF provides better superpixel results than SFFCM. Furthermore, as the former employs prior entropy to obtain the final result, it has clear advantages than the later for image segmentation.To demonstrate further that AFCF is able to generate the best number of clusters, Table V shows the performance of the proposed AFCF on real image segmentation. Because our purpose is to demonstrate that the proposed AFCF can obtain the best number of clusters, we present the average values of To demonstrate further that the proposed AFCF is effective for most images in the BSDS500, we performed all algorithms on each image in BSDS500. For fair comparison, the same value of c obtained by the proposed MMGR-AFCF is used for all algorithms. Experimental results are shown in Figs. these algorithms employ fixed-size windows to obtain spatial neighboring information. The Liu's algorithm, SFFCM, and the proposed AFCF obtain better segmentation results due to the employment of superpixel algorithms, which means that the adaptive spatial information is useful for improving segmentations. However, because MMGR-WT always generates better superpixel areas than SLIC and LSC, MMGR-AFCF provides better segmentation results than SLIC-AFCF and LSC-AFCF. Compared to SFFCM that employs Sobel operators, MMGR-AFCF employs structured forests (SE) [52] to generate a gradient image to be used for MMGR. Therefore, MMGR-AFCF provides better superpixel results than SFFCM. Furthermore, as the former employs prior entropy to obtain the final result, it has clear advantages than the later for image segmentation.</p>
        <p>In Table V, FCM, FGFCM, FLICM, NWFCM, NDFCM, and DSFCM N obtain similar values of PRI, CV, VI, GCE and BDE, which shows that these algorithms have similar performance on real image segmentation. Similarly, HMRF-FCM has a similar performance with FRFCM. Liu's clearly outperforms other algorithms due to the employment of superpixel images generated by mean-shift algorithm. SFFCM obtains better CV, VI, GCE but worse PRI and BDE than Liu's algorithm. The proposed SLIC-AFCF and LSC-AFCF show similar performance with Liu's algorithm. However, they are superior to Liu's algorithm since the proposed AFCF is fully automatic and it has lower computational complexity than Liu's algorithm. The proposed MMGR-AFCF provides the best results in each of the five performance metrics, which demonstrates that MMGR-AFCF is able to obtain excellent segmentation results for real images.In Table V, FCM, FGFCM, FLICM, NWFCM, NDFCM, and DSFCM N obtain similar values of PRI, CV, VI, GCE and BDE, which shows that these algorithms have similar performance on real image segmentation. Similarly, HMRF-FCM has a similar performance with FRFCM. Liu's clearly outperforms other algorithms due to the employment of superpixel images generated by mean-shift algorithm. SFFCM obtains better CV, VI, GCE but worse PRI and BDE than Liu's algorithm. The proposed SLIC-AFCF and LSC-AFCF show similar performance with Liu's algorithm. However, they are superior to Liu's algorithm since the proposed AFCF is fully automatic and it has lower computational complexity than Liu's algorithm. The proposed MMGR-AFCF provides the best results in each of the five performance metrics, which demonstrates that MMGR-AFCF is able to obtain excellent segmentation results for real images.</p>
        <p>Execution time is important because it evaluates the practicability of a segmentation algorithm. We computed the execution time of different algorithms on all images in the BSDS500, and Table VI shows the comparison of average execution time on 500 images. In all comparative algorithms, it is known that the FLICM, HMRF-FCM, NWFCM, and Liu's algorithm are quite time-consuming [16]. Moreover, FRFCM has a lower computational complexity than FGFCM and NDFCM [50]. Therefore, we only present the execution time of FCM, FRFCM, DSFCM N, SFFCM, and the proposed SLIC-AFCF, LSC-AFCM, MMGR-AFCF in Table VI.Execution time is important because it evaluates the practicability of a segmentation algorithm. We computed the execution time of different algorithms on all images in the BSDS500, and Table VI shows the comparison of average execution time on 500 images. In all comparative algorithms, it is known that the FLICM, HMRF-FCM, NWFCM, and Liu's algorithm are quite time-consuming [16]. Moreover, FRFCM has a lower computational complexity than FGFCM and NDFCM [50]. Therefore, we only present the execution time of FCM, FRFCM, DSFCM N, SFFCM, and the proposed SLIC-AFCF, LSC-AFCM, MMGR-AFCF in Table VI.</p>
        <p>In Table VI, we can see that SFFCM is the fastest due to the utilization of superpixel and color histogram. DSFCM N needs a long execution time because the neighboring information is repeatedly computed in each iteration. The proposed SLIC-FCM and LSC-FCM require a similar execution time that is longer than MMGR-AFCF, for both SLIC and LSC has a higher computational complexity than MMGR-WT. Clearly, the proposed MMGR-AFCF is not only faster than all comparative algorithms except SFFCM, but also it is fully automatic for image segmentation.In Table VI, we can see that SFFCM is the fastest due to the utilization of superpixel and color histogram. DSFCM N needs a long execution time because the neighboring information is repeatedly computed in each iteration. The proposed SLIC-FCM and LSC-FCM require a similar execution time that is longer than MMGR-AFCF, for both SLIC and LSC has a higher computational complexity than MMGR-WT. Clearly, the proposed MMGR-AFCF is not only faster than all comparative algorithms except SFFCM, but also it is fully automatic for image segmentation.</p>
        <p>V. CONCLUSION In this paper, we have studied automatic image segmentation algorithms using fuzzy clustering. We proposed an automatic fuzzy clustering framework for image segmentation by integrating superpixel algorithms, density peak clustering, and prior entropy. The proposed AFCF addresses two difficulties that exists in popular algorithms. One is that the proposed AFCF is fully automatic for image segmentation since the number of clusters is obtained automatically. The other one is that the proposed AFCF provides better image segmentation results than popular clustering algorithms due to the employment of superpixel algorithms and the prior entropy. The proposed AFCF was used to segment synthetic and real images. The experimental results demonstrate that the proposed AFCF is able to obtain accurate number of clusters. Moreover, the AFCF is superior to state-of-the-art clustering algorithms because it provides the best segmentation results.V. CONCLUSION In this paper, we have studied automatic image segmentation algorithms using fuzzy clustering. We proposed an automatic fuzzy clustering framework for image segmentation by integrating superpixel algorithms, density peak clustering, and prior entropy. The proposed AFCF addresses two difficulties that exists in popular algorithms. One is that the proposed AFCF is fully automatic for image segmentation since the number of clusters is obtained automatically. The other one is that the proposed AFCF provides better image segmentation results than popular clustering algorithms due to the employment of superpixel algorithms and the prior entropy. The proposed AFCF was used to segment synthetic and real images. The experimental results demonstrate that the proposed AFCF is able to obtain accurate number of clusters. Moreover, the AFCF is superior to state-of-the-art clustering algorithms because it provides the best segmentation results.</p>
        <p>However, we only use the average color in a superpixel area as the features of the superpixel area, which is a drawback for image segmentation. In the future, we will explore feature learning algorithm to achieve better automatic image segmentation algorithms, and we will consider convolutional neural networks (CNN) to extract image features.However, we only use the average color in a superpixel area as the features of the superpixel area, which is a drawback for image segmentation. In the future, we will explore feature learning algorithm to achieve better automatic image segmentation algorithms, and we will consider convolutional neural networks (CNN) to extract image features.</p>
        <p>tion, where GCE computes the global error to which two segmentations are mutually consistent and BDE measures the average displacement error of boundary pixels between two segmentations. In general, a good segmentation corresponds to high value of PRI and CV, but corresponds to low values of VI, GCE, and BDE.tion, where GCE computes the global error to which two segmentations are mutually consistent and BDE measures the average displacement error of boundary pixels between two segmentations. In general, a good segmentation corresponds to high value of PRI and CV, but corresponds to low values of VI, GCE, and BDE.</p>
        <p>This work was supported in part by the National Natural Science Foundation of China under Grant 61871259, Grant 61811530325 (NSFC-RC), Grant 61461025, Grant 61871260, Grant 61672333, Grant 61873155, in part by China Postdoctoral Science Foundation under Grant 2016M602856.This work was supported in part by the National Natural Science Foundation of China under Grant 61871259, Grant 61811530325 (NSFC-RC), Grant 61461025, Grant 61871260, Grant 61672333, Grant 61873155, in part by China Postdoctoral Science Foundation under Grant 2016M602856.</p>
        <p>T. Lei is with the School of Electronical and Information Engineering,T. Lei is with the School of Electronical and Information Engineering,</p>
        <p>In 1983 Professor Nandi co-discovered the three fundamental particles known as W + , W -and Z 0 (by the UA1 team at CERN), providing the evidence for the unification of the electromagnetic and weak forces, for which the Nobel Committee for Physics in 1984 awarded the prize to his two team leaders for their decisive contributions. His current research interests lie in the areas of signal processing and machine learning, with applications to communications, gene expression data, functional magnetic resonance data, and biomedical data. He has made many fundamental theoretical and algorithmic contributions to many aspects of signal processing and machine learning. He has much expertise in "Big Data", dealing with heterogeneous data, and extracting information from multiple datasets obtained in different laboratories and different times.In 1983 Professor Nandi co-discovered the three fundamental particles known as W + , W -and Z 0 (by the UA1 team at CERN), providing the evidence for the unification of the electromagnetic and weak forces, for which the Nobel Committee for Physics in 1984 awarded the prize to his two team leaders for their decisive contributions. His current research interests lie in the areas of signal processing and machine learning, with applications to communications, gene expression data, functional magnetic resonance data, and biomedical data. He has made many fundamental theoretical and algorithmic contributions to many aspects of signal processing and machine learning. He has much expertise in "Big Data", dealing with heterogeneous data, and extracting information from multiple datasets obtained in different laboratories and different times.</p>
    </text>
</tei>
