<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-13T15:18+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Ensemble learning combines several individual models to obtain better generalization performance. Currently, deep learning architectures are showing better performance compared to the shallow or traditional models. Deep ensemble learning models combine the advantages of both the deep learning models as well as the ensemble learning such that the final model has better generalization performance. This paper reviews the state-of-art deep ensemble models and hence serves as an extensive summary for the researchers. The ensemble models are broadly categorized into bagging, boosting, stacking, negative correlation based deep ensemble models, explicit/implicit ensembles, homogeneous/heterogeneous ensemble, decision fusion strategies based deep ensemble models. Applications of deep ensemble models in different domains are also briefly discussed. Finally, we conclude this paper with some potential future research directions.Ensemble learning combines several individual models to obtain better generalization performance. Currently, deep learning architectures are showing better performance compared to the shallow or traditional models. Deep ensemble learning models combine the advantages of both the deep learning models as well as the ensemble learning such that the final model has better generalization performance. This paper reviews the state-of-art deep ensemble models and hence serves as an extensive summary for the researchers. The ensemble models are broadly categorized into bagging, boosting, stacking, negative correlation based deep ensemble models, explicit/implicit ensembles, homogeneous/heterogeneous ensemble, decision fusion strategies based deep ensemble models. Applications of deep ensemble models in different domains are also briefly discussed. Finally, we conclude this paper with some potential future research directions.</p>
        <p>Deep learning architectures have been successfully employed across a wide range of applications from image/video classification to the health care. The success of these models is attributed to the better feature representation via multi layer processing architectures. The deep learning models have been mainly used for classification, regression and clustering problems. Classification problem is defined as the categorization of the new observations based on the hypothesis ‚Ñé learned from the set of training data. The hypothesis ‚Ñé represents a mapping of input data features to the appropriate target labels/classes. The main objective, while learning the hypothesis ‚Ñé, is that it should approximate the true unknown function as close as possible to reduce the generalization error. There exist several applications of these classification algorithms ranging from medical diagnosis to remote sensing. Mathematically,Deep learning architectures have been successfully employed across a wide range of applications from image/video classification to the health care. The success of these models is attributed to the better feature representation via multi layer processing architectures. The deep learning models have been mainly used for classification, regression and clustering problems. Classification problem is defined as the categorization of the new observations based on the hypothesis ‚Ñé learned from the set of training data. The hypothesis ‚Ñé represents a mapping of input data features to the appropriate target labels/classes. The main objective, while learning the hypothesis ‚Ñé, is that it should approximate the true unknown function as close as possible to reduce the generalization error. There exist several applications of these classification algorithms ranging from medical diagnosis to remote sensing. Mathematically,</p>
        <p>where ùë• is the input feature vector, ùëÇ ùëê is the category of the sample ùë•, ùúÉ ùëê is the set of learning parameters of the hypothesis ‚Ñé and Z is the set of class labels. Regression problems deal with the continuous decisions, instead of discrete categories. Mathematically,where ùë• is the input feature vector, ùëÇ ùëê is the category of the sample ùë•, ùúÉ ùëê is the set of learning parameters of the hypothesis ‚Ñé and Z is the set of class labels. Regression problems deal with the continuous decisions, instead of discrete categories. Mathematically,</p>
        <p>where ùë• is the observation vector, ùëÇ ùëü is the output, and ùúÉ ùëü is the set of learning parameters of the hypothesis ‚Ñé.where ùë• is the observation vector, ùëÇ ùëü is the output, and ùúÉ ùëü is the set of learning parameters of the hypothesis ‚Ñé.</p>
        <p>Broadly speaking, there are different approaches of classification like supervised, unsupervised classification, few-shot, one-shot and so on. Here, we only discuss supervised and unsupervised classification problems. In supervised learning, the building of hypothesis ‚Ñé is supervised based on the known output labels provided in the training data samples, while as in unsupervised learning hypothesis ‚Ñé is generated without any supervision as no known output values are available with the training data. This approach, also known as clustering, generates the hypothesis ‚Ñé based on the similarities and dissimilarities present in the training data.Broadly speaking, there are different approaches of classification like supervised, unsupervised classification, few-shot, one-shot and so on. Here, we only discuss supervised and unsupervised classification problems. In supervised learning, the building of hypothesis ‚Ñé is supervised based on the known output labels provided in the training data samples, while as in unsupervised learning hypothesis ‚Ñé is generated without any supervision as no known output values are available with the training data. This approach, also known as clustering, generates the hypothesis ‚Ñé based on the similarities and dissimilarities present in the training data.</p>
        <p>Generally speaking, the goal of generating the hypothesis ‚Ñé in Machine learning area is that it should perform better when applied to unknown data. The performance of the model is measured with respect to the area in which the model is applied. Combining the predictions from several models has proven to be an elegant approach for increasing the performance of the models. Combination of several different predictions from different models to make the final prediction is known as ensemble learning or ensemble model. The ensemble learning involves multiple models combined in some fashion like averaging, voting such that the ensemble model is better than any of the individual models. To prove that average voting in an ensemble is better than individual model, Marquis de Condorcet proposed a theorem wherein he proved that if the probability of each voter being correct is above 0.5 and the voters are independent, then addition of more voters increases the probability of majority vote being correct until it approaches 1 (Condorcet, 1785). Although Marquis de Condorcet proposed this theorem in the field of political science and had no idea of the field of Machine learning, but it is the similar mechanism that leads to better performance of the ensemble models. Assumptions of Marquis de Condorcet theorem also holds true for ensembles (Hansen and Salamon, 1990). The reasons for the success of ensemble learning include: statistical, computational and representation learning (Dietterich, 2000), bias-variance decomposition (Kohavi and Wolpert, 1996) and strength-correlation (Breiman, 2001).Generally speaking, the goal of generating the hypothesis ‚Ñé in Machine learning area is that it should perform better when applied to unknown data. The performance of the model is measured with respect to the area in which the model is applied. Combining the predictions from several models has proven to be an elegant approach for increasing the performance of the models. Combination of several different predictions from different models to make the final prediction is known as ensemble learning or ensemble model. The ensemble learning involves multiple models combined in some fashion like averaging, voting such that the ensemble model is better than any of the individual models. To prove that average voting in an ensemble is better than individual model, Marquis de Condorcet proposed a theorem wherein he proved that if the probability of each voter being correct is above 0.5 and the voters are independent, then addition of more voters increases the probability of majority vote being correct until it approaches 1 (Condorcet, 1785). Although Marquis de Condorcet proposed this theorem in the field of political science and had no idea of the field of Machine learning, but it is the similar mechanism that leads to better performance of the ensemble models. Assumptions of Marquis de Condorcet theorem also holds true for ensembles (Hansen and Salamon, 1990). The reasons for the success of ensemble learning include: statistical, computational and representation learning (Dietterich, 2000), bias-variance decomposition (Kohavi and Wolpert, 1996) and strength-correlation (Breiman, 2001).</p>
        <p>In this era of machine learning, deep learning automates the extraction of high-level features via hierarchical feature learning mechanism wherein the upper layer of features are generated on the previous set of layer/layers. Deep learning has been successfully applied across different fields since the ImageNet Large Scale Recognition Challenge (ILSVRC) competitions (Russakovsky et al., 2015;Krizhevsky et al., 2012) and has achieved state-of-art performance. It has obtained promising results in object detection, semantic segmentation, edge detection and number of other domains. However, given the computational cost, the training of deep ensemble models is an uphill task. Different views have been provided to understand how the deep learning models learn the features like learning through hierarchy of concepts via many levels of representation (Deng and Yu, 2014;Goodfellow et al., 2016;LeCun et al., 2015). Given the advantages of deep learning models from deep architectures, there are several bottlenecks like vanishing/exploding gradients (Hochreiter, 1991;Glorot and Bengio, 2010) and degradation problem (He et al., 2016) which prevent to reach this goal. Recently, training deep network's has become feasible through the Highway networks (Srivastava et al., 2015) and Residual networks (He et al., 2016). Both these networks enabled to train very deep networks. The ensemble learning has been recently known to be strong reason for enhancing the performance of deep learning models (Veit et al., 2016). Thus, the objective of deep ensemble models is to obtain a model that has best of both the ensemble and deep models.In this era of machine learning, deep learning automates the extraction of high-level features via hierarchical feature learning mechanism wherein the upper layer of features are generated on the previous set of layer/layers. Deep learning has been successfully applied across different fields since the ImageNet Large Scale Recognition Challenge (ILSVRC) competitions (Russakovsky et al., 2015;Krizhevsky et al., 2012) and has achieved state-of-art performance. It has obtained promising results in object detection, semantic segmentation, edge detection and number of other domains. However, given the computational cost, the training of deep ensemble models is an uphill task. Different views have been provided to understand how the deep learning models learn the features like learning through hierarchy of concepts via many levels of representation (Deng and Yu, 2014;Goodfellow et al., 2016;LeCun et al., 2015). Given the advantages of deep learning models from deep architectures, there are several bottlenecks like vanishing/exploding gradients (Hochreiter, 1991;Glorot and Bengio, 2010) and degradation problem (He et al., 2016) which prevent to reach this goal. Recently, training deep network's has become feasible through the Highway networks (Srivastava et al., 2015) and Residual networks (He et al., 2016). Both these networks enabled to train very deep networks. The ensemble learning has been recently known to be strong reason for enhancing the performance of deep learning models (Veit et al., 2016). Thus, the objective of deep ensemble models is to obtain a model that has best of both the ensemble and deep models.</p>
        <p>There exist multiple surveys in the literature which mainly focus on the review of ensemble learning like learning of ensemble models in classification problems (Zhao et al., 2005;Rokach, 2010;Gopika and Azhagusundari, 2014;Yang et al., 2010), regression problems (Mendes-Moreira et al., 2012;Ren et al., 2015) and clustering (Vega-Pons and Ruiz-Shulcloper, 2011). Review of both the classification and regression models was given in Ren et al. (2016). Comprehensive review of the ensemble methods and the challenges were given in Sagi and Rokach (2018). Though Sagi and Rokach (2018) provided some insight about the deep ensemble models but could not give the comprehensive review of the deep ensemble learning while as Cao et al. (2020) reviewed the ensemble deep models in the context of bioinformatics. The past decade has successively evolved different deep learning strategies which have lead to the exploration and innovation of these models in multiple areas like health care, speech, image classification, forecasting and other applications. Broadly speaking, ensemble learning approaches have followed classical methods, general methods and different fusion strategies for improving the performance of the models. Since deep learning models are computation and data extensive, hence, ensemble deep learning models need special attention while exploring the complementary information of multiple algorithms into a uniform framework. Ensemble deep learning models need to handle multiple questions like how to induce diversity among the baseline models, how to keep the training time as well the models complexity lower for the practical applications, how to fuse the predictions of the complementary algorithms. Multiple studies have handled these problems differently. In this review paper, we comprehensively review the different approaches used to handle the aforementioned problems. In this paper, we give a comprehensive review of deep ensemble models. To the best of our knowledge, this is the first comprehensive review paper on deep ensemble models.There exist multiple surveys in the literature which mainly focus on the review of ensemble learning like learning of ensemble models in classification problems (Zhao et al., 2005;Rokach, 2010;Gopika and Azhagusundari, 2014;Yang et al., 2010), regression problems (Mendes-Moreira et al., 2012;Ren et al., 2015) and clustering (Vega-Pons and Ruiz-Shulcloper, 2011). Review of both the classification and regression models was given in Ren et al. (2016). Comprehensive review of the ensemble methods and the challenges were given in Sagi and Rokach (2018). Though Sagi and Rokach (2018) provided some insight about the deep ensemble models but could not give the comprehensive review of the deep ensemble learning while as Cao et al. (2020) reviewed the ensemble deep models in the context of bioinformatics. The past decade has successively evolved different deep learning strategies which have lead to the exploration and innovation of these models in multiple areas like health care, speech, image classification, forecasting and other applications. Broadly speaking, ensemble learning approaches have followed classical methods, general methods and different fusion strategies for improving the performance of the models. Since deep learning models are computation and data extensive, hence, ensemble deep learning models need special attention while exploring the complementary information of multiple algorithms into a uniform framework. Ensemble deep learning models need to handle multiple questions like how to induce diversity among the baseline models, how to keep the training time as well the models complexity lower for the practical applications, how to fuse the predictions of the complementary algorithms. Multiple studies have handled these problems differently. In this review paper, we comprehensively review the different approaches used to handle the aforementioned problems. In this paper, we give a comprehensive review of deep ensemble models. To the best of our knowledge, this is the first comprehensive review paper on deep ensemble models.</p>
        <p>The rest of this paper is organized as follows: Section 3 discusses the theoretical aspects of deep ensemble learning, Section 4 discusses the different approaches used in deep ensemble strategies, applications of deep ensemble methods are given in Section 5 and finally conclusions and future directions are given in Section 6 (see Fig. 1).The rest of this paper is organized as follows: Section 3 discusses the theoretical aspects of deep ensemble learning, Section 4 discusses the different approaches used in deep ensemble strategies, applications of deep ensemble methods are given in Section 5 and finally conclusions and future directions are given in Section 6 (see Fig. 1).</p>
        <p>The studies in this review are searched from the 
            <rs type="software">Google Scholar</rs> and Scopus search engines. The papers are the result of ensemble learning, ensemble deep learning, deep ensemble learning, deep ensembles keywords. The articles where screened based on the title and abstract, followed by the screening of full-text version. The articles are elaborated based on the ensemble learning and deep learning approaches.
        </p>
        <p>The various reasons which have been justified for the success of ensemble learning can be discussed under the following subheadings:The various reasons which have been justified for the success of ensemble learning can be discussed under the following subheadings:</p>
        <p>Initially, the success of ensemble methods was theoretically investigated for regression problems. Krogh and Vedelsby (1995) and Brown et al. (2005a) proved via ambiguity decomposition that the proper ensemble classifier guarantees a smaller squared error as compared to the individual predictors of the classifier. Ambiguity decomposition was given for single dataset based ensemble methods, later on, multiple dataset bias-variance-covariance decomposition was introduced in Brown et al. (2005a), Geman et al. (1992), Brown et al. (2005b) and Pedro (2000) and is given as:Initially, the success of ensemble methods was theoretically investigated for regression problems. Krogh and Vedelsby (1995) and Brown et al. (2005a) proved via ambiguity decomposition that the proper ensemble classifier guarantees a smaller squared error as compared to the individual predictors of the classifier. Ambiguity decomposition was given for single dataset based ensemble methods, later on, multiple dataset bias-variance-covariance decomposition was introduced in Brown et al. (2005a), Geman et al. (1992), Brown et al. (2005b) and Pedro (2000) and is given as:</p>
        <p>where ùë° is target, ùëú ùëñ is the output of ùëñth model and ùëÄ is the ensemble size. Here, ùëèùëñùëéùë† term measures the average difference between the base learner and the model output, ùë£ùëéùëü indicates their average variance, and ùëêùëúùë£ùëéùëü is the covariance term measuring the pairwise difference of the base learners.where ùë° is target, ùëú ùëñ is the output of ùëñth model and ùëÄ is the ensemble size. Here, ùëèùëñùëéùë† term measures the average difference between the base learner and the model output, ùë£ùëéùëü indicates their average variance, and ùëêùëúùë£ùëéùëü is the covariance term measuring the pairwise difference of the base learners.</p>
        <p>Ensemble methods have been supported by several theories like bias-variance (Kohavi and Wolpert, 1996;Wolpert, 1997), strength correlation (Breiman, 2001), stochastic discrimination (Kleinberg, 1990), and margin theory (Schapire et al., 1998). These theories provide the equivalent of bias-variance-covariance decomposition (Pisetta, 2012).Ensemble methods have been supported by several theories like bias-variance (Kohavi and Wolpert, 1996;Wolpert, 1997), strength correlation (Breiman, 2001), stochastic discrimination (Kleinberg, 1990), and margin theory (Schapire et al., 1998). These theories provide the equivalent of bias-variance-covariance decomposition (Pisetta, 2012).</p>
        <p>The above given equations of decomposition error cannot be directly applied to the datasets with discrete class labels due to their categorical nature. However, alternate ways to decompose the error in classification problems are given in Kohavi and Wolpert (1996), Kong and Dietterich (1995), Friedman (1997), Breiman (1998) andJames (2003).The above given equations of decomposition error cannot be directly applied to the datasets with discrete class labels due to their categorical nature. However, alternate ways to decompose the error in classification problems are given in Kohavi and Wolpert (1996), Kong and Dietterich (1995), Friedman (1997), Breiman (1998) andJames (2003).</p>
        <p>Multiple approaches like bagging, boosting have been proposed for generating the ensemble methods. Bagging reduces the variance among the base classifiers (Breiman, 1996b) while as boosting based ensembles lead to the bias and variance reduction (Breiman, 1996a;Zhang and Zhang, 2008).Multiple approaches like bagging, boosting have been proposed for generating the ensemble methods. Bagging reduces the variance among the base classifiers (Breiman, 1996b) while as boosting based ensembles lead to the bias and variance reduction (Breiman, 1996a;Zhang and Zhang, 2008).</p>
        <p>Dietterich provided Statistical, Computational and Representational reasons (Dietterich, 2000) for success of ensemble models. The learning model is viewed as the search of the optimal hypothesis ‚Ñé among the several hypothesis in the search space. When the amount of data available for the training is smaller compared to the size of the hypothesis space, the statistical problem arises. Due to this statistical problem, the learning algorithm identifies the different hypothesis which gives same performance on the training samples. Ensembling of these hypothesis results in an algorithm which reduces the risk of being a wrong classifier. The second reason is computational wherein a learning algorithm stucks in a local optima due to some form of local search. Ensemble model overcomes this issue by performing some form of local search via different starting points which leads to better approximation of the true unknown function. Another reason is representational wherein none of the hypotheses among the set of hypothesis is able to represent the true unknown function. Hence, ensembling of these hypothesis via some weighting technique results into the hypothesis which expands the representable function space.Dietterich provided Statistical, Computational and Representational reasons (Dietterich, 2000) for success of ensemble models. The learning model is viewed as the search of the optimal hypothesis ‚Ñé among the several hypothesis in the search space. When the amount of data available for the training is smaller compared to the size of the hypothesis space, the statistical problem arises. Due to this statistical problem, the learning algorithm identifies the different hypothesis which gives same performance on the training samples. Ensembling of these hypothesis results in an algorithm which reduces the risk of being a wrong classifier. The second reason is computational wherein a learning algorithm stucks in a local optima due to some form of local search. Ensemble model overcomes this issue by performing some form of local search via different starting points which leads to better approximation of the true unknown function. Another reason is representational wherein none of the hypotheses among the set of hypothesis is able to represent the true unknown function. Hence, ensembling of these hypothesis via some weighting technique results into the hypothesis which expands the representable function space.</p>
        <p>One of the main reasons behind the success of ensemble methods is increasing the diversity among the base classifiers and the same thing was highlighted in Dietterich (2000). Different approaches have been followed to generate diverse classifiers. Different methods like bootstrap aggregation (bagging) (Breiman, 1996b), Adaptive Boosting (AdaBoost) (Freund and Schapire, 1996), random subspace (Barandiaran, 1998), and random forest (Breiman, 2001) approaches are followed for generating the multiple datasets from the original dataset to train the different predictors such that the outputs of predictors are diverse. Attempts have been made to increase diversity in the output data wherein multiple outputs are created instead of multiple datasets for the supervision of the base learners. 'Output smearing' (Breiman, 2000) is one of this kind which induces random noise to introduce diversity in the output space.One of the main reasons behind the success of ensemble methods is increasing the diversity among the base classifiers and the same thing was highlighted in Dietterich (2000). Different approaches have been followed to generate diverse classifiers. Different methods like bootstrap aggregation (bagging) (Breiman, 1996b), Adaptive Boosting (AdaBoost) (Freund and Schapire, 1996), random subspace (Barandiaran, 1998), and random forest (Breiman, 2001) approaches are followed for generating the multiple datasets from the original dataset to train the different predictors such that the outputs of predictors are diverse. Attempts have been made to increase diversity in the output data wherein multiple outputs are created instead of multiple datasets for the supervision of the base learners. 'Output smearing' (Breiman, 2000) is one of this kind which induces random noise to introduce diversity in the output space.</p>
        <p>The different ensemble strategies have evolved over a period of time which results in better generalization of the learning models. The ensemble strategies are broadly categorized as follows:The different ensemble strategies have evolved over a period of time which results in better generalization of the learning models. The ensemble strategies are broadly categorized as follows:</p>
        <p>Bagging (Breiman, 1996b), also known as bootstrap aggregating, is one of the standard techniques for generating the ensemble-based algorithms. Bagging is applied to enhance the performance of an ensemble classifier. The main idea in bagging is to generate a series of independent observations with the same size, and distribution as that of the original data. Given the series of observations, generate an ensemble predictor which is better than the single predictor generated on the original data. Bagging increases two steps in the original models: First, generating the bagging samples and passing each bag of samples to the base models and second, strategy for combining the predictions of the multiple predictors. Bagging samples may be generated with or without replacement. Combining the output of the base predictors may vary as mostly majority voting is used for classification problems while the averaging strategy is used in regression problems for generating the ensemble output. Fig. 2 shows the diagram of the bagging technique. Here, ùê∑ ùëñ represents the bagged datasets, ùê∂ ùëñ represents the algorithms and ùêπ ùëíùëõùë† calculates the final outcome.Bagging (Breiman, 1996b), also known as bootstrap aggregating, is one of the standard techniques for generating the ensemble-based algorithms. Bagging is applied to enhance the performance of an ensemble classifier. The main idea in bagging is to generate a series of independent observations with the same size, and distribution as that of the original data. Given the series of observations, generate an ensemble predictor which is better than the single predictor generated on the original data. Bagging increases two steps in the original models: First, generating the bagging samples and passing each bag of samples to the base models and second, strategy for combining the predictions of the multiple predictors. Bagging samples may be generated with or without replacement. Combining the output of the base predictors may vary as mostly majority voting is used for classification problems while the averaging strategy is used in regression problems for generating the ensemble output. Fig. 2 shows the diagram of the bagging technique. Here, ùê∑ ùëñ represents the bagged datasets, ùê∂ ùëñ represents the algorithms and ùêπ ùëíùëõùë† calculates the final outcome.</p>
        <p>Random Forest (Breiman, 2001) is an improved version of the decision trees that uses the bagging strategy for improving the predictions of the base classifier which is a decision tree. The fundamental difference between these two methods is that at each tree split in Random Forest, only a subset of features is randomly selected and considered for splitting. The purpose of this method is to decorrelate the trees and prevent over-fitting. Breiman (2001) showed heuristically that the variance of the bagged predictor is smaller than the original predictor and proposed that bagging is better in higher dimensional data. However, the analysis of the smoothing effect of bagging (Buja and Stuetzle, 2000) revealed that bagging does not depend on the data dimensionality. B√ºhlmann and Yu (2002) gave theoretical explanation of how bagging gives smooth hard decisions, small variance, and mean squared error. Since bagging is computationally expensive, hence subbagging and half subbagging (B√ºhlmann and Yu, 2002) were introduced. Half subbagging, being computationally efficient, is as accurate as the bagging.Random Forest (Breiman, 2001) is an improved version of the decision trees that uses the bagging strategy for improving the predictions of the base classifier which is a decision tree. The fundamental difference between these two methods is that at each tree split in Random Forest, only a subset of features is randomly selected and considered for splitting. The purpose of this method is to decorrelate the trees and prevent over-fitting. Breiman (2001) showed heuristically that the variance of the bagged predictor is smaller than the original predictor and proposed that bagging is better in higher dimensional data. However, the analysis of the smoothing effect of bagging (Buja and Stuetzle, 2000) revealed that bagging does not depend on the data dimensionality. B√ºhlmann and Yu (2002) gave theoretical explanation of how bagging gives smooth hard decisions, small variance, and mean squared error. Since bagging is computationally expensive, hence subbagging and half subbagging (B√ºhlmann and Yu, 2002) were introduced. Half subbagging, being computationally efficient, is as accurate as the bagging.</p>
        <p>Several attempts tried to combine bagging with other machine learning algorithms. Kim et al. (2002) used bagging method to generate multiple bags of the dataset and multiple support vector machines were trained independently with each bag as the input. The output of the models is combined via majority voting, least squares estimation weighting and double layer hierarchical approach. In the double layer hierarchical approach, another support vector machines (SVM) is used to combine the outcomes of the multiple SVM's efficiently. Tao et al. (2006) used asymmetric bagging strategy to generate the ensemble model to handle the class imbalance problems. A case study of bagging, boosting and basic ensembles (Mao, 1998) revealed that at higher rejection rates of samples boosting is better as compared to bagging and basic ensembles. However, as the rejection rate increases the difference disappears among the boosting, bagging and basic ensembles. Bagging based multilayer perceptron (Ha et al., 2005) combined bagging to train multiple perceptrons with the corresponding bag and showed that bagging based ensemble models perform better as compared to individual multilayer perceptron. In Gen√ßay and Qi (2001), the analysis of the bagging approach and other regularization techniques revealed that bagging regularized the neural networks and hence provide better generalization. In Khwaja et al. (2015), bagged neural networks (BNNs) was proposed wherein each neural network was trained over different dataset sampled randomly with replacement from original dataset and was implemented for the short term load forecasting. Unlike Random forest (Breiman, 2001) which uses majority voting for aggregating the ensemble of decision trees, bagging based survival trees (Hothorn et al., 2004) used Kaplan-Meier curve to predict the ensemble output for breast cancer and lymphoma patients. In Alvear-Sandoval and Figueiras-Vidal (2018), ensembles of stacked denoising autoencoders for classification showed that the bagging and switching technique in a general deep machine results in improved diversity.Several attempts tried to combine bagging with other machine learning algorithms. Kim et al. (2002) used bagging method to generate multiple bags of the dataset and multiple support vector machines were trained independently with each bag as the input. The output of the models is combined via majority voting, least squares estimation weighting and double layer hierarchical approach. In the double layer hierarchical approach, another support vector machines (SVM) is used to combine the outcomes of the multiple SVM's efficiently. Tao et al. (2006) used asymmetric bagging strategy to generate the ensemble model to handle the class imbalance problems. A case study of bagging, boosting and basic ensembles (Mao, 1998) revealed that at higher rejection rates of samples boosting is better as compared to bagging and basic ensembles. However, as the rejection rate increases the difference disappears among the boosting, bagging and basic ensembles. Bagging based multilayer perceptron (Ha et al., 2005) combined bagging to train multiple perceptrons with the corresponding bag and showed that bagging based ensemble models perform better as compared to individual multilayer perceptron. In Gen√ßay and Qi (2001), the analysis of the bagging approach and other regularization techniques revealed that bagging regularized the neural networks and hence provide better generalization. In Khwaja et al. (2015), bagged neural networks (BNNs) was proposed wherein each neural network was trained over different dataset sampled randomly with replacement from original dataset and was implemented for the short term load forecasting. Unlike Random forest (Breiman, 2001) which uses majority voting for aggregating the ensemble of decision trees, bagging based survival trees (Hothorn et al., 2004) used Kaplan-Meier curve to predict the ensemble output for breast cancer and lymphoma patients. In Alvear-Sandoval and Figueiras-Vidal (2018), ensembles of stacked denoising autoencoders for classification showed that the bagging and switching technique in a general deep machine results in improved diversity.</p>
        <p>Bagging has also been applied to solve the problem of imbalanced data. Roughly Balanced Bagging (Hido et al., 2009) tries to equalize each class's sampling probability in binary class problems wherein the negative class samples are sampled via negative binomial distribution, instead of keeping the sample size of each class the same number. Neighborhood Balanced Bagging (B≈Çaszczy≈Ñski and Stefanowski, 2015) incorporated the neighborhood information for generating the bagging samples for the class imbalance problems. B≈Çaszczy≈Ñski and Stefanowski (2015) concluded that applying conventional diversification is more effective when applied at the last classification methods. Both roughly balanced Bagging and Neighborhood Balanced Bagging have not been explored in deep learning architectures. Thus, these approaches can be exploited to handle the class imbalance problems via deep ensemble models.Bagging has also been applied to solve the problem of imbalanced data. Roughly Balanced Bagging (Hido et al., 2009) tries to equalize each class's sampling probability in binary class problems wherein the negative class samples are sampled via negative binomial distribution, instead of keeping the sample size of each class the same number. Neighborhood Balanced Bagging (B≈Çaszczy≈Ñski and Stefanowski, 2015) incorporated the neighborhood information for generating the bagging samples for the class imbalance problems. B≈Çaszczy≈Ñski and Stefanowski (2015) concluded that applying conventional diversification is more effective when applied at the last classification methods. Both roughly balanced Bagging and Neighborhood Balanced Bagging have not been explored in deep learning architectures. Thus, these approaches can be exploited to handle the class imbalance problems via deep ensemble models.</p>
        <p>The theoretical and experimental analysis of online bagging and boosting (Oza, 2005) showed that the online bagging algorithm can achieve similar accuracy as the batch bagging algorithm with only a little more training time. However, online bagging is an option when all training samples cannot be loaded into the memory due to memory issues.The theoretical and experimental analysis of online bagging and boosting (Oza, 2005) showed that the online bagging algorithm can achieve similar accuracy as the batch bagging algorithm with only a little more training time. However, online bagging is an option when all training samples cannot be loaded into the memory due to memory issues.</p>
        <p>Although ensembling may lead to increase in the computational complexity, but bagging possesses the property that it can be paralleled and can lead to effective reduction in the training time subject to the availability of hardware for running the parallel models. Since deep learning models have high training time, hence optimization of multiple deep models on different training bags is not a feasible option (see Table 1).Although ensembling may lead to increase in the computational complexity, but bagging possesses the property that it can be paralleled and can lead to effective reduction in the training time subject to the availability of hardware for running the parallel models. Since deep learning models have high training time, hence optimization of multiple deep models on different training bags is not a feasible option (see Table 1).</p>
        <p>Boosting technique is used in ensemble models for converting a weak learning model into a learning model with better generalization. Fig. 3 shows the diagram of the boosting technique. The techniques such as majority voting in case of classification problems or a linear combination of weak learners in the regression problems results in better prediction as compared to the single weak learner. Boosting methods like AdaBoost (Freund and Schapire, 1996) and Gradient Boosting (Friedman, 2001) have been used across different domains. AdaBoost uses a greedy technique for minimizing a convex surrogate function upper bounded by misclassification loss via augmentation, at each iteration, the current model with the appropriately weighted predictor. AdaBoost learns an effective ensemble classifier as it leverages the incorrectly classified sample at each stage of the learning. AdaBoost minimizes the exponential loss function while as the Gradient boosting generalized this framework to the arbitrary differential loss function.Boosting technique is used in ensemble models for converting a weak learning model into a learning model with better generalization. Fig. 3 shows the diagram of the boosting technique. The techniques such as majority voting in case of classification problems or a linear combination of weak learners in the regression problems results in better prediction as compared to the single weak learner. Boosting methods like AdaBoost (Freund and Schapire, 1996) and Gradient Boosting (Friedman, 2001) have been used across different domains. AdaBoost uses a greedy technique for minimizing a convex surrogate function upper bounded by misclassification loss via augmentation, at each iteration, the current model with the appropriately weighted predictor. AdaBoost learns an effective ensemble classifier as it leverages the incorrectly classified sample at each stage of the learning. AdaBoost minimizes the exponential loss function while as the Gradient boosting generalized this framework to the arbitrary differential loss function.</p>
        <p>Boosting, also known as forward stagewise additive modeling, was originally proposed to improve the performance of the classification trees. It has been recently incorporated in the deep learning models to further improve their performance.Boosting, also known as forward stagewise additive modeling, was originally proposed to improve the performance of the classification trees. It has been recently incorporated in the deep learning models to further improve their performance.</p>
        <p>Boosted deep belief network (DBN) (Liu et al., 2014) for facial expression recognition unified the boosting technique and multiple To avoid overfitting, incremental Boosting CNN (IBCNN) (Han et al., 2016) accumulated the information of multiple batches of the training data samples. IBCNN uses decision stumps on the top of single neurons as the weak learners and learns weights via AdaBoost method in each mini batch. Unlike DBN (Liu et al., 2014) which uses image patch for learning the weak classifiers, IBCNN trains the weak classifiers from the fully connected layer i.e. the whole image is used for learning the weak classifiers. To make the IBCNN model more efficient, the weak learners loss functions are combined with the global loss function.Boosted deep belief network (DBN) (Liu et al., 2014) for facial expression recognition unified the boosting technique and multiple To avoid overfitting, incremental Boosting CNN (IBCNN) (Han et al., 2016) accumulated the information of multiple batches of the training data samples. IBCNN uses decision stumps on the top of single neurons as the weak learners and learns weights via AdaBoost method in each mini batch. Unlike DBN (Liu et al., 2014) which uses image patch for learning the weak classifiers, IBCNN trains the weak classifiers from the fully connected layer i.e. the whole image is used for learning the weak classifiers. To make the IBCNN model more efficient, the weak learners loss functions are combined with the global loss function.</p>
        <p>Boosted CNN (Moghimi et al., 2016) used boosting for training the deep CNN. Instead of averaging, least squares objective function was used to incorporate the boosting weights into CNN. Moghimi et al. (2016) also showed that CNN can be replaced by network structure within their boosting framework for improving the performance of the base classifier. Boosting increases the complexity of training the networks, hence the concept of dense connections was introduced in a deep boosting framework to overcome the problem of vanishing gradient problem for image denoising (Chen et al., 2018b). Deep boosting framework was extended to image restoration in Chen et al. (2019b) wherein the dilated dense fusion network was used to boost the performance.Boosted CNN (Moghimi et al., 2016) used boosting for training the deep CNN. Instead of averaging, least squares objective function was used to incorporate the boosting weights into CNN. Moghimi et al. (2016) also showed that CNN can be replaced by network structure within their boosting framework for improving the performance of the base classifier. Boosting increases the complexity of training the networks, hence the concept of dense connections was introduced in a deep boosting framework to overcome the problem of vanishing gradient problem for image denoising (Chen et al., 2018b). Deep boosting framework was extended to image restoration in Chen et al. (2019b) wherein the dilated dense fusion network was used to boost the performance.</p>
        <p>The convolutional channel features (Yang et al., 2015) generated the high level features via CNN and then used boosted forest for final classification. Since CNN has high number of hyperparameters than the boosted forest, hence the model proved to be efficient than endto-end training of CNN models both in terms of performance and time. Yang et al. (2015) showed its application in edge detection, object proposal generation, pedestrian and face detection. A stagewise boosting deep CNN (Walach and Wolf, 2016) trains several models of the CNNs within the offline paradigm boosting framework. To extend the concept of boosting in online scenario's wherein only a chunk of data is available at given time, Boosting Independent Embeddings Robustly (BIER) (Opitz et al., 2017) was proposed to cope up the online scenario's. In 
            <rs type="software">BIER</rs>, a single CNN model is trained end-to-end with an online boosting technique. The training set in the BIER is reweighed via the negative gradient of the loss function to project the input spaces (images) into a collection of independent output spaces. To make BIER more robust, Hierarchical Boosted deep metric learning (Waltner et al., 2019) incorporated the hierarchical label information into the embedding ensemble which improves the performance of the model on the large scale image retrieval application. Using deep boosting results in higher training time, to reduce the warm-up phase of training which trains the classifier from scratch deep incremental boosting (Mosca and Magoulas, 2017) used transfer learning approach. This approach leveraged the initial warm-up phase of each incremental base model of the ensemble during the training of the network. To reduce the training time of boosting based ensembles, snapshot boosting (Zhang et al., 2020a) combined the merits of snapshot ensembling and boosting to improve the generalization without increasing the cost of training. Snapshot boosting trains each base network and combines the outputs via meta learner to combine the output of base learners more efficiently.
        </p>
        <p>Literature shows that the boosting concept is the backbone behind well-known architectures like Deep Residual networks (He et al., 2016;Siu, 2019), AdaNet (Cortes et al., 2017) . The theoretical background for the success of the Deep Residual networks (DeepResNet) (He et al., 2016) was explained in the context of boosting theory (Huang et al., 2018). The authors proposed multi-channel telescoping sum boosting learning framework, known as BoostResNet, wherein each channel is a scalar value updated during rounds of boosting to minimize the multiclass error rate. The fundamental difference between the AdaNet and BoostResnet is that the former maps the feature vectors to classifier space and boosts weak classifiers while the latter used multi-channel representation boosting. Moreover, BoostResNet is more efficient than 
            <rs type="software">DeepResnet</rs> in terms of computational time.
        </p>
        <p>The theory of boosting was extended to online boosting in Beygelzimer et al. (2015) and provided theoretical convergence guarantees. Online boosting shows improved convergence guarantees for batch boosting algorithms.The theory of boosting was extended to online boosting in Beygelzimer et al. (2015) and provided theoretical convergence guarantees. Online boosting shows improved convergence guarantees for batch boosting algorithms.</p>
        <p>The ensembles of bagging and boosting have been evaluated in Gonz√°lez et al. (2020). The study evaluated the different algorithms based on the concept of bagging and boosting along with the availability of software tools. The study highlighted the practical issues and opportunities of their feasibility in ensemble modeling (see Table 2).The ensembles of bagging and boosting have been evaluated in Gonz√°lez et al. (2020). The study evaluated the different algorithms based on the concept of bagging and boosting along with the availability of software tools. The study highlighted the practical issues and opportunities of their feasibility in ensemble modeling (see Table 2).</p>
        <p>Ensembling can be done either by combining outputs of multiple base models in some fashion or using some method to choose the ''best" base model. Fig. 4 shows the stacking technique. Stacking is one of the integration techniques wherein the meta-learning model is used to integrate the output of base models. If the final decision part is a linear model, the staking is often referred to as ''model blending'' or simply ''blending''. The concept of stacking or stacked regression was initially given by Wolpert (1992). In this technique, the dataset is randomly split into ùêΩ equal parts. For the ùëóth-fold cross-validation one set is used for testing and the rest are used for training. With these training testing pair subsets, we obtain the predictions of different learning models which are used as the meta-data to build the meta-model. Meta-model makes the final prediction, which is also called the winner-takes-all strategy.Ensembling can be done either by combining outputs of multiple base models in some fashion or using some method to choose the ''best" base model. Fig. 4 shows the stacking technique. Stacking is one of the integration techniques wherein the meta-learning model is used to integrate the output of base models. If the final decision part is a linear model, the staking is often referred to as ''model blending'' or simply ''blending''. The concept of stacking or stacked regression was initially given by Wolpert (1992). In this technique, the dataset is randomly split into ùêΩ equal parts. For the ùëóth-fold cross-validation one set is used for testing and the rest are used for training. With these training testing pair subsets, we obtain the predictions of different learning models which are used as the meta-data to build the meta-model. Meta-model makes the final prediction, which is also called the winner-takes-all strategy.</p>
        <p>Stacking is a bias reducing technique (Leblanc and Tibshirani, 1996). Following Wolpert (1992), Deep convex net (DCN) (Deng and Yu, 2011) was proposed which is a deep learning architecture composed of a variable number of modules stacked together to form the deep architecture. Each learning module in DCN is convex. DCN is a stack of several modules consisting of linear input units, hidden layer non-linear units, and the second linear layer with the number of units as that of target classification classes. The modules are connected layerwise as the output of the lower module is given as input to the adjacent higher module in addition to the original input data. The deep stacking network (DSN) enabling parallel training on very large scale datasets was proposed in Deng et al. (2012c), the network was named stacking based as it shared the concept of ''stacked generalization" (Wolpert, 1992). The kernelized version of DCN, known as kernel deep convex networks (K-DCN), was given in Deng et al. (2012a), here the number of hidden layer approach infinity via kernel trick. Deng et al. (2012a) showed that K-DCN performs better as compared to the DCN. However, due to kernel trick the memory requirements increase and hence may not be scalable to large scale datasets. Also, we need to optimize the hyperparameters like the number of levels in the stacked network, the kernel parameters to get the optimal performance of the network. To leverage the memory requirements, random Fourier feature-based kernel deep convex network (Huang et al., 2013) approximated the Gaussian kernel which reduces the training time and helps in the evaluation of K-DCN over large scale datasets. A framework for parameter estimation and model selection in kernel deep stacking networks (Welchowski and Schmid, 2016) is based on the combination of model-based optimization and hill-climbing approaches. Welchowski and Schmid (2016) used data-driven framework for parameter estimation, hyperparameter tuning and model selection in kernel deep stacking networks. Another improvement over DSN was Tensor Deep Stacking Network (T-DSN) (Hutchinson et al., 2012), here in each block of the stacked network, large single hidden layer was split into two smaller ones and then mapped bilinearly to capture the higher-order interactions among the features. Comprehensive evaluation, the more detailed analysis of the learning algorithm and T-DSN implementation is given in Hutchinson et al. (2013). Sparse coding is another popular method that is used in the deep learning area. The advantage of sparse representation is numerous, including robust to noise, effective for learning useful features, etc. Sparse Deep Stacking Network (S-DSN) is proposed for image classification and abnormal detection (Li et al., 2015;Sun et al., 2018). Li et al. (2015) and Sun et al. (2018) stacked many sparse simplified neural network modules (SNNM) with mixednorm regularization, in which weights are solved by using the convex optimization and the gradient descent algorithm. In order to make sparse SNNM learning the local dependencies between hidden units, Li et al. (2017a) split the hidden units or representations into different groups, which is termed as group sparse DSN (GS-DSN). The DSN idea is also utilized in the Deep Reinforcement Learning field. Zhang et al. (2020b) employed DSN method to integrate the observations from the formal network: Grasp network and Stacking network based on Qlearning algorithm to make an integrated robotic arm system do grasp and place actions. Wang et al. (2020) stacked blocks multiple times to increase the performance of the neural architecture search task. Zhang et al. (2019a) presents a deep hierarchical multi-patch network for image deblurring via stacking approach.Stacking is a bias reducing technique (Leblanc and Tibshirani, 1996). Following Wolpert (1992), Deep convex net (DCN) (Deng and Yu, 2011) was proposed which is a deep learning architecture composed of a variable number of modules stacked together to form the deep architecture. Each learning module in DCN is convex. DCN is a stack of several modules consisting of linear input units, hidden layer non-linear units, and the second linear layer with the number of units as that of target classification classes. The modules are connected layerwise as the output of the lower module is given as input to the adjacent higher module in addition to the original input data. The deep stacking network (DSN) enabling parallel training on very large scale datasets was proposed in Deng et al. (2012c), the network was named stacking based as it shared the concept of ''stacked generalization" (Wolpert, 1992). The kernelized version of DCN, known as kernel deep convex networks (K-DCN), was given in Deng et al. (2012a), here the number of hidden layer approach infinity via kernel trick. Deng et al. (2012a) showed that K-DCN performs better as compared to the DCN. However, due to kernel trick the memory requirements increase and hence may not be scalable to large scale datasets. Also, we need to optimize the hyperparameters like the number of levels in the stacked network, the kernel parameters to get the optimal performance of the network. To leverage the memory requirements, random Fourier feature-based kernel deep convex network (Huang et al., 2013) approximated the Gaussian kernel which reduces the training time and helps in the evaluation of K-DCN over large scale datasets. A framework for parameter estimation and model selection in kernel deep stacking networks (Welchowski and Schmid, 2016) is based on the combination of model-based optimization and hill-climbing approaches. Welchowski and Schmid (2016) used data-driven framework for parameter estimation, hyperparameter tuning and model selection in kernel deep stacking networks. Another improvement over DSN was Tensor Deep Stacking Network (T-DSN) (Hutchinson et al., 2012), here in each block of the stacked network, large single hidden layer was split into two smaller ones and then mapped bilinearly to capture the higher-order interactions among the features. Comprehensive evaluation, the more detailed analysis of the learning algorithm and T-DSN implementation is given in Hutchinson et al. (2013). Sparse coding is another popular method that is used in the deep learning area. The advantage of sparse representation is numerous, including robust to noise, effective for learning useful features, etc. Sparse Deep Stacking Network (S-DSN) is proposed for image classification and abnormal detection (Li et al., 2015;Sun et al., 2018). Li et al. (2015) and Sun et al. (2018) stacked many sparse simplified neural network modules (SNNM) with mixednorm regularization, in which weights are solved by using the convex optimization and the gradient descent algorithm. In order to make sparse SNNM learning the local dependencies between hidden units, Li et al. (2017a) split the hidden units or representations into different groups, which is termed as group sparse DSN (GS-DSN). The DSN idea is also utilized in the Deep Reinforcement Learning field. Zhang et al. (2020b) employed DSN method to integrate the observations from the formal network: Grasp network and Stacking network based on Qlearning algorithm to make an integrated robotic arm system do grasp and place actions. Wang et al. (2020) stacked blocks multiple times to increase the performance of the neural architecture search task. Zhang et al. (2019a) presents a deep hierarchical multi-patch network for image deblurring via stacking approach.</p>
        <p>Since there is no temporal representation of the data in DSNs, they are less effective to the problems where temporal dependencies exist in the input data. To embed the temporal information in DSNs, Recurrent Deep Stacking Networks (R-DSNs) (Palangi et al., 2014) combined the advantages of DSNs and Recurrent neural networks (RNN). Unlike RNN which uses Back Propagation through time for training the network, R-DSNs use Echo State Network (ESN) to initialize the weights and then fine-tuning them via batch-mode gradient descent. A stacked extreme learning machine was proposed in Zhou et al. (2015). Here, at each level of the network ELM with the reduced number of hidden nodes was used to solve the large scale problems. The number of hidden nodes was reduced via the principal component analysis (PCA) reduction technique. Keeping in view the efficiency of stacked models, the number of stacked models based on support vector machine have been proposed (Wang et al., 2019d,a;Li et al., 2019b). Traditional models like Random Forests have also been extended to deep architecture, known as deep forests (Zhou and Feng, 2017), via stacking concept.Since there is no temporal representation of the data in DSNs, they are less effective to the problems where temporal dependencies exist in the input data. To embed the temporal information in DSNs, Recurrent Deep Stacking Networks (R-DSNs) (Palangi et al., 2014) combined the advantages of DSNs and Recurrent neural networks (RNN). Unlike RNN which uses Back Propagation through time for training the network, R-DSNs use Echo State Network (ESN) to initialize the weights and then fine-tuning them via batch-mode gradient descent. A stacked extreme learning machine was proposed in Zhou et al. (2015). Here, at each level of the network ELM with the reduced number of hidden nodes was used to solve the large scale problems. The number of hidden nodes was reduced via the principal component analysis (PCA) reduction technique. Keeping in view the efficiency of stacked models, the number of stacked models based on support vector machine have been proposed (Wang et al., 2019d,a;Li et al., 2019b). Traditional models like Random Forests have also been extended to deep architecture, known as deep forests (Zhou and Feng, 2017), via stacking concept.</p>
        <p>In addition to DSNs, there are some novel network architectures proposed based on the stacking method, Low et al. ( 2019) contributed a stacking-based deep neural network (S-DNN) which is trained without a backpropagation algorithm. Kang et al. (2020) presented a model by stacking conditionally restricted Boltzmann machine and deep neural network, which achieved significant superior performance with fewer parameters and fewer training samples.In addition to DSNs, there are some novel network architectures proposed based on the stacking method, Low et al. ( 2019) contributed a stacking-based deep neural network (S-DNN) which is trained without a backpropagation algorithm. Kang et al. (2020) presented a model by stacking conditionally restricted Boltzmann machine and deep neural network, which achieved significant superior performance with fewer parameters and fewer training samples.</p>
        <p>Negative correlation learning (NCL) (Liu and Yao, 1999) is an important technique for training the learning algorithms. The main concept behind the NCL is to encourage diversity among the individual models of the ensemble to learn the diverse aspects of the training data. NCL minimizes the empirical risk function of the ensemble model via minimization of error functions of the individual networks. NCL (Liu and Yao, 1999) was evaluated for regression as well as classification tasks. The evaluation used different measures like simple averaging and winner-takes-all measures on classification tasks and simple average combination methods for regression problems. The authors figured out that winner-takes-all is better as compared to simple averaging in NCL ensemble models. Shi et al. (2018) proposed deep negative correlation learning architecture for crowd counting known as D-ConvNet i.e. decorrelated convolutional networks. Here, counting is done based on regressionbased ensemble learning from a pool of convolutional feature mapped weak regressors. The main idea behind this is to introduce the NCL concept in deep architectures. Robust regression via deep NCL (Zhang et al., 2019b) is an extension of Shi et al. (2018) in which theoretical insights about the Rademacher complexity are given and extended to more regression-based problems.Negative correlation learning (NCL) (Liu and Yao, 1999) is an important technique for training the learning algorithms. The main concept behind the NCL is to encourage diversity among the individual models of the ensemble to learn the diverse aspects of the training data. NCL minimizes the empirical risk function of the ensemble model via minimization of error functions of the individual networks. NCL (Liu and Yao, 1999) was evaluated for regression as well as classification tasks. The evaluation used different measures like simple averaging and winner-takes-all measures on classification tasks and simple average combination methods for regression problems. The authors figured out that winner-takes-all is better as compared to simple averaging in NCL ensemble models. Shi et al. (2018) proposed deep negative correlation learning architecture for crowd counting known as D-ConvNet i.e. decorrelated convolutional networks. Here, counting is done based on regressionbased ensemble learning from a pool of convolutional feature mapped weak regressors. The main idea behind this is to introduce the NCL concept in deep architectures. Robust regression via deep NCL (Zhang et al., 2019b) is an extension of Shi et al. (2018) in which theoretical insights about the Rademacher complexity are given and extended to more regression-based problems.</p>
        <p>Buschj√§ger et al. ( 2020) formulated a generalized bias-variance decomposition method to control the diversity and smoothly interpolates. They present the Generalized Negative Correlation Learning (GNCL) algorithm, which can encapsulate many existing works in literature and achieve superior performance.Buschj√§ger et al. ( 2020) formulated a generalized bias-variance decomposition method to control the diversity and smoothly interpolates. They present the Generalized Negative Correlation Learning (GNCL) algorithm, which can encapsulate many existing works in literature and achieve superior performance.</p>
        <p>The NCL can also be employed for incremental learning tasks. Muhlbaier and Polikar (2007) employed a dynamically modified weighted majority voting strategy to combine the sub-classifiers. Tang et al. (2009) proposed a negative correlation learning (NCL) based approach for ensemble incremental learning.The NCL can also be employed for incremental learning tasks. Muhlbaier and Polikar (2007) employed a dynamically modified weighted majority voting strategy to combine the sub-classifiers. Tang et al. (2009) proposed a negative correlation learning (NCL) based approach for ensemble incremental learning.</p>
        <p>Ensembling of deep neural networks does not seem to be an easy option as it may lead to increase in computational cost heavily due to the training of multiple neural networks. High performance hardware's with GPU acceleration may take weeks of weeks to train the deep networks. Implicit/Explicit ensembles obtain the contradictory goal wherein a single model is trained in such a manner that it behaves like ensemble of training multiple neural networks without incurring additional cost or to keep the additional cost as minimum as possible.Ensembling of deep neural networks does not seem to be an easy option as it may lead to increase in computational cost heavily due to the training of multiple neural networks. High performance hardware's with GPU acceleration may take weeks of weeks to train the deep networks. Implicit/Explicit ensembles obtain the contradictory goal wherein a single model is trained in such a manner that it behaves like ensemble of training multiple neural networks without incurring additional cost or to keep the additional cost as minimum as possible.</p>
        <p>Here, the training time of an ensemble is same as the training time of a single model. In implicit ensembles, the model parameters are shared and the single unthinned network at test times approximates the model averaging of the ensemble models. However, in explicit ensembles model parameters are not shared and the ensemble output is taken as the combination of the predictions of the ensemble models via different approaches like majority voting, averaging and so on.Here, the training time of an ensemble is same as the training time of a single model. In implicit ensembles, the model parameters are shared and the single unthinned network at test times approximates the model averaging of the ensemble models. However, in explicit ensembles model parameters are not shared and the ensemble output is taken as the combination of the predictions of the ensemble models via different approaches like majority voting, averaging and so on.</p>
        <p>Dropout (Srivastava et al., 2014) creates an ensemble network by randomly dropping out hidden nodes from the network during the training of the network. During the time of testing, all nodes are active. Dropout provides regularization of the network to avoid overfitting and introduces sparsity in the output vectors. Overfitting is reduced as it trains exponential number of models with shared weights and provides an implicit ensemble of networks during testing. Dropping the units randomly avoids coadaptation of the units by making the presence of a particular unit unreliable. The network with dropout takes 2 -3 times more time for training as compared to a standard neural network. Hence, a balance is to be set appropriately between the training time of the network and the overfitting. Generalization of DropOut was given in 
            <rs type="software">DropConnect</rs> (Wan et al., 2013). Unlike DropOut which drops each output unit, 
            <rs type="software">DropConnect</rs> randomly drops each connection and hence, introduces sparsity in the weight parameters of the model. Similar to DropOut, 
            <rs type="software">DropConnect</rs> creates an implicit ensemble during test time by dropping out the connections (setting weights to zero) during training. Both DropOut and 
            <rs type="software">DropConnect</rs> suffer from high training time. To alleviate this problem, deep networks with Stochastic depth (Huang et al., 2016b) aimed to reduce the network depth during training while keeping it unchanged during testing of the network. Stochastic depth is an improvement on ResNet (He et al., 2016) wherein residual blocks are randomly dropped during training and bypassing these transformation blocks connections via skip connections. Swapout (Singh et al., 2016) is a generalization of DropOut and Stochastic depth. Swapout involves dropping of individual units or to skip the blocks randomly. Embarking on a distinctive approach of reducing the test time, distilling the knowledge in a network (Hinton et al., 2015) transferred the ''knowledge" from ensembles to a single model. Gradual DropIn or regularized DropIn Smith et al. (2016) of layers starts from a shallow network wherein the layers are added gradually. DropIN trains the exponential number of thinner networks, similar to 
            <rs type="software">DropOut</rs>, and also shallower networks.
        </p>
        <p>All the aforementioned methods provided an ensemble of networks by sharing the weights. There have been attempts to explore explicit ensembles in which models do not share the weights. Snapshot ensembling (Huang et al., 2017a) develops an explicit ensemble without sharing the weights. The authors exploited good and bad local minima and let the stochastic gradient descent (SGD) converge ùëÄ-times to local minima along the optimization path and take the snapshots only when the model reaches the minimum. These snapshots are then ensembled by averaging at multiple local minima for object recognition. The training time of the ensemble is the same as that of the single model. The ensemble out is taken as the average of the output of the snapshot outputs at multiple local minimas. Random vector functional link network (Pao et al., 1994;Malik et al., 2022) has also been explored for creating the explicit ensembles (Shi et al., 2021) where different random initialization of the hidden layer weights in a hierarchy diversifies the ensemble predictions.All the aforementioned methods provided an ensemble of networks by sharing the weights. There have been attempts to explore explicit ensembles in which models do not share the weights. Snapshot ensembling (Huang et al., 2017a) develops an explicit ensemble without sharing the weights. The authors exploited good and bad local minima and let the stochastic gradient descent (SGD) converge ùëÄ-times to local minima along the optimization path and take the snapshots only when the model reaches the minimum. These snapshots are then ensembled by averaging at multiple local minima for object recognition. The training time of the ensemble is the same as that of the single model. The ensemble out is taken as the average of the output of the snapshot outputs at multiple local minimas. Random vector functional link network (Pao et al., 1994;Malik et al., 2022) has also been explored for creating the explicit ensembles (Shi et al., 2021) where different random initialization of the hidden layer weights in a hierarchy diversifies the ensemble predictions.</p>
        <p>Explicit/implicit produce ensembles out of a single network at the expense of base model diversity (Cao et al., 2020) as the lower level features across the models are likely to be the same. To alleviate this issue, branching based deep models (Han et al., 2017) branch the network to induce more diversity. Motivated by different initializations of the neural networks leads to different local minima, Xue et al. (2021) proposed deep ensemble model wherein ensemble of fully convolution neural network over multiloss module with coarse fine compensation module resulted in better segmentation of central serous chorioretinopathy lesion. Multiple neural networks with different initializations, multiple loss functions resulted in better diversity in an ensemble (see Table 3).Explicit/implicit produce ensembles out of a single network at the expense of base model diversity (Cao et al., 2020) as the lower level features across the models are likely to be the same. To alleviate this issue, branching based deep models (Han et al., 2017) branch the network to induce more diversity. Motivated by different initializations of the neural networks leads to different local minima, Xue et al. (2021) proposed deep ensemble model wherein ensemble of fully convolution neural network over multiloss module with coarse fine compensation module resulted in better segmentation of central serous chorioretinopathy lesion. Multiple neural networks with different initializations, multiple loss functions resulted in better diversity in an ensemble (see Table 3).</p>
        <p>Homogeneous ensemble (HOE) and heterogeneous ensemble (HEE) involve training a group of base learners either from the same family or different families, as shown in Fig. 5 and Fig. 6, respectively. Hence, each model of an ensemble must be as diverse as possible, and each base model must perform better than the random guess. The base learner can be a decision tree, neural network, or any other learning model.Homogeneous ensemble (HOE) and heterogeneous ensemble (HEE) involve training a group of base learners either from the same family or different families, as shown in Fig. 5 and Fig. 6, respectively. Hence, each model of an ensemble must be as diverse as possible, and each base model must perform better than the random guess. The base learner can be a decision tree, neural network, or any other learning model.</p>
        <p>In homogeneous ensembles, the same base learner is used multiple times to generate the family of base classifiers. However, the key issue is to train each base model such that the ensemble model is as diverse as possible, i.e. no two models are making the same error on a particular data sample. The two most common ways of inducing randomness in a homogeneous ensemble are either sampling of the training set multiple times, thereby training each model on a different bootstrapped sample of the training data or sampling the feature space of the training data and train each model on a different feature subset of the training data. In some ensemble models like Random forest (Breiman, 2001) used both these techniques for introducing diversity in the ensemble of decision trees. In neural networks, training models independently with different initialization of the models also induces diversity. However, deep learning models have high training costs and hence, training of multiple deep learning models is not a feasible option. Some attempts, like horizontal vertical voting of deep ensembles (Xie et al., 2013) for image classification (Ciregan et al., 2012) and for disease prediction (Grassmann et al., 2018) showed that better performance is achieved via an ensemble of multiple networks and averaging the outputs. Despite these models, training multiple deep learning models for ensemble is an uphill task as millions or billions of parameters need to be optimized. Hence, some studies have used deep learning in combination with traditional models to build heterogeneous ensemble models, enjoying the benefits of lower computation and higher diversity. Heterogeneous ensemble for default prediction (Li et al., 2018)In homogeneous ensembles, the same base learner is used multiple times to generate the family of base classifiers. However, the key issue is to train each base model such that the ensemble model is as diverse as possible, i.e. no two models are making the same error on a particular data sample. The two most common ways of inducing randomness in a homogeneous ensemble are either sampling of the training set multiple times, thereby training each model on a different bootstrapped sample of the training data or sampling the feature space of the training data and train each model on a different feature subset of the training data. In some ensemble models like Random forest (Breiman, 2001) used both these techniques for introducing diversity in the ensemble of decision trees. In neural networks, training models independently with different initialization of the models also induces diversity. However, deep learning models have high training costs and hence, training of multiple deep learning models is not a feasible option. Some attempts, like horizontal vertical voting of deep ensembles (Xie et al., 2013) for image classification (Ciregan et al., 2012) and for disease prediction (Grassmann et al., 2018) showed that better performance is achieved via an ensemble of multiple networks and averaging the outputs. Despite these models, training multiple deep learning models for ensemble is an uphill task as millions or billions of parameters need to be optimized. Hence, some studies have used deep learning in combination with traditional models to build heterogeneous ensemble models, enjoying the benefits of lower computation and higher diversity. Heterogeneous ensemble for default prediction (Li et al., 2018)</p>
        <p>Ensemble learning trains several base learners and aggregates the outputs of base learners using some rules. The rule used to combine the outputs determines the effective performance of an ensemble. Most of the ensemble models focus on the ensemble architecture followed by their naive averaging to predict the ensemble output. However, naive averaging of the models, followed in most of the ensemble models, is not data adaptive and leads to less optimal performance (Ju et al., 2018) as it is sensitive to the performance of the biased learners. As there are billions of hyperparameters in deep learning architecture, the issue of overfitting may lead to the failure of some base learners. Hence, to overcome these issues, approaches like Bayes optimal classifier and super learner have been followed (Ju et al., 2018).Ensemble learning trains several base learners and aggregates the outputs of base learners using some rules. The rule used to combine the outputs determines the effective performance of an ensemble. Most of the ensemble models focus on the ensemble architecture followed by their naive averaging to predict the ensemble output. However, naive averaging of the models, followed in most of the ensemble models, is not data adaptive and leads to less optimal performance (Ju et al., 2018) as it is sensitive to the performance of the biased learners. As there are billions of hyperparameters in deep learning architecture, the issue of overfitting may lead to the failure of some base learners. Hence, to overcome these issues, approaches like Bayes optimal classifier and super learner have been followed (Ju et al., 2018).</p>
        <p>The different approaches followed in the literature for combining the outputs of the ensemble models are:The different approaches followed in the literature for combining the outputs of the ensemble models are:</p>
        <p>Unweighted averaging of the outputs of the base learners in an ensemble is the most followed approach for fusing the decisions in the literature. Here, the outcomes of the base learners are averaged to get the final prediction of the ensemble model. Deep learning architectures have high variance and low bias, thus, simple averaging of the ensemble models improve the generalization performance due to the reduction of the variance among the models.Unweighted averaging of the outputs of the base learners in an ensemble is the most followed approach for fusing the decisions in the literature. Here, the outcomes of the base learners are averaged to get the final prediction of the ensemble model. Deep learning architectures have high variance and low bias, thus, simple averaging of the ensemble models improve the generalization performance due to the reduction of the variance among the models.</p>
        <p>The averaging of the base learners is performed either on the outputs of the base learners directly or on the predicted probabilities of the classes via softmax function:The averaging of the base learners is performed either on the outputs of the base learners directly or on the predicted probabilities of the classes via softmax function:</p>
        <p>where ùëÉ ùëó ùëñ is the probability outcome of the ùëñth unit on the ùëóth base learner, ùëÇ ùëó ùëñ is the output of the ùëñth unite of the ùëóth base learner and ùêæ is the number of the classes.where ùëÉ ùëó ùëñ is the probability outcome of the ùëñth unit on the ùëóth base learner, ùëÇ ùëó ùëñ is the output of the ùëñth unite of the ùëóth base learner and ùêæ is the number of the classes.</p>
        <p>Unweighted averaging is a reasonable choice when the performance of the base learners is comparable, as suggested in He et al. (2016), Simonyan and Zisserman (2014) and Szegedy et al. (2015). However, when the ensemble contains heterogeneous base learners naive unweighted averaging may result in suboptimal performance as it is affected by the performance of the weak learners and the overconfident learners (Ju et al., 2018). The adaptive metalearner should be good enough to adaptively combine the strengths of the base learners as some learners may have lower overall performance but maybe good at the classification of certain subclasses and hence, leading to better overall performance.Unweighted averaging is a reasonable choice when the performance of the base learners is comparable, as suggested in He et al. (2016), Simonyan and Zisserman (2014) and Szegedy et al. (2015). However, when the ensemble contains heterogeneous base learners naive unweighted averaging may result in suboptimal performance as it is affected by the performance of the weak learners and the overconfident learners (Ju et al., 2018). The adaptive metalearner should be good enough to adaptively combine the strengths of the base learners as some learners may have lower overall performance but maybe good at the classification of certain subclasses and hence, leading to better overall performance.</p>
        <p>Similar to unweighted averaging, majority voting combines the outputs of the base learners. However, instead of taking the average of the probability outcomes, majority voting counts the votes of the base learners and predicts the final labels as the label with the majority of votes. In comparison to unweighted averaging, majority voting is less biased towards the outcome of a particular base learner as the effect is mitigated by majority vote count. However, favoring of a particular event by most of the similar base learners or dependent base learners leads to the dominance of the event in the ensemble model. In majority voting, the analysis by Kuncheva et al. (2003) showed that the pairwise dependence among the base learners plays an important role and for the classification of images, the prediction of shallow networks is more diverse as compared to the deeper networks (Choromanska et al., 2015). Hence, Ju et al. (2018) hypothesized that the performance of the majority voting based shallows ensemble models is better as compared to the majority based deep ensemble models.Similar to unweighted averaging, majority voting combines the outputs of the base learners. However, instead of taking the average of the probability outcomes, majority voting counts the votes of the base learners and predicts the final labels as the label with the majority of votes. In comparison to unweighted averaging, majority voting is less biased towards the outcome of a particular base learner as the effect is mitigated by majority vote count. However, favoring of a particular event by most of the similar base learners or dependent base learners leads to the dominance of the event in the ensemble model. In majority voting, the analysis by Kuncheva et al. (2003) showed that the pairwise dependence among the base learners plays an important role and for the classification of images, the prediction of shallow networks is more diverse as compared to the deeper networks (Choromanska et al., 2015). Hence, Ju et al. (2018) hypothesized that the performance of the majority voting based shallows ensemble models is better as compared to the majority based deep ensemble models.</p>
        <p>Voting methods have also started to be integrated with semisupervised deep learning. Li et al. (2017b) proposed an ensemble semi-supervised deep acoustic models for in automatic speech recognition. Wang et al. (2019c) explored an ensemble self-learning method to enhance semi-supervised performance and extract adverse drug events from social media in Liu et al. (2018). In the semi-supervised classification area, the author proposed a deep coupled ensemble learning method which is combined with complementary consistency regularization and gets the state of the art performance in Li et al. (2019a). Some results have also been achieved with semi-supervised ensemble learning on some datasets where the annotation is costly. Pio et al. (2014) employed an ensemble method to improve the reliability of miRNA:miRNA predicted interactions.Voting methods have also started to be integrated with semisupervised deep learning. Li et al. (2017b) proposed an ensemble semi-supervised deep acoustic models for in automatic speech recognition. Wang et al. (2019c) explored an ensemble self-learning method to enhance semi-supervised performance and extract adverse drug events from social media in Liu et al. (2018). In the semi-supervised classification area, the author proposed a deep coupled ensemble learning method which is combined with complementary consistency regularization and gets the state of the art performance in Li et al. (2019a). Some results have also been achieved with semi-supervised ensemble learning on some datasets where the annotation is costly. Pio et al. (2014) employed an ensemble method to improve the reliability of miRNA:miRNA predicted interactions.</p>
        <p>Furthermore, the multi-label classification (Tsoumakas and Katakis, 2007) problem is also a major point addressed by the voting method, a typical application is the RAndom ùëò-labELsets (RAKEL) algorithm (Tsoumakas and Vlahavas, 2007). The author trained several singlelabel classifiers using small random subsets of actual labels. Then the final output is carried out by a voting scheme based on the predictions of these single classifiers. There are also many variants of RAKEL proposed in recent years (Moyano et al., 2019;Kimura et al., 2016;Wang et al., 2021). Shi et al. (2011) proposed a solution for multilabel ensemble learning problem, which construct several accurate and diverse multi-label based basic classifiers and employ two objective functions to evaluate the accuracy and diversity of multi-label base learners. Another work (Li et al., 2013) proposed an ensemble multilabel classification framework based on variable pairwise constraint projection. Xia et al. (2021) proposed a weighted stacked ensemble scheme that employs the sparsity regularization to facilitate classifier selection and ensemble construction. Besides, there are many applications of ensemble multi-label methods. Some publications employ multi-label ensemble classifiers to explore the protein, such as protein subcellular localization (Guo et al., 2016), protein function prediction (Yu et al., 2012), etc. The Muli-label classifier is also utilized in predicting the drug side effects (Zhang et al., 2015), predicting the gene prediction (Schietgat et al., 2010), etc. Moreover, there is another critical ensemble multi-label algorithm called ensemble classifier chains (ECC) (Read et al., 2011). This method involves binary classifiers linked along a chain. The first classifier is trained using only the input data, and then each subsequent classifier is trained on the input space and all previous classifiers in the chain. The final prediction is obtained by the integration of the predictions and selection above a manually set threshold. Chen et al. (2017) propose an ensemble application of convolutional and recurrent neural networks to capture both the global and local textual semantics and to model high-order label correlations.Furthermore, the multi-label classification (Tsoumakas and Katakis, 2007) problem is also a major point addressed by the voting method, a typical application is the RAndom ùëò-labELsets (RAKEL) algorithm (Tsoumakas and Vlahavas, 2007). The author trained several singlelabel classifiers using small random subsets of actual labels. Then the final output is carried out by a voting scheme based on the predictions of these single classifiers. There are also many variants of RAKEL proposed in recent years (Moyano et al., 2019;Kimura et al., 2016;Wang et al., 2021). Shi et al. (2011) proposed a solution for multilabel ensemble learning problem, which construct several accurate and diverse multi-label based basic classifiers and employ two objective functions to evaluate the accuracy and diversity of multi-label base learners. Another work (Li et al., 2013) proposed an ensemble multilabel classification framework based on variable pairwise constraint projection. Xia et al. (2021) proposed a weighted stacked ensemble scheme that employs the sparsity regularization to facilitate classifier selection and ensemble construction. Besides, there are many applications of ensemble multi-label methods. Some publications employ multi-label ensemble classifiers to explore the protein, such as protein subcellular localization (Guo et al., 2016), protein function prediction (Yu et al., 2012), etc. The Muli-label classifier is also utilized in predicting the drug side effects (Zhang et al., 2015), predicting the gene prediction (Schietgat et al., 2010), etc. Moreover, there is another critical ensemble multi-label algorithm called ensemble classifier chains (ECC) (Read et al., 2011). This method involves binary classifiers linked along a chain. The first classifier is trained using only the input data, and then each subsequent classifier is trained on the input space and all previous classifiers in the chain. The final prediction is obtained by the integration of the predictions and selection above a manually set threshold. Chen et al. (2017) propose an ensemble application of convolutional and recurrent neural networks to capture both the global and local textual semantics and to model high-order label correlations.</p>
        <p>In Bayesian method, hypothesis ‚Ñé ùëó of each base learner with the conditional distribution of target label ùë° given ùë•. Let ‚Ñé ùëó be the hypothesis generated on the training data ùê∑ evaluated on test data (ùë•, ùë°), mathematically, ‚Ñé ùëó (ùë°|ùë•) = ùëÉ [ùë¶|ùë•, ‚Ñé ùëó , ùê∑]. With Bayes rule, we haveIn Bayesian method, hypothesis ‚Ñé ùëó of each base learner with the conditional distribution of target label ùë° given ùë•. Let ‚Ñé ùëó be the hypothesis generated on the training data ùê∑ evaluated on test data (ùë•, ùë°), mathematically, ‚Ñé ùëó (ùë°|ùë•) = ùëÉ [ùë¶|ùë•, ‚Ñé ùëó , ùê∑]. With Bayes rule, we have</p>
        <p>and the Bayesian Optimal classifier is given as:and the Bayesian Optimal classifier is given as:</p>
        <p>where ùëÉ [ùê∑|‚Ñé ùëó ] = ùõ± (ùë°,ùë•)‚ààùê∑ ‚Ñé ùëó (ùë°|ùë•) is the likelihood of the data under ‚Ñé ùëó . However, due to overfitting issues this might be not a good measure. Hence, training data is divided into two sets-one for training the model and the other for evaluating the model. Usually validation set is used to tune the hyperparameters of the model. Choosing prior probabilities in Bayes optimal classifier is difficult and hence, usually set to uniform distribution for simplicity. With a large sample size, one hypothesis tends to give larger posterior probabilities than others and hence the weight vector is dominated by a single base learner and hence Bayes optimal classifier would behave as the discrete super learner with a negative likelihood loss function.where ùëÉ [ùê∑|‚Ñé ùëó ] = ùõ± (ùë°,ùë•)‚ààùê∑ ‚Ñé ùëó (ùë°|ùë•) is the likelihood of the data under ‚Ñé ùëó . However, due to overfitting issues this might be not a good measure. Hence, training data is divided into two sets-one for training the model and the other for evaluating the model. Usually validation set is used to tune the hyperparameters of the model. Choosing prior probabilities in Bayes optimal classifier is difficult and hence, usually set to uniform distribution for simplicity. With a large sample size, one hypothesis tends to give larger posterior probabilities than others and hence the weight vector is dominated by a single base learner and hence Bayes optimal classifier would behave as the discrete super learner with a negative likelihood loss function.</p>
        <p>Stacked generalization (Wolpert, 1992) works by deducing the biases of the generalizer(s) with respect to a provided learning set. To obtain the good linear combination of the base learners in regression, cross-validation data and least squares under non-negativity constraints was used to get the optimal weights of combination (Breiman, 1996c). Consider the linear combination of the predictions of the base learners ùëì 1 , ùëì 2 , ‚Ä¶ , ùëì ùëö given as:Stacked generalization (Wolpert, 1992) works by deducing the biases of the generalizer(s) with respect to a provided learning set. To obtain the good linear combination of the base learners in regression, cross-validation data and least squares under non-negativity constraints was used to get the optimal weights of combination (Breiman, 1996c). Consider the linear combination of the predictions of the base learners ùëì 1 , ùëì 2 , ‚Ä¶ , ùëì ùëö given as:</p>
        <p>where ùë§ is the optimal weight vector learned by the meta learner.where ùë§ is the optimal weight vector learned by the meta learner.</p>
        <p>Inspired by the cross validation for choosing the optimal classifier, Van der Laan et al. (2007) proposed super learner which is weighted combination of the predictions of the base learner. Unlike the stacking approach, it uses cross validation approach to select the optimal weights for combining the predictions of the base learners.Inspired by the cross validation for choosing the optimal classifier, Van der Laan et al. (2007) proposed super learner which is weighted combination of the predictions of the base learner. Unlike the stacking approach, it uses cross validation approach to select the optimal weights for combining the predictions of the base learners.</p>
        <p>With smaller datasets, cross validation approach can be used to optimize the weights. However, with the increase in the size of the data and the number of base learners in the model, it may not be a feasible option. Instead of optimizing the V-fold cross validation, single split cross validation can also be used for optimizing the weights for optimal combination (Ju et al., 2019). In deep learning models, usually, a validation set is used to evaluate the performance instead of using the cross validation.With smaller datasets, cross validation approach can be used to optimize the weights. However, with the increase in the size of the data and the number of base learners in the model, it may not be a feasible option. Instead of optimizing the V-fold cross validation, single split cross validation can also be used for optimizing the weights for optimal combination (Ju et al., 2019). In deep learning models, usually, a validation set is used to evaluate the performance instead of using the cross validation.</p>
        <p>Another application field for super learner is in Reinforcement Learning With the development of Deep learning, some researchers have implemented deep reinforcement learning, which combines deep learning with a Q-learning algorithm (Mnih et al., 2013)Another application field for super learner is in Reinforcement Learning With the development of Deep learning, some researchers have implemented deep reinforcement learning, which combines deep learning with a Q-learning algorithm (Mnih et al., 2013)</p>
        <p>Unsupervised learning is another group of machine learning techniques. The fundamental difference between it and supervised learning is that unsupervised learning usually handles training samples without corresponding labels. Therefore, the primary usage of unsupervised learning is to do clustering. The reason why ensemble methods are employed is to combine some weak clusters into strong one. To create diverse clusters, several approaches can be applied: using different sampling data, using different subsets of the original features, and employing different clustering methods (≈ûenbabaoƒülu et al., 2014). Sometimes, even some random noise can be added to these base models to increase randomness, which is good for ensemble methods according to Bian and Wang (2007). After receiving all the outputs from each cluster, various consensus functions can be chosen to obtain the final output based on the user's requirement (Vega-Pons and Ruiz-Shulcloper, 2011). The ensemble clustering is also known as consensus clustering Fig. 7. Zhou and Tang (2006) explored ensemble methods for unsupervised learning and developed four different approaches to combine the outputs of these clusters. In recent years, some new ensemble clustering methods have been proposed that illustrated the priority of ensemble learning (Huang et al., 2017b;Zheng et al., 2010;Huang et al., 2016a). Most of the clustering ensemble method is based on the co-association matrix solution, which can be regarded as a graph partition problem. Besides, there is some research focus on integrating the deep structure and ensemble clustering method. Liu et al. (2015Liu et al. ( , 2016) ) firstly showed that ensemble unsupervised representation learning with deep structure can be applied in large scale data. Then the author combined the method with auto-encoder and extends it to the vision field. Shaham et al. (2016) first demonstrated that some crowdsourcing algorithms can be replaced by a Restricted Boltzmann Machine with a single hidden neuron, then propose an RBM-based Deep Neural Net (DNN) used for unsupervised ensemble learning. The unsupervised ensemble method also makes some contribution to the field of Natural Language Processing. Alami et al. (2019) demonstrated that the ensemble of unsupervised deep neural network models that use Sentence2Vec representation as the input has the best performance according to the experiments. Hassan et al. (2019) proposed a module that includes four semantic similarity measures, which improves the performance on the semantic textual similarity (STS) task. The unsupervised ensemble method is also widely used for tasks that lack annotation, such as the medical image. Ahn et al. (2019) proposed an unsupervised feature learning method integrated ensemble approach with a traditional convolutional neural network. Lahiri et al. (2016) employed unsupervised hierarchical feature learning with ensemble sparsely autoencoder on retinal blood vessels segmentation task, meanwhile, Liu et al. (2019) also propose an unsupervised ensemble architecture to automatically segment retinal vessel. Besides, there are also some ensemble deep methods working on localization predicting for long non-coding RNAs Cao et al. (2018). Hu and Suganthan (2022) extended the ensemble random vector functional link to unsupervised tasks. The authors employ manifold regularization to re-represent the original features, and then use the Kuhn-Munkre algorithm with consensus clustering to ensemble the clustering results from multiple hidden layers.Unsupervised learning is another group of machine learning techniques. The fundamental difference between it and supervised learning is that unsupervised learning usually handles training samples without corresponding labels. Therefore, the primary usage of unsupervised learning is to do clustering. The reason why ensemble methods are employed is to combine some weak clusters into strong one. To create diverse clusters, several approaches can be applied: using different sampling data, using different subsets of the original features, and employing different clustering methods (≈ûenbabaoƒülu et al., 2014). Sometimes, even some random noise can be added to these base models to increase randomness, which is good for ensemble methods according to Bian and Wang (2007). After receiving all the outputs from each cluster, various consensus functions can be chosen to obtain the final output based on the user's requirement (Vega-Pons and Ruiz-Shulcloper, 2011). The ensemble clustering is also known as consensus clustering Fig. 7. Zhou and Tang (2006) explored ensemble methods for unsupervised learning and developed four different approaches to combine the outputs of these clusters. In recent years, some new ensemble clustering methods have been proposed that illustrated the priority of ensemble learning (Huang et al., 2017b;Zheng et al., 2010;Huang et al., 2016a). Most of the clustering ensemble method is based on the co-association matrix solution, which can be regarded as a graph partition problem. Besides, there is some research focus on integrating the deep structure and ensemble clustering method. Liu et al. (2015Liu et al. ( , 2016) ) firstly showed that ensemble unsupervised representation learning with deep structure can be applied in large scale data. Then the author combined the method with auto-encoder and extends it to the vision field. Shaham et al. (2016) first demonstrated that some crowdsourcing algorithms can be replaced by a Restricted Boltzmann Machine with a single hidden neuron, then propose an RBM-based Deep Neural Net (DNN) used for unsupervised ensemble learning. The unsupervised ensemble method also makes some contribution to the field of Natural Language Processing. Alami et al. (2019) demonstrated that the ensemble of unsupervised deep neural network models that use Sentence2Vec representation as the input has the best performance according to the experiments. Hassan et al. (2019) proposed a module that includes four semantic similarity measures, which improves the performance on the semantic textual similarity (STS) task. The unsupervised ensemble method is also widely used for tasks that lack annotation, such as the medical image. Ahn et al. (2019) proposed an unsupervised feature learning method integrated ensemble approach with a traditional convolutional neural network. Lahiri et al. (2016) employed unsupervised hierarchical feature learning with ensemble sparsely autoencoder on retinal blood vessels segmentation task, meanwhile, Liu et al. (2019) also propose an unsupervised ensemble architecture to automatically segment retinal vessel. Besides, there are also some ensemble deep methods working on localization predicting for long non-coding RNAs Cao et al. (2018). Hu and Suganthan (2022) extended the ensemble random vector functional link to unsupervised tasks. The authors employ manifold regularization to re-represent the original features, and then use the Kuhn-Munkre algorithm with consensus clustering to ensemble the clustering results from multiple hidden layers.</p>
        <p>Active Learning is another popular topic in the deep learning area, which is also often used in conjunction with semi-supervised learning and ensemble learning. The key sight of this is to make the algorithm learning from less annotated data. Some conventional active learning algorithms, such as Query-By-Committee (as shown in Fig. 8), have already adopted the idea of ensemble learning. Melville andMooney (2003, 2004) explored an ensemble method that builds a diverse committee. Beluch et al. (2018) discussed the power of ensembles for active learning is significantly better than Monte-Carlo Dropout and geometric approaches. Sharma and Rani (2018) show some applications in drugtarget interaction prediction. Ensemble active learning is also available to conquer the concept drift and class imbalance problem (Zhang et al., 2018).Active Learning is another popular topic in the deep learning area, which is also often used in conjunction with semi-supervised learning and ensemble learning. The key sight of this is to make the algorithm learning from less annotated data. Some conventional active learning algorithms, such as Query-By-Committee (as shown in Fig. 8), have already adopted the idea of ensemble learning. Melville andMooney (2003, 2004) explored an ensemble method that builds a diverse committee. Beluch et al. (2018) discussed the power of ensembles for active learning is significantly better than Monte-Carlo Dropout and geometric approaches. Sharma and Rani (2018) show some applications in drugtarget interaction prediction. Ensemble active learning is also available to conquer the concept drift and class imbalance problem (Zhang et al., 2018).</p>
        <p>In this section, we briefly present the applications of deep ensemble models across different domains in a tabular form. Ensemble deep models have been implemented in several domains and therefore, in broad sense, we have classified the application domains into five categories, i.e., health care, speech, image classification, forecasting 4 to 8.In this section, we briefly present the applications of deep ensemble models across different domains in a tabular form. Ensemble deep models have been implemented in several domains and therefore, in broad sense, we have classified the application domains into five categories, i.e., health care, speech, image classification, forecasting 4 to 8.</p>
        <p>In this paper, we reviewed the recent developments of ensemble deep learning models. The theoretical background of ensemble learning has been elaborated to understand the success of ensemble learning. The various approaches ranging from traditional ones like bagging, boosting to the recent novel approaches like implicit/explicit ensembles, heterogeneous ensembles, have led to better performance of deep ensemble models. We also reviewed the applications of the deep ensemble models in different domains.In this paper, we reviewed the recent developments of ensemble deep learning models. The theoretical background of ensemble learning has been elaborated to understand the success of ensemble learning. The various approaches ranging from traditional ones like bagging, boosting to the recent novel approaches like implicit/explicit ensembles, heterogeneous ensembles, have led to better performance of deep ensemble models. We also reviewed the applications of the deep ensemble models in different domains.</p>
        <p>Although deep ensemble models have been applied across different domains, there are several open problems which can be explored in the future to fill the gap. Big data (Zhou et al., 2014) is still a challenging problem, one can explore the benefits of deep ensemble models for learning the patterns using the techniques like implicit deep ensemble to maximize the performance in both time and generalization aspects.Although deep ensemble models have been applied across different domains, there are several open problems which can be explored in the future to fill the gap. Big data (Zhou et al., 2014) is still a challenging problem, one can explore the benefits of deep ensemble models for learning the patterns using the techniques like implicit deep ensemble to maximize the performance in both time and generalization aspects.</p>
        <p>Deep learning models are difficult to train than shallow models as large number of weights corresponding to different layers need to be tuned. Creating deep ensemble models may further complicate the problem. Hence, randomized models can be explored to overcome the training cost. Bagging based deep ensemble may incur heavy training time for optimizing the ensemble models. Hence, one can investigate the alternate ways of inducing diversity in the base models with lesser training cost. Randomized learning modules like random vector functional link network (Pao et al., 1994) are best suited for creating the ensemble models as randomized models lead to a significant variance reduction. Also, the hidden layers are randomly initialized, hence, can be used to create deep ensembles without incurring any additional cost of training (Shi et al., 2021). Randomized modules can be further explored using different techniques like implicit/explicit ensembles (Shi et al., 2021), stacking based ensembles (Katuwal and Suganthan, 2019). However, there are still open directions which can be worked upon like negative correlation learning, heterogeneous ensembles and so on.Deep learning models are difficult to train than shallow models as large number of weights corresponding to different layers need to be tuned. Creating deep ensemble models may further complicate the problem. Hence, randomized models can be explored to overcome the training cost. Bagging based deep ensemble may incur heavy training time for optimizing the ensemble models. Hence, one can investigate the alternate ways of inducing diversity in the base models with lesser training cost. Randomized learning modules like random vector functional link network (Pao et al., 1994) are best suited for creating the ensemble models as randomized models lead to a significant variance reduction. Also, the hidden layers are randomly initialized, hence, can be used to create deep ensembles without incurring any additional cost of training (Shi et al., 2021). Randomized modules can be further explored using different techniques like implicit/explicit ensembles (Shi et al., 2021), stacking based ensembles (Katuwal and Suganthan, 2019). However, there are still open directions which can be worked upon like negative correlation learning, heterogeneous ensembles and so on.</p>
        <p>Implicit/explicit ensembles are faster compared to training of multiple deep models. However, creating diversity within a single model is a big challenge. One can explore the methods to induce more diversity among the learners within these ensembles like branching based deep models (Han et al., 2017). Investigate the extension of explicit/implicit ensembles to traditional models.Implicit/explicit ensembles are faster compared to training of multiple deep models. However, creating diversity within a single model is a big challenge. One can explore the methods to induce more diversity among the learners within these ensembles like branching based deep models (Han et al., 2017). Investigate the extension of explicit/implicit ensembles to traditional models.</p>
        <p>Following the stacking based approach, Deep convex net (DCN) (Deng and Yu, 2011), traditional methods like random forest (Breiman, 2001;Zhou and Feng, 2017), support vector machines (Wang et al., 2019d,a;Li et al., 2019b) have been extended to deep learning architectures which resulted in improved performance. One can investigate these traditional models for creating the deep ensemble models.Following the stacking based approach, Deep convex net (DCN) (Deng and Yu, 2011), traditional methods like random forest (Breiman, 2001;Zhou and Feng, 2017), support vector machines (Wang et al., 2019d,a;Li et al., 2019b) have been extended to deep learning architectures which resulted in improved performance. One can investigate these traditional models for creating the deep ensemble models.</p>
        <p>Another big challenge of ensemble deep learning lies in model selection for building the ensemble architecture, homogeneous and heterogeneous ensembles represent two different ways for choosing the model. However, to answer how many different algorithms, and how many base learners in the ensemble architecture, are still problemdependent. Finding a criterion for model selection in ensemble deep learning should be an important target for researchers in the next few years. Since most of the models focus on developing the architectures with little attention towards how to combine the base learners prediction is still unanswered. Hence, one can investigate the effect of different fusion strategies on the prediction of an ensemble output.Another big challenge of ensemble deep learning lies in model selection for building the ensemble architecture, homogeneous and heterogeneous ensembles represent two different ways for choosing the model. However, to answer how many different algorithms, and how many base learners in the ensemble architecture, are still problemdependent. Finding a criterion for model selection in ensemble deep learning should be an important target for researchers in the next few years. Since most of the models focus on developing the architectures with little attention towards how to combine the base learners prediction is still unanswered. Hence, one can investigate the effect of different fusion strategies on the prediction of an ensemble output.</p>
        <p>For unsupervised ensemble learning or consensus clustering, the ensemble approaches include but are not limited to: Hyper-graph partitioning, Voting approach, Mutual information, etc. Consensus clustering is a powerful tool and it can improve performance in most cases. However, there are many concerns remain to be tackled, it is exquisitely sensitive, which might assert as an apparent structure without obvious demarcation or declared cluster stable without cluster resistance. Besides, current method cannot handle some complex but possible scenarios, such as the boundary samples are assigned to the single cluster, clusters do not intersect and the methods are not able to represent outliers. These are the possible research directions for future work.For unsupervised ensemble learning or consensus clustering, the ensemble approaches include but are not limited to: Hyper-graph partitioning, Voting approach, Mutual information, etc. Consensus clustering is a powerful tool and it can improve performance in most cases. However, there are many concerns remain to be tackled, it is exquisitely sensitive, which might assert as an apparent structure without obvious demarcation or declared cluster stable without cluster resistance. Besides, current method cannot handle some complex but possible scenarios, such as the boundary samples are assigned to the single cluster, clusters do not intersect and the methods are not able to represent outliers. These are the possible research directions for future work.</p>
        <p>The problem of semi-supervised ensemble domains has not been extensively studied yet, and most of the literature shows that semisupervised ensemble methods are mainly used in cases where there is insufficient labeling data. Also, combining the semi-supervision with some other machine learning methods, such as active learning, is a direction for future research.The problem of semi-supervised ensemble domains has not been extensively studied yet, and most of the literature shows that semisupervised ensemble methods are mainly used in cases where there is insufficient labeling data. Also, combining the semi-supervision with some other machine learning methods, such as active learning, is a direction for future research.</p>
        <p>Reinforcement learning is another popular topic recently. The idea of integrating model-based reinforcement learning with ensemble learning has been used with promising results in many applications, but there is little integration of planning &amp; learning-based reinforcement learning with ensemble learning methods.Reinforcement learning is another popular topic recently. The idea of integrating model-based reinforcement learning with ensemble learning has been used with promising results in many applications, but there is little integration of planning &amp; learning-based reinforcement learning with ensemble learning methods.</p>
        <p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
        <p>DBN's via objective function which results in a strong classifier. The model learns complex feature representation to build a strong classifier in an iterative manner. Deep boostingDBN's via objective function which results in a strong classifier. The model learns complex feature representation to build a strong classifier in an iterative manner. Deep boosting</p>
        <p>(Cortes et al., 2014(Cortes et al., 2014</p>
        <p>) is an ensemble model that uses the deep decision trees. It can also be used in combination with any other rich family classifier to improve the generalization performance. In each stage of the deep boosting, the decisions of which classifier to add and what weights should be chosen depends on the (data-dependent) complexity of the classifier to which it belongs. The interpretation of the deep boosting classifier is given via structural risk minimization principle at each stage of the learning. Multiclass Deep boosting) is an ensemble model that uses the deep decision trees. It can also be used in combination with any other rich family classifier to improve the generalization performance. In each stage of the deep boosting, the decisions of which classifier to add and what weights should be chosen depends on the (data-dependent) complexity of the classifier to which it belongs. The interpretation of the deep boosting classifier is given via structural risk minimization principle at each stage of the learning. Multiclass Deep boosting</p>
        <p>(Kuznetsov et al., 2014)(Kuznetsov et al., 2014)</p>
        <p>extended the Deep boostingextended the Deep boosting</p>
        <p>(Cortes et al., 2014)(Cortes et al., 2014)</p>
        <p>algorithm to theoretical, algorithmic, and empirical results to the multiclass problems. Due to the limitation of the training data in each mini batch, Boosting CNN may overfit the data.algorithm to theoretical, algorithmic, and empirical results to the multiclass problems. Due to the limitation of the training data in each mini batch, Boosting CNN may overfit the data.</p>
        <p>Introduced Swapout (Hybrid of Dropout and Stochastic depth approach)Introduced Swapout (Hybrid of Dropout and Stochastic depth approach)</p>
        <p>Fig. 9. Ensemble-based approach in different areas. Data from TablesFig. 9. Ensemble-based approach in different areas. Data from Tables</p>
        <p>The funding for this work is provided by the National Supercomputing Mission under DST and Miety, Govt. of India under Grant No. DST/NSM/ R&amp;D_HPC_Appl/2021/03.29, as well as the DDepartment of Science and Technology under Interdisciplinary Cyber Physical Systems (ICPS) Scheme grant no. DST/ICPS/CPS-Individual/2018/276. Mr. Ashwani Kumar Malik acknowledges the financial support (File no -09/1022 (0075)/2019-EMR-I) given as scholarship by Council of Scientific and Industrial Research (CSIR), New Delhi, India. We are grateful to IIT Indore for the facilities and support being provided.The funding for this work is provided by the National Supercomputing Mission under DST and Miety, Govt. of India under Grant No. DST/NSM/ R&amp;D_HPC_Appl/2021/03.29, as well as the DDepartment of Science and Technology under Interdisciplinary Cyber Physical Systems (ICPS) Scheme grant no. DST/ICPS/CPS-Individual/2018/276. Mr. Ashwani Kumar Malik acknowledges the financial support (File no -09/1022 (0075)/2019-EMR-I) given as scholarship by Council of Scientific and Industrial Research (CSIR), New Delhi, India. We are grateful to IIT Indore for the facilities and support being provided.</p>
    </text>
</tei>
