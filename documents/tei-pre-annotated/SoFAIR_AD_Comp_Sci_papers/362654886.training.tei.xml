<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-13T15:15+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Background: Unsupervised machine learners have been increasingly applied to software defect prediction. It is an approach that may be valuable for software practitioners because it reduces the need for labeled training data. Objective: Investigate the use and performance of unsupervised learning techniques in software defect prediction. Method: We conducted a systematic literature review that identified 49 studies containing 2456 individual experimental results, which satisfied our inclusion criteria published between January 2000 and March 2018. In order to compare prediction performance across these studies in a consistent way, we (re-)computed the confusion matrices and employed the Matthews Correlation Coefficient (MCC) as our main performance measure. Results: Our meta-analysis shows that unsupervised models are comparable with supervised models for both withinproject and cross-project prediction. Among the 14 families of unsupervised model, Fuzzy CMeans (FCM) and Fuzzy SOMs (FSOMs) perform best. In addition, where we were able to check, we found that almost 11% (262/2456) of published results (contained in 16 papers) were internally inconsistent and a further 33% (823/2456) provided insufficient details for us to check. Conclusion: Although many factors impact the performance of a classifier, e.g., dataset characteristics, broadly speaking, unsupervised classifiers do not seem to perform worse than the supervised classifiers in our review. However, we note a worrying prevalence of (i) demonstrably erroneous experimental results, (ii) undemanding benchmarks and (iii) incomplete reporting. We therefore encourage researchers to be comprehensive in their reporting.Background: Unsupervised machine learners have been increasingly applied to software defect prediction. It is an approach that may be valuable for software practitioners because it reduces the need for labeled training data. Objective: Investigate the use and performance of unsupervised learning techniques in software defect prediction. Method: We conducted a systematic literature review that identified 49 studies containing 2456 individual experimental results, which satisfied our inclusion criteria published between January 2000 and March 2018. In order to compare prediction performance across these studies in a consistent way, we (re-)computed the confusion matrices and employed the Matthews Correlation Coefficient (MCC) as our main performance measure. Results: Our meta-analysis shows that unsupervised models are comparable with supervised models for both withinproject and cross-project prediction. Among the 14 families of unsupervised model, Fuzzy CMeans (FCM) and Fuzzy SOMs (FSOMs) perform best. In addition, where we were able to check, we found that almost 11% (262/2456) of published results (contained in 16 papers) were internally inconsistent and a further 33% (823/2456) provided insufficient details for us to check. Conclusion: Although many factors impact the performance of a classifier, e.g., dataset characteristics, broadly speaking, unsupervised classifiers do not seem to perform worse than the supervised classifiers in our review. However, we note a worrying prevalence of (i) demonstrably erroneous experimental results, (ii) undemanding benchmarks and (iii) incomplete reporting. We therefore encourage researchers to be comprehensive in their reporting.</p>
        <p>Various software defect prediction models have been proposed to improve the quality of software over the past few decades [1]. An increasingly popular approach is to use machine learning [2,3]. These approaches can be divided into supervised methods where the training data requires labels, typically faulty or not, and unsupervised methods where the data do not need to be labelled. Supervised prediction models predominate. However, in practice it is often difficult to collect defect classification labels to train a Supervised Defect Prediction (SDP) model [4]. As a consequence in recent years, Unsupervised Defect Prediction (UnSDP) models have begun to attract attention.Various software defect prediction models have been proposed to improve the quality of software over the past few decades [1]. An increasingly popular approach is to use machine learning [2,3]. These approaches can be divided into supervised methods where the training data requires labels, typically faulty or not, and unsupervised methods where the data do not need to be labelled. Supervised prediction models predominate. However, in practice it is often difficult to collect defect classification labels to train a Supervised Defect Prediction (SDP) model [4]. As a consequence in recent years, Unsupervised Defect Prediction (UnSDP) models have begun to attract attention.</p>
        <p>The main aim of our systematic review is to provide software practitioners and researchers with guidance for software defect prediction, particularly regarding whether the use of unsupervised prediction models is a viable op-tion. We analyse 49 UnSDP primary studies that satisfy our inclusion criteria. From these primary studies, we investigate which unsupervised learning algorithms were deployed and the relative predictive performance of supervised and unsupervised models.The main aim of our systematic review is to provide software practitioners and researchers with guidance for software defect prediction, particularly regarding whether the use of unsupervised prediction models is a viable op-tion. We analyse 49 UnSDP primary studies that satisfy our inclusion criteria. From these primary studies, we investigate which unsupervised learning algorithms were deployed and the relative predictive performance of supervised and unsupervised models.</p>
        <p>Our systematic review makes the following contributions:Our systematic review makes the following contributions:</p>
        <p>1. Identification of a set of 49 primary studies related to UnSDP published between January 2000 and March 2018. These cover a wide range (14) of unsupervised prediction technique families including six different cluster labelling techniques. 2. Data extraction and regularisation from the 49 studies satisfying our inclusion criteria. We obtain 2456 individual experimental results from these studies. 3. A meta-analysis to compare the relative performance of unsupervised and supervised learners from two perspectives (i) the specific learning algorithm and (ii) the dataset. Based on the analysis results we make suggestions for machine learning-based, software defect prediction for practitioners.1. Identification of a set of 49 primary studies related to UnSDP published between January 2000 and March 2018. These cover a wide range (14) of unsupervised prediction technique families including six different cluster labelling techniques. 2. Data extraction and regularisation from the 49 studies satisfying our inclusion criteria. We obtain 2456 individual experimental results from these studies. 3. A meta-analysis to compare the relative performance of unsupervised and supervised learners from two perspectives (i) the specific learning algorithm and (ii) the dataset. Based on the analysis results we make suggestions for machine learning-based, software defect prediction for practitioners.</p>
        <p>4. A bibliometric analysis that considers research and publishing trends, along with the quality of reported experimental results.4. A bibliometric analysis that considers research and publishing trends, along with the quality of reported experimental results.</p>
        <p>The remainder of this paper is organised as follows: Section 2 describes our systematic review methodology including the research questions, study selection criteria, data extraction and synthesis (process, prediction performance measures and raw data summary). Next, Section 3 presents current research trends and quality issues: both incomplete and inconsistent reporting of results. Section 4 shows our meta-analysis results. Section 5 discusses the threats to validity, followed by conclusions and discussion of actionable results in Section 6.The remainder of this paper is organised as follows: Section 2 describes our systematic review methodology including the research questions, study selection criteria, data extraction and synthesis (process, prediction performance measures and raw data summary). Next, Section 3 presents current research trends and quality issues: both incomplete and inconsistent reporting of results. Section 4 shows our meta-analysis results. Section 5 discusses the threats to validity, followed by conclusions and discussion of actionable results in Section 6.</p>
        <p>Our systematic review follows the guidelines of a systematic review approach for software engineering presented in [5]. First, we clarify our research questions, then search and identify relevant primary studies, next we synthesise results from the selected primary studies, finally we explore and answer our research questions through metaanalysis.Our systematic review follows the guidelines of a systematic review approach for software engineering presented in [5]. First, we clarify our research questions, then search and identify relevant primary studies, next we synthesise results from the selected primary studies, finally we explore and answer our research questions through metaanalysis.</p>
        <p>• RQ1: What are the publication trends in unsupervised software defect prediction research?• RQ1: What are the publication trends in unsupervised software defect prediction research?</p>
        <p>• RQ2: What is the quality of the experimental reporting (completeness and consistency)?• RQ2: What is the quality of the experimental reporting (completeness and consistency)?</p>
        <p>• RQ3: What kinds of unsupervised learning research experiments are conducted?• RQ3: What kinds of unsupervised learning research experiments are conducted?</p>
        <p>• RQ4: What is the difference between unsupervised and supervised software defect predictive performance?• RQ4: What is the difference between unsupervised and supervised software defect predictive performance?</p>
        <p>• RQ5: Which unsupervised prediction models or model families perform better?• RQ5: Which unsupervised prediction models or model families perform better?</p>
        <p>• RQ6: What is the impact of dataset characteristics on predictive performance?• RQ6: What is the impact of dataset characteristics on predictive performance?</p>
        <p>The first three research questions will principally be of interest to software engineering researchers. The remaining three questions will be of interest to both practitioners and researchers.The first three research questions will principally be of interest to software engineering researchers. The remaining three questions will be of interest to both practitioners and researchers.</p>
        <p>We used five search engines to include the papers published between January 2000 and March 2018. The start date was aligned with the search period of the widely cited systematic review by Hall et al. [3]. We undertook our search on 7th March, 2018. The search engines include the ISI Web of Science, ACM Digital Library, IEEE Xplore, ScienceDirect and SpringerLink. Although there are small variants in the five search engines, our key search string is:We used five search engines to include the papers published between January 2000 and March 2018. The start date was aligned with the search period of the widely cited systematic review by Hall et al. [3]. We undertook our search on 7th March, 2018. The search engines include the ISI Web of Science, ACM Digital Library, IEEE Xplore, ScienceDirect and SpringerLink. Although there are small variants in the five search engines, our key search string is:</p>
        <p>("fault prediction" OR "defect prediction" OR "bug prediction" OR "error prediction") AND ("unsupervised" OR "unlabel*" OR "cluster*") AND ("software")("fault prediction" OR "defect prediction" OR "bug prediction" OR "error prediction") AND ("unsupervised" OR "unlabel*" OR "cluster*") AND ("software")</p>
        <p>Table 1 presents the results of our paper search and selection process. This results in 49 papers selected from an initial 1360 papers. We list these 49 primary papers at the end of this paper where Pi denotes the i th primary study. Note that experimental results in [6] and [7] are included in [P46] and [P47] respectively, thus we only took [P46] and [P47] as our primary studies.Table 1 presents the results of our paper search and selection process. This results in 49 papers selected from an initial 1360 papers. We list these 49 primary papers at the end of this paper where Pi denotes the i th primary study. Note that experimental results in [6] and [7] are included in [P46] and [P47] respectively, thus we only took [P46] and [P47] as our primary studies.</p>
        <p>We extracted data related to our research questions from each of the 49 papers, and organised the qualitative and quantitative data into a raw data file (see our Mendeley dataset [8]) . Each paper contains from 1 to 751 ( median = 12) experimental results, yielding a total of 2456 individual results, which involve 128 distinct software project defect datasets (from NASA, ISM, AEEEM, PROMISE, etc.) and 25 prediction model families (14 unsupervised learners and 11 supervised learners).We extracted data related to our research questions from each of the 49 papers, and organised the qualitative and quantitative data into a raw data file (see our Mendeley dataset [8]) . Each paper contains from 1 to 751 ( median = 12) experimental results, yielding a total of 2456 individual results, which involve 128 distinct software project defect datasets (from NASA, ISM, AEEEM, PROMISE, etc.) and 25 prediction model families (14 unsupervised learners and 11 supervised learners).</p>
        <p>From each paper we extracted the following:From each paper we extracted the following:</p>
        <p>Then, from within each paper, we extracted for each experimental result:Then, from within each paper, we extracted for each experimental result:</p>
        <p>• Prediction method name (e.g., DTJ48)• Prediction method name (e.g., DTJ48)</p>
        <p>• Project name trained on (e.g., PC4)• Project name trained on (e.g., PC4)</p>
        <p>• Project name tested on (e.g., PC4) For full details refer to the review protocol at our Mendeley dataset [8]. To ensure data quality, we undertook preprocessing (synthesising across studies) including name unification (project name, method name, response variable name, etc.), confusion matrix (re-)computation2 (see Ta-ble 2) and data quality checking with R 
            <rs type="software">scripts</rs>. We describe the details of confusion matrix re-computation and data quality checks in Section 2.5.
        </p>
        <p>For data classification, the confusion matrix (Table 2) is the fundamental descriptor from which the majority of performance indicators may be derived. Although ideally all primary studies would report consistent performance indicators, in practice, a wide range of indicators are used such as accuracy, precision, recall, the F-measure3 , the Gmeasure and so forth. Consequently, we reconstruct the confusion matrix wherever possible. Unfortunately, there remain about 33% (823/2456) of the experimental results for which this was not possible, due to incomplete reporting as discussed in Section 3.2.2.For data classification, the confusion matrix (Table 2) is the fundamental descriptor from which the majority of performance indicators may be derived. Although ideally all primary studies would report consistent performance indicators, in practice, a wide range of indicators are used such as accuracy, precision, recall, the F-measure3 , the Gmeasure and so forth. Consequently, we reconstruct the confusion matrix wherever possible. Unfortunately, there remain about 33% (823/2456) of the experimental results for which this was not possible, due to incomplete reporting as discussed in Section 3.2.2.</p>
        <p>The performance indicators used by our set of 49 primary studies are summarised below. To further complicate matters, note that different studies may use different names for the same measure [10,11].The performance indicators used by our set of 49 primary studies are summarised below. To further complicate matters, note that different studies may use different names for the same measure [10,11].</p>
        <p>9. AUC: Area Under the Curve (AUC) ROC chart [13]. 10. Popt/ACC: Effort-aware prediction performance [14].9. AUC: Area Under the Curve (AUC) ROC chart [13]. 10. Popt/ACC: Effort-aware prediction performance [14].</p>
        <p>11. G-Mean: Despite being widely used, F1 and AUC are known to be potentially problematic [9,15,16]. F1 focuses on positive classes and ignores negative classes. This is acceptable for information retrieval type problems (since the number of say irrelevant documents, correctly not retrieved can be essentially unbounded). However, this is not the case for defect prediction because the precision of negative classes (non-defective) is also of concern to software developers. Knowing that a component is non-defective is important. Thus the application domain of defect prediction is unsuitable for F1.11. G-Mean: Despite being widely used, F1 and AUC are known to be potentially problematic [9,15,16]. F1 focuses on positive classes and ignores negative classes. This is acceptable for information retrieval type problems (since the number of say irrelevant documents, correctly not retrieved can be essentially unbounded). However, this is not the case for defect prediction because the precision of negative classes (non-defective) is also of concern to software developers. Knowing that a component is non-defective is important. Thus the application domain of defect prediction is unsuitable for F1.</p>
        <p>The AUC metric compares algorithms without classification threshold. So it is suitable to compare classifier families in a theoretical sense but not in a practical sense. The reason being, in practice a deployed classifier must have a particular classification threshold. So unless a classifier strictly dominates another (i.e., for all possible threshold values) we cannot use AUC to prefer one classifier to another.The AUC metric compares algorithms without classification threshold. So it is suitable to compare classifier families in a theoretical sense but not in a practical sense. The reason being, in practice a deployed classifier must have a particular classification threshold. So unless a classifier strictly dominates another (i.e., for all possible threshold values) we cannot use AUC to prefer one classifier to another.</p>
        <p>In contrast, the Matthews Correlation Coefficient (MCC) [12] is based on all four quadrants of the confusion matrix, which gives a better summary of the performance of classification algorithms. It is also known by statisticians as the φ coefficient, originally proposed by Karl Pearson. MCC is easier to interpret as correlation coefficient since it takes a value in the interval [-1, 1], with 1 showing a perfect classifier, -1 showing a perverse classifier, and 0 showing that the prediction is uncorrelated with the ground truth. A more detailed comparison between MCC, F1 and AUC can be found in our another work [17] (see Section 3.1 Classification Performance Measures). Therefore, we prefer to use MCC to assess classification performance in this paper. Note, however, that MCC is undefined if any of the quantities TP + FN, TP + FP, TN + FP, or TN + FN are zero [12].In contrast, the Matthews Correlation Coefficient (MCC) [12] is based on all four quadrants of the confusion matrix, which gives a better summary of the performance of classification algorithms. It is also known by statisticians as the φ coefficient, originally proposed by Karl Pearson. MCC is easier to interpret as correlation coefficient since it takes a value in the interval [-1, 1], with 1 showing a perfect classifier, -1 showing a perverse classifier, and 0 showing that the prediction is uncorrelated with the ground truth. A more detailed comparison between MCC, F1 and AUC can be found in our another work [17] (see Section 3.1 Classification Performance Measures). Therefore, we prefer to use MCC to assess classification performance in this paper. Note, however, that MCC is undefined if any of the quantities TP + FN, TP + FP, TN + FP, or TN + FN are zero [12].</p>
        <p>From the 49 studies we obtain 2456 individual experimental results. Table 3 presents a summary of the categorical attributes. Over half of the results are from supervised learners which are generally deployed as comparators to the unsupervised methods. Within-project (as opposed to cross-project) classification is the dominant approach.From the 49 studies we obtain 2456 individual experimental results. Table 3 presents a summary of the categorical attributes. Over half of the results are from supervised learners which are generally deployed as comparators to the unsupervised methods. Within-project (as opposed to cross-project) classification is the dominant approach.</p>
        <p>The remaining categories relate to primary study quality which we explore in Section 2.5. Likewise, Table 4 summarises the numerical attributes where #NA is the number of unavailable results. From our data extraction and synthesis, 262 inconsistent results were identified (see Section 2.5). Therefore, Table 4 only lists a summary of the valid -in the sense of being internally consistent -2194 experimental results after removing 262 inconsistent ones. Quite striking is the diversity of values, e.g., the fault rate d ranges from 0.4% to over 93%. The AUC ranges from 0.31 to 0.948 (recall that any value below 0.5 suggests a classifier that is predicting worse than by chance). Note that #NA of MCC is larger than 823 unchecked results, which is caused by zero in the denominator of its definition (see Section 2.4.2). F1 is similarly impacted. For the fault rate, #NA is due to unreported values in private (RSDIMU, CSAS, MIS and Embedded used in 20 experiments) or modified public datasets (KC3 314 and JM1 8916 are public NASA data, but are inconsistent with the original datasets and are used in a further 4 experiments). However, the overall unavailability of fault rate data is low, considering the total number of datasets and fortunately has little effect on our meta-analysis. First we explore publication venue. Our search involves a wide range of journal or conference publications. Note that the number of experimental results in these primary studies varies greatly from 1 to 751 thus some papers have more propensity for error. We classify the 49 selected papers into two groups according to whether they are found in the so-called 'predatory' publisher lists [18]. The list aims to identify standalone publishers with questionable approaches to peer review rigour.The remaining categories relate to primary study quality which we explore in Section 2.5. Likewise, Table 4 summarises the numerical attributes where #NA is the number of unavailable results. From our data extraction and synthesis, 262 inconsistent results were identified (see Section 2.5). Therefore, Table 4 only lists a summary of the valid -in the sense of being internally consistent -2194 experimental results after removing 262 inconsistent ones. Quite striking is the diversity of values, e.g., the fault rate d ranges from 0.4% to over 93%. The AUC ranges from 0.31 to 0.948 (recall that any value below 0.5 suggests a classifier that is predicting worse than by chance). Note that #NA of MCC is larger than 823 unchecked results, which is caused by zero in the denominator of its definition (see Section 2.4.2). F1 is similarly impacted. For the fault rate, #NA is due to unreported values in private (RSDIMU, CSAS, MIS and Embedded used in 20 experiments) or modified public datasets (KC3 314 and JM1 8916 are public NASA data, but are inconsistent with the original datasets and are used in a further 4 experiments). However, the overall unavailability of fault rate data is low, considering the total number of datasets and fortunately has little effect on our meta-analysis. First we explore publication venue. Our search involves a wide range of journal or conference publications. Note that the number of experimental results in these primary studies varies greatly from 1 to 751 thus some papers have more propensity for error. We classify the 49 selected papers into two groups according to whether they are found in the so-called 'predatory' publisher lists [18]. The list aims to identify standalone publishers with questionable approaches to peer review rigour.</p>
        <p>We used three lists: for publisher4 , for standalone journals 5 and for conferences6 . Note that these lists have been questioned [19] so we recognise the classification is imperfect.We used three lists: for publisher4 , for standalone journals 5 and for conferences6 . Note that these lists have been questioned [19] so we recognise the classification is imperfect.</p>
        <p>Second, we consider internal experiment quality. For this we check for the use of cross-validation [20], as an indicator for experimental quality. More importantly, we check the internal consistency of results. Hence we recompute the confusion matrix, or construct it if not provided. This is possible for any one of the 10 combinations of performance measure reporting listed below, i.e., we solve for X. From this we can compute MCC.Second, we consider internal experiment quality. For this we check for the use of cross-validation [20], as an indicator for experimental quality. More importantly, we check the internal consistency of results. Hence we recompute the confusion matrix, or construct it if not provided. This is possible for any one of the 10 combinations of performance measure reporting listed below, i.e., we solve for X. From this we can compute MCC.</p>
        <p>For brevity we just present the details for Cases 9 and 10 (N.B. Cases 1 to 8 can be found in [11]). Here, d is the proportion of defective modules, a is accuracy, r is recall and p is precision. Other definitions for FPR, FNR, ER, Accuracy, etc. are given in Section 2.4.2. Note that we use other measures in preference to d, since real d might be slightly divergent from reported d in the original study due to pre-processing or cross validation and differences between folds. Therefore, we only use d when we cannot recompute the confusion matrix for Cases 1-4. Furthermore, we normalize TP, TN, FP and FN with T P + T N + F P + F N = 1 when preprocessing raw data.For brevity we just present the details for Cases 9 and 10 (N.B. Cases 1 to 8 can be found in [11]). Here, d is the proportion of defective modules, a is accuracy, r is recall and p is precision. Other definitions for FPR, FNR, ER, Accuracy, etc. are given in Section 2.4.2. Note that we use other measures in preference to d, since real d might be slightly divergent from reported d in the original study due to pre-processing or cross validation and differences between folds. Therefore, we only use d when we cannot recompute the confusion matrix for Cases 1-4. Furthermore, we normalize TP, TN, FP and FN with T P + T N + F P + F N = 1 when preprocessing raw data.</p>
        <p>As part of our re-computation, we also check the correctness and consistency of the experimental results. If the reported or recomputed result satisfies one of the following six inconsistency rules, we label it as problematic data and remove it from our data analysis. In total, we removed 262 results (Rule 1: 171; Rule 2: 7; Rule 3: 60; Rule 4: 3; Rule 5: 19; Rule 6: 2) out of 2456 experimental results.As part of our re-computation, we also check the correctness and consistency of the experimental results. If the reported or recomputed result satisfies one of the following six inconsistency rules, we label it as problematic data and remove it from our data analysis. In total, we removed 262 results (Rule 1: 171; Rule 2: 7; Rule 3: 60; Rule 4: 3; Rule 5: 19; Rule 6: 2) out of 2456 experimental results.</p>
        <p>Rule 1. The recomputed performance values (including TP, F1, MCC, d, etc.) are out of their correct ranges (for example:Rule 1. The recomputed performance values (including TP, F1, MCC, d, etc.) are out of their correct ranges (for example:</p>
        <p>The recomputed d is 0 (we treat 0 as problematic data since all experiments for defect prediction should include defective modules).The recomputed d is 0 (we treat 0 as problematic data since all experiments for defect prediction should include defective modules).</p>
        <p>Rule 3. Compare the recomputed d with the original reported defect percentage, we treat them as problematic data if the difference is greater than 0.1 (i.e., we allow for some rounding errors).Rule 3. Compare the recomputed d with the original reported defect percentage, we treat them as problematic data if the difference is greater than 0.1 (i.e., we allow for some rounding errors).</p>
        <p>Rule 4. Compare our recomputed results with the original ones where available. If the difference of a measure is greater than the rounding error range, we label them as problematic data. Here, the rounding error intervals are computed by adding ±0.05 to the original data. Note, compared to the rounding error tolerance of 0.01 used by [11], we apply a wider range of 0.05.Rule 4. Compare our recomputed results with the original ones where available. If the difference of a measure is greater than the rounding error range, we label them as problematic data. Here, the rounding error intervals are computed by adding ±0.05 to the original data. Note, compared to the rounding error tolerance of 0.01 used by [11], we apply a wider range of 0.05.</p>
        <p>Rule 5. Special values from Cases 1-10. Here, we use Case1 as example. For a given FPR, FNR, ER in Case1, the confusion matrix is recomputed. We derive the formulae:Rule 5. Special values from Cases 1-10. Here, we use Case1 as example. For a given FPR, FNR, ER in Case1, the confusion matrix is recomputed. We derive the formulae:</p>
        <p>If the numerator is not zero and the denominator is zero, then it is a problematic result. In this case, if ER = F P R and F N R = F P R (e.g., FNR=0.24, FPR=0.24, ER=0.13), it will be labeled as problematic or inconsistent. A complete explanation for all of 10 cases can be found in our Mendeley dataset (see Section 2.4). Rule 6. Obvious problematic experimental data is reported by the primary study. For example, the confusion matrix in [P15] is inconsistent with their dataset. In [P15], their Table X shows that there are 134 bugs (BUGs=true) in their JEdit dataset, however the following clustering prediction result Table XI andTable XII show that there are only 61 bugs in that dataset (Table XIII: TP=46, FN=15). Recall that our search has identified 49 primary studies that have applied unsupervised learning to the task of software defect prediction. These are published over the period of 2000-2018 7 and are summarised in Table 5 and also by Figure 1. From this we can see that conference papers tend to dominate and that there has been pronounced growth in the overall number of papers published in recent years as indicated by the smoother in Figure 1.If the numerator is not zero and the denominator is zero, then it is a problematic result. In this case, if ER = F P R and F N R = F P R (e.g., FNR=0.24, FPR=0.24, ER=0.13), it will be labeled as problematic or inconsistent. A complete explanation for all of 10 cases can be found in our Mendeley dataset (see Section 2.4). Rule 6. Obvious problematic experimental data is reported by the primary study. For example, the confusion matrix in [P15] is inconsistent with their dataset. In [P15], their Table X shows that there are 134 bugs (BUGs=true) in their JEdit dataset, however the following clustering prediction result Table XI andTable XII show that there are only 61 bugs in that dataset (Table XIII: TP=46, FN=15). Recall that our search has identified 49 primary studies that have applied unsupervised learning to the task of software defect prediction. These are published over the period of 2000-2018 7 and are summarised in Table 5 and also by Figure 1. From this we can see that conference papers tend to dominate and that there has been pronounced growth in the overall number of papers published in recent years as indicated by the smoother in Figure 1.</p>
        <p>What is the quality of the experimental reporting (completeness and consistency)?What is the quality of the experimental reporting (completeness and consistency)?</p>
        <p>The 49 primary studies contain a total of 2456 individual experimental results. This ranges from 1 to 751 results with the median number being 12 results contained in a single paper. Problematic results are analyzed in Section 3.2, so here we just provide an overall summary of the remaining 2194 non-erroneous results in Table 6.The 49 primary studies contain a total of 2456 individual experimental results. This ranges from 1 to 751 results with the median number being 12 results contained in a single paper. Problematic results are analyzed in Section 3.2, so here we just provide an overall summary of the remaining 2194 non-erroneous results in Table 6.</p>
        <p>Next we consider the prevalence of papers published in socalled 'predatory' journals and conferences. Table 7 reveals that 18 out of the 49 papers (i.e., approximately 37%) were from venues considered to be 'predatory' publishers. This was a higher proportion than we had expected. This phenomenon appears to have started in the late 2000s and is possibly declining since 2014 (see also Figure 2). This could be a reaction to the adverse publicity such publishers are gaining.Next we consider the prevalence of papers published in socalled 'predatory' journals and conferences. Table 7 reveals that 18 out of the 49 papers (i.e., approximately 37%) were from venues considered to be 'predatory' publishers. This was a higher proportion than we had expected. This phenomenon appears to have started in the late 2000s and is possibly declining since 2014 (see also Figure 2). This could be a reaction to the adverse publicity such publishers are gaining.</p>
        <p>Next we consider potential issues which may arise from incomplete reporting of experimental design and results. First, we examine whether a study explicitly reports if a cross-validation procedure was used or not. We were surprised to find that 25 out of 49 studies did not report this information. This is important because although unsupervised learners need not initially be trained with data that have class labels (i.e., defective or not), the learner must still be configured to classify and hence be evaluated on unseen data. Of course some, or even all, of the other studies may have applied cross-validation techniques, however this ought to be confirmed by the authors in the first instance.Next we consider potential issues which may arise from incomplete reporting of experimental design and results. First, we examine whether a study explicitly reports if a cross-validation procedure was used or not. We were surprised to find that 25 out of 49 studies did not report this information. This is important because although unsupervised learners need not initially be trained with data that have class labels (i.e., defective or not), the learner must still be configured to classify and hence be evaluated on unseen data. Of course some, or even all, of the other studies may have applied cross-validation techniques, however this ought to be confirmed by the authors in the first instance.</p>
        <p>In terms of results, as described in Section 2.5 we endeavoured to apply basic consistency checks to the raw results. Unfortunately this was not possible for over 30% of the results (823/2456) listed in Table 6. Given, as we discuss in the following section, that we uncovered 262 (262/1633 ≈ 16%) instances of problematic data in the results we are able to check, this is a little disturbing.In terms of results, as described in Section 2.5 we endeavoured to apply basic consistency checks to the raw results. Unfortunately this was not possible for over 30% of the results (823/2456) listed in Table 6. Given, as we discuss in the following section, that we uncovered 262 (262/1633 ≈ 16%) instances of problematic data in the results we are able to check, this is a little disturbing.</p>
        <p>As previously indicated, we were able to apply consistency checking to approximately 66% (1633 out of 2456) of all the results in our systematic review. Figure 3 shows the distribution of errors (or inconsistencies) per paper. There are 14 papers that are excluded due to incomplete reporting. Note also that in order to improve the visual qualities of the histogram, the largest bin is 10+ and it contains a single extreme outlier of 165 inconsistencies and two studies including 19 and 30 inconsistencies respectively. Next, we consider whether there is any relationship between publication venue and study errors. For example, one might expect that 'non-predatory' publishers deploy more rigorous peer review, or that 'predatory' publishers are less diligent in this regard. To validate such an expectation, we examine the odds ratio [21] which is defined as:As previously indicated, we were able to apply consistency checking to approximately 66% (1633 out of 2456) of all the results in our systematic review. Figure 3 shows the distribution of errors (or inconsistencies) per paper. There are 14 papers that are excluded due to incomplete reporting. Note also that in order to improve the visual qualities of the histogram, the largest bin is 10+ and it contains a single extreme outlier of 165 inconsistencies and two studies including 19 and 30 inconsistencies respectively. Next, we consider whether there is any relationship between publication venue and study errors. For example, one might expect that 'non-predatory' publishers deploy more rigorous peer review, or that 'predatory' publishers are less diligent in this regard. To validate such an expectation, we examine the odds ratio [21] which is defined as:</p>
        <p>where Odds pc is the odds that a member of Population pc will fall into Category T , similarly Odds notpc is the odds that a member not from Population pc will fall into Category T .where Odds pc is the odds that a member of Population pc will fall into Category T , similarly Odds notpc is the odds that a member not from Population pc will fall into Category T .</p>
        <p>When calculating the odds ratio (from Table 8) the likelihood of containing an error is 1.21(with 95% confidence limits [0.31, 4.73]) between 'non-predatory' and 'predatory' publishers. This is close to unity and so suggests little effect at the study level. However, a similar analysis for individual experimental results (from Table 9) reveals a different picture of 0.02 (with 95% confidence limits [0.02, 0.04]). This suggests individual results from a 'nonpredatory' paper clearly are less likely to contain errors than a 'predatory' paper, however, this is skewed by a small number of papers that contain very high numbers of results (and inconsistencies).When calculating the odds ratio (from Table 8) the likelihood of containing an error is 1.21(with 95% confidence limits [0.31, 4.73]) between 'non-predatory' and 'predatory' publishers. This is close to unity and so suggests little effect at the study level. However, a similar analysis for individual experimental results (from Table 9) reveals a different picture of 0.02 (with 95% confidence limits [0.02, 0.04]). This suggests individual results from a 'nonpredatory' paper clearly are less likely to contain errors than a 'predatory' paper, however, this is skewed by a small number of papers that contain very high numbers of results (and inconsistencies).</p>
        <p>When assessing prediction methods such as UnSDP it is usual to provide some kind of comparator or benchmark. This is particularly helpful when a new or improved learning method is proposed. However, there is a risk that most of the research 'energy' is invested in the new method, as opposed to the benchmark. As Michie et al. [22] remarked 25 years ago, we need to be vigilant about the problems of comparing 'pet' algorithms in which researchers are expert with others with which they are less familiar. They also note the more general danger of selecting comparator benchmarks that are not state of the art. How does this apply to our systematic review? It potentially raises a bias most obviously with RQ4 that compares UnSDP and SDP predictive performance. Unfortunately we cannot know researchers' intentions and making judgements as to when a method is a 'pet' requires a great deal of subjectivity. However, we can at least examine how papers use the literature with regard to SDPs. Table 10 shows that of the 28 papers that use SDPs as a comparator for their UnSDPs, 71% (20/28) cite related articles, although only four use previous prediction results. In contrast 8/28 papers use supervised methods without citation suggesting that common methods such as logistic regression are seen as basic default methods that do not warrant discussion or tuning (see Section 4.1.2 for an analysis of hyper-parameter tuning).When assessing prediction methods such as UnSDP it is usual to provide some kind of comparator or benchmark. This is particularly helpful when a new or improved learning method is proposed. However, there is a risk that most of the research 'energy' is invested in the new method, as opposed to the benchmark. As Michie et al. [22] remarked 25 years ago, we need to be vigilant about the problems of comparing 'pet' algorithms in which researchers are expert with others with which they are less familiar. They also note the more general danger of selecting comparator benchmarks that are not state of the art. How does this apply to our systematic review? It potentially raises a bias most obviously with RQ4 that compares UnSDP and SDP predictive performance. Unfortunately we cannot know researchers' intentions and making judgements as to when a method is a 'pet' requires a great deal of subjectivity. However, we can at least examine how papers use the literature with regard to SDPs. Table 10 shows that of the 28 papers that use SDPs as a comparator for their UnSDPs, 71% (20/28) cite related articles, although only four use previous prediction results. In contrast 8/28 papers use supervised methods without citation suggesting that common methods such as logistic regression are seen as basic default methods that do not warrant discussion or tuning (see Section 4.1.2 for an analysis of hyper-parameter tuning).</p>
        <p>experiments are conducted? For a more detailed analysis of unsupervised learning techniques, we categorise them into clustering and non-clustering techniques as presented in Table 11. We further divide these unsupervised prediction techniques into 14 families and 21 sub-families.experiments are conducted? For a more detailed analysis of unsupervised learning techniques, we categorise them into clustering and non-clustering techniques as presented in Table 11. We further divide these unsupervised prediction techniques into 14 families and 21 sub-families.</p>
        <p>Clustering-based prediction approaches dominate. They typically include two phases: clustering and labelling. Firstly, all instances in a dataset are clustered into different groups, then each group is labeled as defective or not. In addition, there are also a few non-clustering unsupervised prediction techniques, such as using thresholds that label instances directly.Clustering-based prediction approaches dominate. They typically include two phases: clustering and labelling. Firstly, all instances in a dataset are clustered into different groups, then each group is labeled as defective or not. In addition, there are also a few non-clustering unsupervised prediction techniques, such as using thresholds that label instances directly.</p>
        <p>Labelling is a necessary step in unsupervised clustering defect prediction. Table 12 summarises all labelling approaches located by our review. This shows considerable diversity. Generic thresholds, as opposed to Pareto or distribution methods, are the most popular. However, practitioners have to consider what is the most suitable labelling technique when employing some particular learners, such as Clustering and Labelling (CLA, see Table 11).Labelling is a necessary step in unsupervised clustering defect prediction. Table 12 summarises all labelling approaches located by our review. This shows considerable diversity. Generic thresholds, as opposed to Pareto or distribution methods, are the most popular. However, practitioners have to consider what is the most suitable labelling technique when employing some particular learners, such as Clustering and Labelling (CLA, see Table 11).</p>
        <p>This section addresses research questions RQ4-RQ6. In RQ4, we conduct a meta-analysis by vote-counting. This is not an ideal approach to meta-analysis, but necessary in order to avoid losing important primary studies which employ a range of response measures (performance measures) and experimental designs. Meta-analysis by votecounting is a simple, though less powerful, procedure than directly using effect size, to compare the performance of two groups. It is only recommended when there are problems with a direct approach [23].This section addresses research questions RQ4-RQ6. In RQ4, we conduct a meta-analysis by vote-counting. This is not an ideal approach to meta-analysis, but necessary in order to avoid losing important primary studies which employ a range of response measures (performance measures) and experimental designs. Meta-analysis by votecounting is a simple, though less powerful, procedure than directly using effect size, to compare the performance of two groups. It is only recommended when there are problems with a direct approach [23].</p>
        <p>Our meta-analysis is hindered in that some studies only report AUC. So even though the experiments are informative, they would not contribute to the meta-analysis if we choose F1 to make comparisons. Essentially votecounting does not directly use effect size estimates from each primary study. Although at its simplest it proceeds by comparing the number of 'positive' studies with 'negative' studies8 , more sophisticated vote-counting methods are available (see Bushman and Wang [23] for an overview). We use the Hedges and Olkins method for unequal sample sizes [25, pp. 47-74] in order to compare UnSDP and SDP.Our meta-analysis is hindered in that some studies only report AUC. So even though the experiments are informative, they would not contribute to the meta-analysis if we choose F1 to make comparisons. Essentially votecounting does not directly use effect size estimates from each primary study. Although at its simplest it proceeds by comparing the number of 'positive' studies with 'negative' studies8 , more sophisticated vote-counting methods are available (see Bushman and Wang [23] for an overview). We use the Hedges and Olkins method for unequal sample sizes [25, pp. 47-74] in order to compare UnSDP and SDP.</p>
        <p>We also carry out a finer grained meta-analysis. Table 13 describes the number of studies, experimental results and projects (or datasets) in our meta-analysis. Note that in the RQ4 vote-counting comparison, 26 studies including 2052 results are involved when we take study as a voting unit, and 110 projects including 2128 results are employed when project is taken as the voting unit.We also carry out a finer grained meta-analysis. Table 13 describes the number of studies, experimental results and projects (or datasets) in our meta-analysis. Note that in the RQ4 vote-counting comparison, 26 studies including 2052 results are involved when we take study as a voting unit, and 110 projects including 2128 results are employed when project is taken as the voting unit.</p>
        <p>and supervised software defect predictive performance?and supervised software defect predictive performance?</p>
        <p>In this section, we compare UnSDP and SDP models at a coarse (Section 4.1.1) and finer (Section 4.1.2) level respectively. All 1306 supervised experimental results are taken from our 26 primary studies which include both unsupervised and supervised models.In this section, we compare UnSDP and SDP models at a coarse (Section 4.1.1) and finer (Section 4.1.2) level respectively. All 1306 supervised experimental results are taken from our 26 primary studies which include both unsupervised and supervised models.</p>
        <p>We carry out vote-counting comparison in this section. Note the possibility that some conclusions might be unstable depending on whether making the voting unit a project or study (paper). For example, this has been a factor for studies [P36], [P37]. So we carry out the vote-counting analysis from both perspectives: (i) by study (each study can vote once) and (ii) dataset (a vote per project). Based on the method of Hedges and Olkin [25], we carry out the vote-counting as follows.We carry out vote-counting comparison in this section. Note the possibility that some conclusions might be unstable depending on whether making the voting unit a project or study (paper). For example, this has been a factor for studies [P36], [P37]. So we carry out the vote-counting analysis from both perspectives: (i) by study (each study can vote once) and (ii) dataset (a vote per project). Based on the method of Hedges and Olkin [25], we carry out the vote-counting as follows.</p>
        <p>1. Identify voting units which must include both unsupervised and supervised prediction models to enable comparability. Voting unit type can be study or project. Depending on the choice of voting unit, all eligible studies or projects are identified. Each eligible study or project will vote once to determine whether UnSDP is better than SDP.1. Identify voting units which must include both unsupervised and supervised prediction models to enable comparability. Voting unit type can be study or project. Depending on the choice of voting unit, all eligible studies or projects are identified. Each eligible study or project will vote once to determine whether UnSDP is better than SDP.</p>
        <p>for each paper or project, and calculate mean, standard deviation of response variable for each group (UnSDP and SDP). The priority of response variable is: MCC, F1, AUC, Popt, Accuracy, ER, Recall, Precision, GMean, ACC, GMeasure, Balance, Purity, MAE and MeanAIC. For example, if MCC and F1 are available in primary study P 1, MCC will be used to vote for P 1. This priority is determined by the number of all available results for each response variable among 2194 consistent experiment results. 3. Determine X for each voting unit, where X is the sign of ES (Effect Size based on the mean difference between two groups). If Ȳ U i -Ȳ S i &gt; 0, X = 1 otherwise, X = 0, where Ȳ U i refers to the mean of response variable for unsupervised models, and Ȳ S i is the mean for supervised models. [P23], [P24] FSOMs Fuzzy Self-Organizing Maps: Combines SOMs with the concept of fuzziness in fuzzy clustering.for each paper or project, and calculate mean, standard deviation of response variable for each group (UnSDP and SDP). The priority of response variable is: MCC, F1, AUC, Popt, Accuracy, ER, Recall, Precision, GMean, ACC, GMeasure, Balance, Purity, MAE and MeanAIC. For example, if MCC and F1 are available in primary study P 1, MCC will be used to vote for P 1. This priority is determined by the number of all available results for each response variable among 2194 consistent experiment results. 3. Determine X for each voting unit, where X is the sign of ES (Effect Size based on the mean difference between two groups). If Ȳ U i -Ȳ S i &gt; 0, X = 1 otherwise, X = 0, where Ȳ U i refers to the mean of response variable for unsupervised models, and Ȳ S i is the mean for supervised models. [P23], [P24] FSOMs Fuzzy Self-Organizing Maps: Combines SOMs with the concept of fuzziness in fuzzy clustering.</p>
        <p>Spectral Clustering (SC): Make use of the eigenvalues of the similarity matrix to reduce dimensionality before clustering.Spectral Clustering (SC): Make use of the eigenvalues of the similarity matrix to reduce dimensionality before clustering.</p>
        <p>Partitions a dataset based on the connectivity between its nodes in a graph with spectral clustering.Partitions a dataset based on the connectivity between its nodes in a graph with spectral clustering.</p>
        <p>[P25], [P44] Expectation maximization clustering (EM): Get probabilities of cluster memberships based on probability distributions.[P25], [P44] Expectation maximization clustering (EM): Get probabilities of cluster memberships based on probability distributions.</p>
        <p>EM EM clustering: Finds the maximum likelihood by iteratively alternating between expectation step and maximization step.EM EM clustering: Finds the maximum likelihood by iteratively alternating between expectation step and maximization step.</p>
        <p>[P9], [P13], [P20], [P21], [P44] Optimization (Opt): Taking clustering as an optimization problem, such as apply Particle Swarm Optimization to solve.[P9], [P13], [P20], [P21], [P44] Optimization (Opt): Taking clustering as an optimization problem, such as apply Particle Swarm Optimization to solve.</p>
        <p>Particle Swarm Optimization clustering: Find the optimal clusters by iteratively improving a candidate solution with regard to a given measure.Particle Swarm Optimization clustering: Find the optimal clusters by iteratively improving a candidate solution with regard to a given measure.</p>
        <p>Affinity Propagation (AP): Finds exemplars, members of the input set that are representative of clusters.Affinity Propagation (AP): Finds exemplars, members of the input set that are representative of clusters.</p>
        <p>Affinity Propagation clustering: Obtains the data similarities, and iteratively exchanges the realvalued messages between data points to clustering.Affinity Propagation clustering: Obtains the data similarities, and iteratively exchanges the realvalued messages between data points to clustering.</p>
        <p>[P12], [P27] Self Learning (SL) : Clustering by comparing each metric value with the corresponding median or average value.[P12], [P27] Self Learning (SL) : Clustering by comparing each metric value with the corresponding median or average value.</p>
        <p>Clustering and labelling: Compute instances violation degree based on the relation between metric values and corresponding median values, then clustering by the violation or consistency information.Clustering and labelling: Compute instances violation degree based on the relation between metric values and corresponding median values, then clustering by the violation or consistency information.</p>
        <p>[P26], [P34] CLAMI Based on CLA, add two extra steps: Metric selection and instance selection based on violation degree.[P26], [P34] CLAMI Based on CLA, add two extra steps: Metric selection and instance selection based on violation degree.</p>
        <p>[P26], [P34], [P46] ACL Average Clustering and labelling: Clustering by metrics of instances violation score (MIVS), which is computed based on the relation between each metric value and their average metric value.[P26], [P34], [P46] ACL Average Clustering and labelling: Clustering by metrics of instances violation score (MIVS), which is computed based on the relation between each metric value and their average metric value.</p>
        <p>Clustering Ensemble (CLEM): Ensemble multiple clustering solutions.Clustering Ensemble (CLEM): Ensemble multiple clustering solutions.</p>
        <p>Clustering Ensemble: Uses multiple algorithms to clustering, then combines the different clustering solutions and produces a single cluster.Clustering Ensemble: Uses multiple algorithms to clustering, then combines the different clustering solutions and produces a single cluster.</p>
        <p>No Clustering Threshold (THD) 1 : Threshold for selected metric. It might be determined by experiences, rules, calculations, or machine learning, etc. THD Threshold: Use some metric thresholds to classify instances directly. e.g., given a metric threshold vector, if any metric of an instance exceeds the corresponding threshold, it will be defect-prone.No Clustering Threshold (THD) 1 : Threshold for selected metric. It might be determined by experiences, rules, calculations, or machine learning, etc. THD Threshold: Use some metric thresholds to classify instances directly. e.g., given a metric threshold vector, if any metric of an instance exceeds the corresponding threshold, it will be defect-prone.</p>
        <p>[P47] Expert (EXP): Determine by experience.[P47] Expert (EXP): Determine by experience.</p>
        <p>Expert determine the labels of each instance directly by their experience.Expert determine the labels of each instance directly by their experience.</p>
        <p>Ranking (MR): Metric Ranking MR Metric Ranking: For some change metrics in justin-time prediction, using 1/Metric to rank instances in ascending order.Ranking (MR): Metric Ranking MR Metric Ranking: For some change metrics in justin-time prediction, using 1/Metric to rank instances in ascending order.</p>
        <p>[P36], [P37], [P39-42],[P36], [P37], [P39-42],</p>
        <p>[P49] 1 For 'Threshold', if the threshold is calculated with defective labels in some method, we will mark it as supervised method, e.g. STHD-ROC in [P47] . According to the number of nodes in a cluster, scattered (smaller) is defect-prone; concentrative (larger) is defect-free.[P49] 1 For 'Threshold', if the threshold is calculated with defective labels in some method, we will mark it as supervised method, e.g. STHD-ROC in [P47] . According to the number of nodes in a cluster, scattered (smaller) is defect-prone; concentrative (larger) is defect-free.</p>
        <p>[P12], [P20], [P21], [P23], [P27] Majority vote Majority voting by the most three similar clustering centres.[P12], [P20], [P21], [P23], [P27] Majority vote Majority voting by the most three similar clustering centres.</p>
        <p>[P18] Top half[P18] Top half</p>
        <p>Half of the top clusters are defect-prone, e.g., rank clusters according to violation degree in descending order.Half of the top clusters are defect-prone, e.g., rank clusters according to violation degree in descending order.</p>
        <p>[P25], [P34], [P40], [P46] Supervised learning Use supervised models to predict cluster labels.[P25], [P34], [P40], [P46] Supervised learning Use supervised models to predict cluster labels.</p>
        <p>[P47] 1 'Expert' and 'Threshold' are used for cluster label decision in Table 12, and for instance label decision in Table 11. 1 Expt = count of experimental results; Prj = count of software projects. 2 One study could contain both inconsistent and consistent results.[P47] 1 'Expert' and 'Threshold' are used for cluster label decision in Table 12, and for instance label decision in Table 11. 1 Expt = count of experimental results; Prj = count of software projects. 2 One study could contain both inconsistent and consistent results.</p>
        <p>When we use "be counted if some result is consistent or incomplete", then Expt will be increased by 1, while when using "not be counted as long as a result is inconsistent", Expt will not be increased. Similarly with Prj. 3 There are 16 studies that include inconsistent experiments according to Figure 3, thus only 33 studies could be used. 4 In total, there are 28 studies that include both UnSDP and SDP model, but all of the SDP experiment results in these two studies are inconsistent, so we removed these two papers.When we use "be counted if some result is consistent or incomplete", then Expt will be increased by 1, while when using "not be counted as long as a result is inconsistent", Expt will not be increased. Similarly with Prj. 3 There are 16 studies that include inconsistent experiments according to Figure 3, thus only 33 studies could be used. 4 In total, there are 28 studies that include both UnSDP and SDP model, but all of the SDP experiment results in these two studies are inconsistent, so we removed these two papers.</p>
        <p>4. Construct the log likelihood function, and obtain the maximum likelihood estimator θ (θ refers to ES).4. Construct the log likelihood function, and obtain the maximum likelihood estimator θ (θ refers to ES).</p>
        <p>where k is the number of eligible vote entities, Φ is the cumulative distribution function, ni is average sample size, ni = (n1 × n2)/(n1 + n2), n1 and n2 are the sample sizes of unsupervised and supervised experiments respectively. 5. Compute the 95% confidence interval for θ:where k is the number of eligible vote entities, Φ is the cumulative distribution function, ni is average sample size, ni = (n1 × n2)/(n1 + n2), n1 and n2 are the sample sizes of unsupervised and supervised experiments respectively. 5. Compute the 95% confidence interval for θ:</p>
        <p>..</p>
        <p>where, V ar( θ) is the variance of θ. C α/2 is the twotailed critical value of the standard normal distribution. In this paper, we use C α/2 = 1.96 (central area=0.95, Z α =1.96). The variance computation is:where, V ar( θ) is the variance of θ. C α/2 is the twotailed critical value of the standard normal distribution. In this paper, we use C α/2 = 1.96 (central area=0.95, Z α =1.96). The variance computation is:</p>
        <p>where,where,</p>
        <p>). δ is the population standardized mean difference, and it is equal to the estimated θ in Step 4. In terms of voting unit, from our meta-analysis there are 26 studies (papers) or 110 projects that conduct both unsupervised and supervised learning for defect prediction. If we carry out vote-counting for all combinations of Prediction Type, Predatory and Cross validation, unfortunately there are only sufficient results for 8 conditions, these are listed in bold in Table 14. This is because the number of eligible study or project results is only zero or one for the other 8 combinations, e.g., if we use project as the voting unit there are no results for cross-project prediction from 'predatory' publishers that explicitly use cross-validation. Another point worth mentioning is that the sum of studies or projects in Table 14 or Figure 4 is not necessarily 26 or 110. For example: one study may include both within-project and cross-project prediction, it would be counted more than once.). δ is the population standardized mean difference, and it is equal to the estimated θ in Step 4. In terms of voting unit, from our meta-analysis there are 26 studies (papers) or 110 projects that conduct both unsupervised and supervised learning for defect prediction. If we carry out vote-counting for all combinations of Prediction Type, Predatory and Cross validation, unfortunately there are only sufficient results for 8 conditions, these are listed in bold in Table 14. This is because the number of eligible study or project results is only zero or one for the other 8 combinations, e.g., if we use project as the voting unit there are no results for cross-project prediction from 'predatory' publishers that explicitly use cross-validation. Another point worth mentioning is that the sum of studies or projects in Table 14 or Figure 4 is not necessarily 26 or 110. For example: one study may include both within-project and cross-project prediction, it would be counted more than once.</p>
        <p>Figures 4(a) and 4(b) show the results of voting by study (paper) and by (software) project respectively. Here, positive values of ES mean UnSDP models perform better than SDP models and negative values the reverse. The vertical dashed line indicates no difference (ES = 0). The length of each horizontal line shows the 95% Confidence Intervals (CI) and the rectangle the estimate of the effect from the relevant set of results. Note that some of the CI lines (e.g., the second case in Figure 4(a)) end with an arrow, meaning the lower or upper bound of CI exceeds the bound of the legend [-0.6, 0.6]. Figure 4(a) shows the results of voting by 26 studies including 2052 experiments. For within-project prediction, although the sign of effect sizes is negative, both of their 95% CI contain zero, which indicates that it is possible that neither performs better than the other. In other words, UnSDP appears comparable with SDP. The same explanation also can be applied in Cross-project prediction as the 95% CI goes across zero.Figures 4(a) and 4(b) show the results of voting by study (paper) and by (software) project respectively. Here, positive values of ES mean UnSDP models perform better than SDP models and negative values the reverse. The vertical dashed line indicates no difference (ES = 0). The length of each horizontal line shows the 95% Confidence Intervals (CI) and the rectangle the estimate of the effect from the relevant set of results. Note that some of the CI lines (e.g., the second case in Figure 4(a)) end with an arrow, meaning the lower or upper bound of CI exceeds the bound of the legend [-0.6, 0.6]. Figure 4(a) shows the results of voting by 26 studies including 2052 experiments. For within-project prediction, although the sign of effect sizes is negative, both of their 95% CI contain zero, which indicates that it is possible that neither performs better than the other. In other words, UnSDP appears comparable with SDP. The same explanation also can be applied in Cross-project prediction as the 95% CI goes across zero.</p>
        <p>Figure 4(b) shows the results of voting by 110 projects including 2128 experiments. These results are broadly similar to voting by paper in that the 95% CIs straddle zero for all five cases. So this suggests that for both within-project and cross-project prediction UnSDP has potential, since it has data collection advantages over SDP, specifically the need for labelled training data is reduced or removed.Figure 4(b) shows the results of voting by 110 projects including 2128 experiments. These results are broadly similar to voting by paper in that the 95% CIs straddle zero for all five cases. So this suggests that for both within-project and cross-project prediction UnSDP has potential, since it has data collection advantages over SDP, specifically the need for labelled training data is reduced or removed.</p>
        <p>Note that the CIs are widest when there are only a few data points. It would also seem that the results are most positive to UnSDP when it is unclear that a crossvalidation has been employed. This suggests a certain degree of caution is warranted before claims of superiority are made.Note that the CIs are widest when there are only a few data points. It would also seem that the results are most positive to UnSDP when it is unclear that a crossvalidation has been employed. This suggests a certain degree of caution is warranted before claims of superiority are made.</p>
        <p>Overall, the results of vote-counting show that UnSDP models are comparable with SDP models, which could make practitioners and researchers more confident to use UnSDP models. However differences in performance are reduced to single votes. So next we focus on the more informative performance indicator MCC, despite the fact that this reduces the amount of data available.Overall, the results of vote-counting show that UnSDP models are comparable with SDP models, which could make practitioners and researchers more confident to use UnSDP models. However differences in performance are reduced to single votes. So next we focus on the more informative performance indicator MCC, despite the fact that this reduces the amount of data available.</p>
        <p>The previous vote-counting comparison does not consider the impact of different prediction performance measures, dataset, etc. For subsequent analysis we restrict our data to all available, non-predatory results for which MCC is available. This yields 1178 observations to compare UnSDP and SDP. N.B. the MCC results are all recomputed from the reported raw results from the primary studies.The previous vote-counting comparison does not consider the impact of different prediction performance measures, dataset, etc. For subsequent analysis we restrict our data to all available, non-predatory results for which MCC is available. This yields 1178 observations to compare UnSDP and SDP. N.B. the MCC results are all recomputed from the reported raw results from the primary studies.</p>
        <p>The side by side boxplots in Figure 5 and Figure 6 illustrate within-project and cross-project defect prediction performance respectively. For more details on the prediction model family refer to Table 11 (for unsupervised learners) and Table A.17 (for supervised learners). Although sample size varies considerably among these learners (from 1 to 127), they still illustrate some overall patterns. The boxplot notches indicate the 95% confidence intervals of the median. For smaller samples these are wider. Although not a formal test, there is evidence that medians differ if the notches do not overlap [26].The side by side boxplots in Figure 5 and Figure 6 illustrate within-project and cross-project defect prediction performance respectively. For more details on the prediction model family refer to Table 11 (for unsupervised learners) and Table A.17 (for supervised learners). Although sample size varies considerably among these learners (from 1 to 127), they still illustrate some overall patterns. The boxplot notches indicate the 95% confidence intervals of the median. For smaller samples these are wider. Although not a formal test, there is evidence that medians differ if the notches do not overlap [26].</p>
        <p>From Figure 5, we observe that unsupervised models for within-project prediction show greater variance in prediction performance than the supervised ones. Although it is hard to obtain a definitive overall conclusion which is better, these results are consistent with our vote-counting results, namely the evidence does not differentiate their relative performances to any substantial degree.From Figure 5, we observe that unsupervised models for within-project prediction show greater variance in prediction performance than the supervised ones. Although it is hard to obtain a definitive overall conclusion which is better, these results are consistent with our vote-counting results, namely the evidence does not differentiate their relative performances to any substantial degree.</p>
        <p>In more detail, and restricting our comments to sample sizes &gt; 10, and therefore more reliable (from Figure 5) we note the following.In more detail, and restricting our comments to sample sizes &gt; 10, and therefore more reliable (from Figure 5) we note the following.</p>
        <p>1. Un Fuzzy (the blue line in Figure 5) outperforms other SDP and UnSDP model families in that the median of Un Fuzzy is higher than that of the other families of learners. However, the CI is wide so it overlaps with other models at least in some contexts. Among the supervised approaches Bayes appears to be the best model which has also been noted by previous studies [3].1. Un Fuzzy (the blue line in Figure 5) outperforms other SDP and UnSDP model families in that the median of Un Fuzzy is higher than that of the other families of learners. However, the CI is wide so it overlaps with other models at least in some contexts. Among the supervised approaches Bayes appears to be the best model which has also been noted by previous studies [3].</p>
        <p>tering including the widely used KMeans and its variants) family did not perform very well. The median of KPart (the red line in Figure 5, 0.33) is lower than the median of all SDP models(0.41). This suggests that researchers should be cautious in using KMeans as a benchmark with which to compare their new UnSDP. 3. Un MR (metric or 1/metric ranking) performs least well. However, this is based on only a small subset of the data, since only 39 out of the 234 Un MR experimental results use MCC with the majority only reporting Popt or ACC.tering including the widely used KMeans and its variants) family did not perform very well. The median of KPart (the red line in Figure 5, 0.33) is lower than the median of all SDP models(0.41). This suggests that researchers should be cautious in using KMeans as a benchmark with which to compare their new UnSDP. 3. Un MR (metric or 1/metric ranking) performs least well. However, this is based on only a small subset of the data, since only 39 out of the 234 Un MR experimental results use MCC with the majority only reporting Popt or ACC.</p>
        <p>Figure 6 shows the performance of cross-project prediction models. Un Fuzzy again seems to have the best performance. Presently there are quite limited UnSDP results so further experimentation would be welcome.Figure 6 shows the performance of cross-project prediction models. Un Fuzzy again seems to have the best performance. Presently there are quite limited UnSDP results so further experimentation would be welcome.</p>
        <p>We also observe that an unsupervised spectrum clustering models approach (Un SC family) was proposed in [P25], and their model was evaluated with 26 different datasets. It outperformed most of the SDP and other UnSDP models in within-project prediction, and outperformed all others UnSDP and SDP in cross-project prediction. But unfortunately they only reported AUC, so their model could not be included in Figure 5 and Figure 6. This is somewhat frustrating as more complete reporting (of the confusion matrix) would enable integration of their results into meta-analyses.We also observe that an unsupervised spectrum clustering models approach (Un SC family) was proposed in [P25], and their model was evaluated with 26 different datasets. It outperformed most of the SDP and other UnSDP models in within-project prediction, and outperformed all others UnSDP and SDP in cross-project prediction. But unfortunately they only reported AUC, so their model could not be included in Figure 5 and Figure 6. This is somewhat frustrating as more complete reporting (of the confusion matrix) would enable integration of their results into meta-analyses.</p>
        <p>In summary, UnSDP models are comparable with SDP models both in within-project and cross-project prediction. Compared with most of the SDP models, Un Fuzzy, Un NN and Un SC are potentially the strongest UnSDP approaches. However, we also checked all 28 primary studies that include SDP models to see whether any parameter tuning was used. Surprisingly, we found only 3 studies state clearly that tuned SDP models are used in their comparisons, 6 studies use default parameter and 19 studies provide no information about tuning. Therefore, most of SDP models used in comparison might not be best models. We discuss this potential source of bias in the threats to validity (Section 5).In summary, UnSDP models are comparable with SDP models both in within-project and cross-project prediction. Compared with most of the SDP models, Un Fuzzy, Un NN and Un SC are potentially the strongest UnSDP approaches. However, we also checked all 28 primary studies that include SDP models to see whether any parameter tuning was used. Surprisingly, we found only 3 studies state clearly that tuned SDP models are used in their comparisons, 6 studies use default parameter and 19 studies provide no information about tuning. Therefore, most of SDP models used in comparison might not be best models. We discuss this potential source of bias in the threats to validity (Section 5).</p>
        <p>Due to incomplete reporting, there are 823 unchecked experimental results that could not be compared with MCC in Section 4.1.2. Among these results, AUC and Popt are dominant reported performance measures. Because effortaware just-in-time (JiT) defect-prone commit prediction differentiates with defect-prone module prediction, effort consideration is its important characteristic. To present a comprehensive comparison of UnSDP and SDP, we carry out the meta-analysis with Popt for JiT prediction in this section. To solid our meta-analysis results, we also com- q q q q q q q q q 42 12 72 6 5 72 Supervised Unsupervised pare the unchecked UnSDP and SDP with AUC in Section 5. Figure 7 presents Popt comparison for JiT prediction. It illustrates that SDP model GA (multi-objective optimization based on genetic algorithm) performs better than the only UnSDP model family UN MR. However, GA needs to cost more time than UN MR for model construction analyzed in [P49]. From Figure 7, we can found that UN MR is still a good choice for practitioners since it has better prediction performance and less time. In summary, for JiT defect prediction, the comparisons with Popt indicates UnSDP models are comparable with SDP models both in within-project and cross-project prediction.Due to incomplete reporting, there are 823 unchecked experimental results that could not be compared with MCC in Section 4.1.2. Among these results, AUC and Popt are dominant reported performance measures. Because effortaware just-in-time (JiT) defect-prone commit prediction differentiates with defect-prone module prediction, effort consideration is its important characteristic. To present a comprehensive comparison of UnSDP and SDP, we carry out the meta-analysis with Popt for JiT prediction in this section. To solid our meta-analysis results, we also com- q q q q q q q q q 42 12 72 6 5 72 Supervised Unsupervised pare the unchecked UnSDP and SDP with AUC in Section 5. Figure 7 presents Popt comparison for JiT prediction. It illustrates that SDP model GA (multi-objective optimization based on genetic algorithm) performs better than the only UnSDP model family UN MR. However, GA needs to cost more time than UN MR for model construction analyzed in [P49]. From Figure 7, we can found that UN MR is still a good choice for practitioners since it has better prediction performance and less time. In summary, for JiT defect prediction, the comparisons with Popt indicates UnSDP models are comparable with SDP models both in within-project and cross-project prediction.</p>
        <p>To investigate which models are better among UnSDP models, we compare them with more detailed subfamily categories. Table 15 lists the mean value of MCC, the corresponding 95% CI and the number of experiments for all available unsupervised models in within-project defect prediction. Here, we only list the models in which experimental sample size is greater than 1.To investigate which models are better among UnSDP models, we compare them with more detailed subfamily categories. Table 15 lists the mean value of MCC, the corresponding 95% CI and the number of experiments for all available unsupervised models in within-project defect prediction. Here, we only list the models in which experimental sample size is greater than 1.</p>
        <p>From these results, we can see that FCM and FSOMs learners perform best. Although EXP also performs competitively, we do not recommend it since EXP requires software modules to be classified by an expert manually. Hence we do not consider it to be an effective UnSDP approach.From these results, we can see that FCM and FSOMs learners perform best. Although EXP also performs competitively, we do not recommend it since EXP requires software modules to be classified by an expert manually. Hence we do not consider it to be an effective UnSDP approach.</p>
        <p>For JiT defect prediction, we conduct the comparison based on 197 high quality Popt values including 12 UnSDP methods based on MR (metric ranking). Here, we named these methods with MR-v and v is the metric used to rank, e.g. MR-RFC. Also if time-wise cross validation is used in one method, we name it MR-v TIME, otherwise it means normal n-fold cross validation is used. Table 16 shows MR-CCUM performs best among those unsupervised JiT within-project prediction methods. MR-LT and MR-AGE 4.3. RQ6: What is the impact of dataset characteristics on predictive performance?For JiT defect prediction, we conduct the comparison based on 197 high quality Popt values including 12 UnSDP methods based on MR (metric ranking). Here, we named these methods with MR-v and v is the metric used to rank, e.g. MR-RFC. Also if time-wise cross validation is used in one method, we name it MR-v TIME, otherwise it means normal n-fold cross validation is used. Table 16 shows MR-CCUM performs best among those unsupervised JiT within-project prediction methods. MR-LT and MR-AGE 4.3. RQ6: What is the impact of dataset characteristics on predictive performance?</p>
        <p>Studies ([P36] and [P37]) reveal inconsistent results between predicting defect-prone modules by each project and by all projects as a whole in a just-in-time context. Stimulated by these findings, we also undertake a comparison between UnSDP and SDP models based on individual projects. Before the comparison of RQ6, we removed two kinds of unsuitable data from Table 15: (i) EXP related data since expert-based approach is more subjective. (ii) Just-in-time datasets(including BUG, POS, MOZ, PLA, JDT, COL) due to their prominent characteristics compared with defectprone module prediction dataset.Studies ([P36] and [P37]) reveal inconsistent results between predicting defect-prone modules by each project and by all projects as a whole in a just-in-time context. Stimulated by these findings, we also undertake a comparison between UnSDP and SDP models based on individual projects. Before the comparison of RQ6, we removed two kinds of unsuitable data from Table 15: (i) EXP related data since expert-based approach is more subjective. (ii) Just-in-time datasets(including BUG, POS, MOZ, PLA, JDT, COL) due to their prominent characteristics compared with defectprone module prediction dataset.</p>
        <p>Figure 8 presents the comparison between the best UnSDP and SDP models by project. In this figure, horizontal axis label stands for each project and its fault rate. For example, Ant (0.24) denotes the project Ant (Ant1.3, Ant 1.4, etc. are different versions which we unify as Ant). The fault rate 0.24 is average value across all versions. Figure 8 shows that there are similar performance change tendencies for UnSDP different projects. The two groups both have higher prediction performance on some projects (such as Argouml, AR5, Weka, etc.) and lower performance on other projects (such as KC2, ZXing, etc.). This suggests dataset characteristics have a non-trivial moderating effect on prediction performance. We analysed the largest two abnormal gaps Xerces and Xalan (Diff &gt; 0.2) between UnSDP and SDP in Figure 8. According to the original primary study [P26], the gaps of Xerces and Xalan seem to be caused by their dataset characteristics.Figure 8 presents the comparison between the best UnSDP and SDP models by project. In this figure, horizontal axis label stands for each project and its fault rate. For example, Ant (0.24) denotes the project Ant (Ant1.3, Ant 1.4, etc. are different versions which we unify as Ant). The fault rate 0.24 is average value across all versions. Figure 8 shows that there are similar performance change tendencies for UnSDP different projects. The two groups both have higher prediction performance on some projects (such as Argouml, AR5, Weka, etc.) and lower performance on other projects (such as KC2, ZXing, etc.). This suggests dataset characteristics have a non-trivial moderating effect on prediction performance. We analysed the largest two abnormal gaps Xerces and Xalan (Diff &gt; 0.2) between UnSDP and SDP in Figure 8. According to the original primary study [P26], the gaps of Xerces and Xalan seem to be caused by their dataset characteristics.</p>
        <p>We investigated one dataset characteristic, namely imbalance (fault rate), but the one-way ANOVA analysis for fault rate and max MCC shows that it is a very weak explanatory factor for MCC (one-way ANOVA F = 0.66, p-value = 0.6261). In summary, we consider the dataset characteristics have an obvious impact on predictive performance. However, it is unclear which kinds of characteristics are important. We consider this would be well worth exploring in the future.We investigated one dataset characteristic, namely imbalance (fault rate), but the one-way ANOVA analysis for fault rate and max MCC shows that it is a very weak explanatory factor for MCC (one-way ANOVA F = 0.66, p-value = 0.6261). In summary, we consider the dataset characteristics have an obvious impact on predictive performance. However, it is unclear which kinds of characteristics are important. We consider this would be well worth exploring in the future.</p>
        <p>Threats to internal validity relate to our ability to reconstruct a confusion matrix for each experimental result.Threats to internal validity relate to our ability to reconstruct a confusion matrix for each experimental result.</p>
        <p>Ideally, we would recompute the confusion matrix for all 2456 individual experimental results, however, there are 823 ( 33%) results that could not be checked due to incomplete reporting. Where we could check we found 262 results are problematic, but it may well be that there is an interaction between non-reporting and error-proneness so simply extrapolating from our error-rate findings may not be safe.Ideally, we would recompute the confusion matrix for all 2456 individual experimental results, however, there are 823 ( 33%) results that could not be checked due to incomplete reporting. Where we could check we found 262 results are problematic, but it may well be that there is an interaction between non-reporting and error-proneness so simply extrapolating from our error-rate findings may not be safe.</p>
        <p>Another threat is our implementation of re-computation with R. Where possible we compared our partial analysis with the java tool DConfusion [11]. Our results are consistent with DConfusion.Another threat is our implementation of re-computation with R. Where possible we compared our partial analysis with the java tool DConfusion [11]. Our results are consistent with DConfusion.</p>
        <p>The third threat is that we only carry out the fine level comparison between UnSDP and SDP with MCC. Although MCC is the best choice for our meta-analysis, to strengthen our analysis, we also conduct an extra analysis with high quality 458 AUC results (Predatory = No and AUC is not NA) . Our comparison results also indicate that Un Fuzzy is the most potential approach for withinproject prediction. However, the number of experiments is relatively smaller, and more evaluations of Un Fuzzy would be recommended. The family of Un SC reported only with AUC performs best in cross-project prediction. The analysis results could be found in our Mendeley data.The third threat is that we only carry out the fine level comparison between UnSDP and SDP with MCC. Although MCC is the best choice for our meta-analysis, to strengthen our analysis, we also conduct an extra analysis with high quality 458 AUC results (Predatory = No and AUC is not NA) . Our comparison results also indicate that Un Fuzzy is the most potential approach for withinproject prediction. However, the number of experiments is relatively smaller, and more evaluations of Un Fuzzy would be recommended. The family of Un SC reported only with AUC performs best in cross-project prediction. The analysis results could be found in our Mendeley data.</p>
        <p>Further, we could only check for consistency errors so it is quite possible that the actual rate of experimental analysis errors is greater than we were able to detect. We have to remark that an overall (knowable) error rate of 11% across all publication venues does not engender confidence. We hope a move towards more open science [27] will assist in this regard.Further, we could only check for consistency errors so it is quite possible that the actual rate of experimental analysis errors is greater than we were able to detect. We have to remark that an overall (knowable) error rate of 11% across all publication venues does not engender confidence. We hope a move towards more open science [27] will assist in this regard.</p>
        <p>Threats to external validity concern the selected experiments and the extent to which the experiments we have located generalise. By undertaking an explicitly systematic approach [5] to this review we hope to have included all relevant studies. These have used a wide range of datasets but we cannot be certain how representative these might be of all possible software defect prediction scenarios. q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.0 Another difficulty with the comparison of unsupervised and supervised classification is that the majority of papers appear to focus principally upon novel or innovative uses of UnSDP and that the comparator supervised approaches serve the role of very basic benchmarks. Consequently it is not always obvious that state of the art supervised algorithms are deployed, nor that much effort has gone into hyper-parameter tuning. For instance, only 3/28 papers explicitly state they have tuned the SDP models and a further 6/28 indicated defaults were chosen. Thus there is a danger we are sometimes comparing state of the art UnSDP with off-the-shelf SDP.Threats to external validity concern the selected experiments and the extent to which the experiments we have located generalise. By undertaking an explicitly systematic approach [5] to this review we hope to have included all relevant studies. These have used a wide range of datasets but we cannot be certain how representative these might be of all possible software defect prediction scenarios. q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.0 Another difficulty with the comparison of unsupervised and supervised classification is that the majority of papers appear to focus principally upon novel or innovative uses of UnSDP and that the comparator supervised approaches serve the role of very basic benchmarks. Consequently it is not always obvious that state of the art supervised algorithms are deployed, nor that much effort has gone into hyper-parameter tuning. For instance, only 3/28 papers explicitly state they have tuned the SDP models and a further 6/28 indicated defaults were chosen. Thus there is a danger we are sometimes comparing state of the art UnSDP with off-the-shelf SDP.</p>
        <p>A final danger is researcher bias and the tendency to confirm what we already believe to be true. This is an additional reason why we have made our materials and research processes public 1 .A final danger is researcher bias and the tendency to confirm what we already believe to be true. This is an additional reason why we have made our materials and research processes public 1 .</p>
        <p>UnSDP techniques are attracting more and more researchers and practitioners since labelling information for the training data is not a prerequisite. We reviewed 49 unsupervised software defect prediction primary studies published between January 2000 to March 2018, which includes 2456 independent experimental results. These 2456 independent results involved 128 software projects (from NASA, PROMISE, ISM, AEEEM, etc.) and 25 prediction model families (172 models). All the UnSDP models and labelling techniques for clusters are listed in Table 11 andTable 12. These tables also provide researchers with an indication of the diversity of unsupervised learning techniques used for software defect prediction.UnSDP techniques are attracting more and more researchers and practitioners since labelling information for the training data is not a prerequisite. We reviewed 49 unsupervised software defect prediction primary studies published between January 2000 to March 2018, which includes 2456 independent experimental results. These 2456 independent results involved 128 software projects (from NASA, PROMISE, ISM, AEEEM, etc.) and 25 prediction model families (172 models). All the UnSDP models and labelling techniques for clusters are listed in Table 11 andTable 12. These tables also provide researchers with an indication of the diversity of unsupervised learning techniques used for software defect prediction.</p>
        <p>We have carried out two kinds of analysis. Firstly, we conducted a bibliometric analysis. We found a growth in research activity in recent years. We also have found some problems with experimental data quality. We were surprised to find that 25 out of 49 studies did not explicitly report whether a cross-validation procedure was used or not, and there were 14 papers (out of 49) that could not be checked for the existence of problematic data due to incomplete reporting. Therefore, the quality and completeness of reporting should be paid more attention when publishing papers. Where we were able to assess consistency of results, we found something of the order of 11% of all results were demonstrably in error due to inconsistencies. This was an issue in both 'predatory' and non-predatory publication venues.We have carried out two kinds of analysis. Firstly, we conducted a bibliometric analysis. We found a growth in research activity in recent years. We also have found some problems with experimental data quality. We were surprised to find that 25 out of 49 studies did not explicitly report whether a cross-validation procedure was used or not, and there were 14 papers (out of 49) that could not be checked for the existence of problematic data due to incomplete reporting. Therefore, the quality and completeness of reporting should be paid more attention when publishing papers. Where we were able to assess consistency of results, we found something of the order of 11% of all results were demonstrably in error due to inconsistencies. This was an issue in both 'predatory' and non-predatory publication venues.</p>
        <p>Secondly, we undertook a meta-analysis concerning the performance of UnSDP models. To compare these results in a reliable way, we recomputed the confusion matrices of the primary studies to obtain consistent performance measures. In our vote-counting analysis, MCC is the main performance measure and others (such as AUC, F1, Popt, etc) are only used when MCC is unavailable. We found, that UnSDP models are comparable with SDP models both for within-project and cross-project prediction. This indicates UnSDP models may not be as problematic as might be supposed, so we suggest they might be considered in most situations when labelled training data is scarce. Among the different UnSDP learners, Un Fuzzy and Un SC appear to have most potential. Meanwhile, for effort-aware JiT defect prediction, the state-of-the-art complex SDP model GA (multi-objective optimization based on genetic algorithms) performs better than the simple sorting UnSDP model (UN MR). However, it less clear whether it has better performance when effort is ignored. Overall, we consider that UN MR remains a good choice for practitioners since it has comparable prediction performance to GA model and requires less time.Secondly, we undertook a meta-analysis concerning the performance of UnSDP models. To compare these results in a reliable way, we recomputed the confusion matrices of the primary studies to obtain consistent performance measures. In our vote-counting analysis, MCC is the main performance measure and others (such as AUC, F1, Popt, etc) are only used when MCC is unavailable. We found, that UnSDP models are comparable with SDP models both for within-project and cross-project prediction. This indicates UnSDP models may not be as problematic as might be supposed, so we suggest they might be considered in most situations when labelled training data is scarce. Among the different UnSDP learners, Un Fuzzy and Un SC appear to have most potential. Meanwhile, for effort-aware JiT defect prediction, the state-of-the-art complex SDP model GA (multi-objective optimization based on genetic algorithms) performs better than the simple sorting UnSDP model (UN MR). However, it less clear whether it has better performance when effort is ignored. Overall, we consider that UN MR remains a good choice for practitioners since it has comparable prediction performance to GA model and requires less time.</p>
        <p>We also note that when clustering-based UnSDP approaches are applied, it is a simple and effective way to use the node distribution in clusters (see Table 12) to label each cluster as defective or not according to the Pareto principle (sometimes referred to as the 80:20 rule).We also note that when clustering-based UnSDP approaches are applied, it is a simple and effective way to use the node distribution in clusters (see Table 12) to label each cluster as defective or not according to the Pareto principle (sometimes referred to as the 80:20 rule).</p>
        <p>Finally, we found dataset characteristics can have a clear impact on predictive performance, no matter whether UnSDP or SDP models are used. Therefore it will be worth exploring in more depth which of these dataset characteristics are most important. Likewise, the exploration of the interaction between learner and dataset could be fruitful, as it would appear unlikely that a single learning algorithm, be it supervised or unsupervised, will always be optimal.Finally, we found dataset characteristics can have a clear impact on predictive performance, no matter whether UnSDP or SDP models are used. Therefore it will be worth exploring in more depth which of these dataset characteristics are most important. Likewise, the exploration of the interaction between learner and dataset could be fruitful, as it would appear unlikely that a single learning algorithm, be it supervised or unsupervised, will always be optimal.</p>
        <p>Search completed 7th March, 2018.Search completed 7th March, 2018.</p>
        <p># of remaining papers Search of five academic search engines# of remaining papers Search of five academic search engines</p>
        <p>Here we seek to understand the researchers' approach to SDP benchmark through their citations of related work and published results.Here we seek to understand the researchers' approach to SDP benchmark through their citations of related work and published results.</p>
        <p>We parenthesise the 're' of computation to convey that for some papers we are constructing the confusion matrix ab initio. In other situations there is already an explicit matrix but we re-construct it from other data provided, although potentially by different means from that presented in the original paper. For brevity, in the remainder of our paper we simply state 're-computation'.We parenthesise the 're' of computation to convey that for some papers we are constructing the confusion matrix ab initio. In other situations there is already an explicit matrix but we re-construct it from other data provided, although potentially by different means from that presented in the original paper. For brevity, in the remainder of our paper we simply state 're-computation'.</p>
        <p>The F-measure that researchers use is the balanced F score and is often referred to as F1. Other weightings are possible but in practice seldom used[9].The F-measure that researchers use is the balanced F score and is often referred to as F1. Other weightings are possible but in practice seldom used[9].</p>
        <p>https://beallslist.weebly.com/https://beallslist.weebly.com/</p>
        <p>https://beallslist.weebly.com/standalone-journals.htmlhttps://beallslist.weebly.com/standalone-journals.html</p>
        <p>https://libguides.caltech.edu/c.php?g=512665&amp;p=3503029https://libguides.caltech.edu/c.php?g=512665&amp;p=3503029</p>
        <p>Note that year 2018 is incomplete.Note that year 2018 is incomplete.</p>
        <p>Despite the recent emergence of software defect studies that only vote count studies where there is a statistically 'significant' effect sometimes referred to as a win-tie-loss procedure, this is in error and studies should always be included[23,24].Despite the recent emergence of software defect studies that only vote count studies where there is a statistically 'significant' effect sometimes referred to as a win-tie-loss procedure, this is in error and studies should always be included[23,24].</p>
        <p>We would like to thank the editors and the anonymous reviewers for their insightful comments and suggestions. We also wish to acknowledge the use of the DConfusion tool developed by David Bowes and David Gray whilst at the University of Hertfordshire. This work was supported by the National Key Basic Research Program of China [2018YFB1004401]; the National Natural Science Foundation of China [61972317, 61402370].We would like to thank the editors and the anonymous reviewers for their insightful comments and suggestions. We also wish to acknowledge the use of the DConfusion tool developed by David Bowes and David Gray whilst at the University of Hertfordshire. This work was supported by the National Key Basic Research Program of China [2018YFB1004401]; the National Natural Science Foundation of China [61972317, 61402370].</p>
        <p>For performance comparison, we conclude supervised defect prediction models used in our review studies as Ta-ble A.17.For performance comparison, we conclude supervised defect prediction models used in our review studies as Ta-ble A.17.</p>
    </text>
</tei>
