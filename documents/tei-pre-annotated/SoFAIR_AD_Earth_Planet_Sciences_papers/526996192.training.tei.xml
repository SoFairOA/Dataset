<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-13T16:02+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>This paper aims to improve automation in brick segmentation and crack detection of masonry walls through image-based techniques and machine learning. Initially, a large dataset of hand-labelled images of different in colour, texture, and size of brickwork masonry walls has been developed. Then, different deep learning networks (
            <rs type="software">U-Net</rs>, 
            <rs type="software">DeepLabV3+</rs>, 
            <rs type="software">U-Net</rs> (SM), LinkNet (SM), and FPN (SM)) were utilised and their quality was assessed. Furthermore, the ability to generate geometric models of masonry structures and the evaluation of the geometric properties of detected cracks was also investigated. Additional metrics were also developed to compare the CNN output with other image-processing algorithms. From the analysis of results it was shown that the use of machine learning, for brick segmentation, provides better outcome than typical image-processing applications. This implementation of deep-learning for crack detection and localisation of bricks in masonry walls highlights the great potential of new technologies for documentation of masonry fabric.
        </p>
        <p>Masonry is one of the oldest building materials. It is composed of individual masonry units (bricks, blocks, ashlars, irregular stones, etc.) jointed together with or without mortar. Masonry structures represent the highest building stock worldwide. A large portion of masonry structures are "Listed Buildings" and form part of our "Cultural Heritage" [34]. According to UN Sustainable Development Goal 11,Target 11.4, there is a need to "repair and maintenance" rather than "demolish and rebuild" our structures. Currently, for the inspection of existing masonry structures, damage pathologies (i.e., cracking, spalling etc) are captured with traditional techniques (e.g., visual inspection and manual surveying methods [18]), which are labour intensive, subjective, and error prone [37,40]. Additionally, for the documentation of masonry structures, the size and location of masonry units and mortar is of particular interest for the engineers and architects, since such information can be used for the development of high-fidelity computational models for their structural analysis and assessment [17,26,30,31].Masonry is one of the oldest building materials. It is composed of individual masonry units (bricks, blocks, ashlars, irregular stones, etc.) jointed together with or without mortar. Masonry structures represent the highest building stock worldwide. A large portion of masonry structures are "Listed Buildings" and form part of our "Cultural Heritage" [34]. According to UN Sustainable Development Goal 11,Target 11.4, there is a need to "repair and maintenance" rather than "demolish and rebuild" our structures. Currently, for the inspection of existing masonry structures, damage pathologies (i.e., cracking, spalling etc) are captured with traditional techniques (e.g., visual inspection and manual surveying methods [18]), which are labour intensive, subjective, and error prone [37,40]. Additionally, for the documentation of masonry structures, the size and location of masonry units and mortar is of particular interest for the engineers and architects, since such information can be used for the development of high-fidelity computational models for their structural analysis and assessment [17,26,30,31].</p>
        <p>In the last ten years, advances in laser scanning and photogrammetry have started to drastically change the building industry since such techniques are able to capture rapidly and remotely digital objects and features in images and points' cloud format. Past research demonstrated that computer-vision and image-processing can be used to create detailed digital records of masonry structures [25] using feature detection [4,7,10,33] and segmentation algorithms [4,6,27]. Applications of Image-processing can be used to quantify deformations on masonry (i.e., when coupled with the installation of simple markers on the structure on key-locations to identify their position [5,42]). Furthermore, featureextraction has the potential to generate the "as is" numerical model of masonry for detailed analysis [26,32,43,44]. Those applications of image-processing offer a low-cost and reliable alternative to more traditional methods. Although, feature detection and segmentation has already been applied to identify the shape and location of masonry units from images [8,15,36,39,46], their application proves challenging due to digital noise produced by the change in brightness, colour, and texture presented within the digital images.In the last ten years, advances in laser scanning and photogrammetry have started to drastically change the building industry since such techniques are able to capture rapidly and remotely digital objects and features in images and points' cloud format. Past research demonstrated that computer-vision and image-processing can be used to create detailed digital records of masonry structures [25] using feature detection [4,7,10,33] and segmentation algorithms [4,6,27]. Applications of Image-processing can be used to quantify deformations on masonry (i.e., when coupled with the installation of simple markers on the structure on key-locations to identify their position [5,42]). Furthermore, featureextraction has the potential to generate the "as is" numerical model of masonry for detailed analysis [26,32,43,44]. Those applications of image-processing offer a low-cost and reliable alternative to more traditional methods. Although, feature detection and segmentation has already been applied to identify the shape and location of masonry units from images [8,15,36,39,46], their application proves challenging due to digital noise produced by the change in brightness, colour, and texture presented within the digital images.</p>
        <p>An alternative approach for image segmentation of masonry units and mortar is with the use of ML (Machine-Learning), such as Deep-Learning (DL; subset of M.L.) [20,41]. Some typical examples of DL include:An alternative approach for image segmentation of masonry units and mortar is with the use of ML (Machine-Learning), such as Deep-Learning (DL; subset of M.L.) [20,41]. Some typical examples of DL include:</p>
        <p>• Classic-Networks: Multilayer architecture of fully-connected-layers of neurons, which are typically used in data classification and predictions. • Convolutional-Neural-Networks (CNN): Typically used for image classification and segmentation.• Classic-Networks: Multilayer architecture of fully-connected-layers of neurons, which are typically used in data classification and predictions. • Convolutional-Neural-Networks (CNN): Typically used for image classification and segmentation.</p>
        <p>• Fully-Connected-Networks (FCN): Typically used for imagesegmentation, often combined with a CNN backbone.• Fully-Connected-Networks (FCN): Typically used for imagesegmentation, often combined with a CNN backbone.</p>
        <p>The most efficient image-segmentation architectures consider the use of FCN [12][13][14]28,29,38], since they allow any image resolution as input by replacing the final fully-connected-layers of a CNN with convolution layers [29]. CNN and FCN architectures require a large labelled or annotated dataset, trained in representative sample to provide adequate results. However, they have the potential to detect complex features by training the model to a large variety of different cases. Furthermore, for smaller datasets, DL approaches can use a technique called Transfer-Learning which involves pre-training a CNN model on a different and larger dataset with the purpose to learn to detect complex features. This has been shown to provide a reduction to the required computational effort and a boost to overall performance of the model for smaller datasets [21]. Transfer learning can be applied to any CNN and FCN (Fully Convolutional Networks) architecture. Multiple architectures have been used to demonstrate the ability of DL in the semantic segmentation of images.The most efficient image-segmentation architectures consider the use of FCN [12][13][14]28,29,38], since they allow any image resolution as input by replacing the final fully-connected-layers of a CNN with convolution layers [29]. CNN and FCN architectures require a large labelled or annotated dataset, trained in representative sample to provide adequate results. However, they have the potential to detect complex features by training the model to a large variety of different cases. Furthermore, for smaller datasets, DL approaches can use a technique called Transfer-Learning which involves pre-training a CNN model on a different and larger dataset with the purpose to learn to detect complex features. This has been shown to provide a reduction to the required computational effort and a boost to overall performance of the model for smaller datasets [21]. Transfer learning can be applied to any CNN and FCN (Fully Convolutional Networks) architecture. Multiple architectures have been used to demonstrate the ability of DL in the semantic segmentation of images.</p>
        <p>There are different architectures that could be used for the segmentation of masonry units. U-Net is a FCN architecture that was developed initially for biomedical image-segmentation [38]. It is based on a contracting followed by an expansive path, which initially decreases and then increases the input-size. Due to its performance, U-Net is considered the benchmark in image-segmentation and has been used extensively especially in relatively small datasets. 
            <rs type="software">DeepLabV3+</rs> is another state-of-the-art FCN architecture for general use semantic segmentation [13,14]. Additional features on 
            <rs type="software">DeepLabV3+</rs> compared to previous iterations and simpler architectures aim to deliver a faster and more accurate network. FPN (Feature-Pyramid-Network) is an FCN network for object detection [28]. It is a feature extractor that follows a bottom-up followed by a top-down path with the addition of lateral connections between the two to merge feature maps of equal spatial size. LinkNet is a light FCN network developed for pixel-wise segmentation optimized for efficiency [12]. LinkNet is a lightweight and fast FCN architecture able to be used for real time applications (i.e., video streaming).
        </p>
        <p>ML applications have already seen use in structural engineering due to their immerse potential to assist with visual inspection and monitoring applications [41]. In cases of damage detection, ML provides the means to identify, locate, and asses detected deterioration on structural elements. Valero et al. [47] extracted statistical data from a 3D pointcloud and used them to train a ML algorithm for the detection and classification of chromatic (i.e., discoloration) and geometric defects on ashlar masonry (using logistic regression with multi-class classification). However, most typical applications of defect detection include the use of DL due to its architecture, which allows the detection of complex features on unstructured data. [11], investigated the use of patch classification using CNN algorithm coupled with a classifier to identify small patches that contained damaged location of historical structures. Their work was continued by 
            <rs type="software">Ali</rs> [2], where Faster-R-CNN was used for the detection of bounding-boxes that contain locations of damaged bricks on masonry structures. The same year, Wang et al. [48] used a Faster-R-CNN model based on ResNet101 for the real time detection and classification of bounding-boxes that include defected areas on historic masonry buildings. A workflow that utilises mobile-phones for the direct capture and processing of image-data was also proposed by them. Brackenbury et al. [9] discussed the use of 
            <rs type="software">GoogleNet-Inception-</rs>V3 algorithm and the use of transfer learning for the classification and segmentation of mortar and defects in masonry components. Each defect (i. e., cracking, spalling, or vegetation) was classified separately. Kalfarisi et al. [24] used 
            <rs type="software">Mask-RCNN</rs> and 
            <rs type="software">FRCNN-FED</rs> to detect bounding boxes that contain cracks on structures and performed pixel-wise segmentation within the detected areas. Furthermore, they transferred the segmented locations to a 3D reality-mesh object, generated using photogrammetry. Recently, Dais et al. [16] tested different CNN algorithms for the detection of cracks on masonry images. Both patchclassification (with 95.3% accuracy) and semantic-segmentation (with 79.6% F1-score) on pre-trained networks were investigated.
        </p>
        <p>Past research demonstrated that the use of CNN algorithms for the detection of masonry units and mortar is also necessary to provide a complete visualization of the detailed geometry of masonry structures. Ibrahim et al. [23] proposed the use of U-Net for the segmentation of mortar in masonry structures with different bonding pattern (i.e., including rubble). Additionally, they used watershed-transform for the segmentation of each brick unit. Ergün Hatir and ˙Ince [19] proposed the use of Mask-R-CNN for the classification and segmentation of masonry units in historic stone masonry buildings. Each stone detected was classified to a different lithology based on their detected features (i.e., colour, texture, etc).Past research demonstrated that the use of CNN algorithms for the detection of masonry units and mortar is also necessary to provide a complete visualization of the detailed geometry of masonry structures. Ibrahim et al. [23] proposed the use of U-Net for the segmentation of mortar in masonry structures with different bonding pattern (i.e., including rubble). Additionally, they used watershed-transform for the segmentation of each brick unit. Ergün Hatir and ˙Ince [19] proposed the use of Mask-R-CNN for the classification and segmentation of masonry units in historic stone masonry buildings. Each stone detected was classified to a different lithology based on their detected features (i.e., colour, texture, etc).</p>
        <p>From the above, although some work has been done in the segmentation of cracks and mortar, there is still no research on coupling brick segmentation with crack detection in masonry structures. The aim of this research is to couple brick segmentation and defect detection techniques and automatically provide holistic and real time information for the documentation, visual inspection, and evaluation of existing masonry structures. In this research, brick segmentation and defect detection (i.e. cracks) were acquired using different state-of-the-art FCN architectures including 
            <rs type="software">U-Net</rs>, 
            <rs type="software">DeepLabV3+</rs>, 
            <rs type="software">U-Net</rs> (SM), LinkNet (SM), and FPN (SM). The work presented here provides algorithms necessary for the automatic documentation and structural inspection of masonry structures from digital images.
        </p>
        <p>Initially, a database was created that includes various images of brick masonry walls of regular pattern (ignoring rubble masonry). Some images were obtained from the internet while others were captured using different sources (i.e., DSLR camera, smartphones) of varied resolution. To improve generalisation, the dataset included masonry walls with cracks, with windows and doors, with varied in colour masonry units as well as with varied illumination and capture-angle. A sample of the raw database used for training and evaluation is shown in Fig. 1.Initially, a database was created that includes various images of brick masonry walls of regular pattern (ignoring rubble masonry). Some images were obtained from the internet while others were captured using different sources (i.e., DSLR camera, smartphones) of varied resolution. To improve generalisation, the dataset included masonry walls with cracks, with windows and doors, with varied in colour masonry units as well as with varied illumination and capture-angle. A sample of the raw database used for training and evaluation is shown in Fig. 1.</p>
        <p>In total 107 images of masonry structures were fully annotated, including multi-class annotations of masonry blocks, openings, lintels, other/random objects, and background (Fig. 2). Each class was annotated to a different binarized image where black was the background and white was the annotated element. The software used for annotation was the "
            <rs type="software">SuperAnnotate</rs> V.
            <rs type="version">1.1.0</rs>". This specific software was selected since it allowed vector annotations, which permits a simplified annotation of each block, ignoring unnecessary details. It was found that simpler shapes could allow easier transferability to CAD environment, and this will also be represented to the output from the CNN model.
        </p>
        <p>Each image resolution was normalised based on the resolution of the image-slice passed through the network. This was to allow each slice to contain several blocks, but not allow the average block-size to be larger than the image-slice. Doing so, the accuracy of the model has increased, since each image-slice had similar-sized blocks that improved the detection rate. The normalised resolution of each image was evaluated and compared with the normalised resolution of the image-slice (imagepart passed through the network). Thus, the image was allowed a specific range of resolution. Although, using a specific resolution of block elements (i.e., pixels contained within a block; by capturing pictures with specific resolution, angle, and distance), would potentially increase the accuracy. However, that would reduce generalisation of the final model and complicate its use (i.e., would require the user to capture images from specific distance/angle). Eqs. (1) to (5) were used to adjust each image. If the image was within the limits of Eq. (3), it retained its resolution. However, if the image was outside the limits of Eq. (3), it was adjusted based on Eq. (5).Each image resolution was normalised based on the resolution of the image-slice passed through the network. This was to allow each slice to contain several blocks, but not allow the average block-size to be larger than the image-slice. Doing so, the accuracy of the model has increased, since each image-slice had similar-sized blocks that improved the detection rate. The normalised resolution of each image was evaluated and compared with the normalised resolution of the image-slice (imagepart passed through the network). Thus, the image was allowed a specific range of resolution. Although, using a specific resolution of block elements (i.e., pixels contained within a block; by capturing pictures with specific resolution, angle, and distance), would potentially increase the accuracy. However, that would reduce generalisation of the final model and complicate its use (i.e., would require the user to capture images from specific distance/angle). Eqs. (1) to (5) were used to adjust each image. If the image was within the limits of Eq. (3), it retained its resolution. However, if the image was outside the limits of Eq. (3), it was adjusted based on Eq. (5).</p>
        <p>where, ImDim is the average image-resolution, OutDim is the average image-slice resolution, MaxLimit/MinLimit is the maximum over the minimum limit that is allowed to disregard further adjustments, and x final /y final is the final resolution per image-axis. Furthermore, the only variables provided were the MaxRatio/Min-Ratio that are the Maximum over the Minimum preferred ratio to adjust the images based on the size of the image-slices. Those values were adjusted manually until all the image slices contained a satisfactory number of block elements for the specific database. For the current database, the values used were the following:where, ImDim is the average image-resolution, OutDim is the average image-slice resolution, MaxLimit/MinLimit is the maximum over the minimum limit that is allowed to disregard further adjustments, and x final /y final is the final resolution per image-axis. Furthermore, the only variables provided were the MaxRatio/Min-Ratio that are the Maximum over the Minimum preferred ratio to adjust the images based on the size of the image-slices. Those values were adjusted manually until all the image slices contained a satisfactory number of block elements for the specific database. For the current database, the values used were the following:</p>
        <p>The size of each image slice was equal to 224 × 224 × 3 (i.e., OutDim = 224). Each image was padded to adjust its resolution to multiples of the slice-resolution per axis (i.e., 224 pixels) to retain the original aspect-ratio when the image is sliced to smaller parts. The padding value was equal to 255, which created a white border around most of the edges. Then, the white padding was used as filtered locations of postprocessed images (i.e., images were background and openings have been replaced with white) and would be instantly disregarded from the CNN output. This provided a total of 2814 image-slices which were used as training and validation (i.e., approx. 25% of them were used for validation), see (Fig. 3). Other slice-resolutions were also tested. The smaller resolutions provided more accurate models. This was due to the increased number of training and validation data.The size of each image slice was equal to 224 × 224 × 3 (i.e., OutDim = 224). Each image was padded to adjust its resolution to multiples of the slice-resolution per axis (i.e., 224 pixels) to retain the original aspect-ratio when the image is sliced to smaller parts. The padding value was equal to 255, which created a white border around most of the edges. Then, the white padding was used as filtered locations of postprocessed images (i.e., images were background and openings have been replaced with white) and would be instantly disregarded from the CNN output. This provided a total of 2814 image-slices which were used as training and validation (i.e., approx. 25% of them were used for validation), see (Fig. 3). Other slice-resolutions were also tested. The smaller resolutions provided more accurate models. This was due to the increased number of training and validation data.</p>
        <p>In this research, the networks evaluated were the U-Net, 
            <rs type="software">Deep-LabV3+</rs> (Fig. 4), U-Net (SM), LinkNet (SM), and FPN (SM). The latter three (i.e., the SM's) were generated through the python package called "Segmentation Models", which includes: ready-to-use semantic-segmentation models, multiple backbones of renown architectures, and pretrained models for transfer learning. The training procedure involved only the use of the brick class since the dataset of other classes was not considered to be large enough.
        </p>
        <p>Multiple tests were conducted to identify the best combination of backbone, pretrained dataset, loss function, optimiser, and parameters (see Table 1), that would provide the most efficient model. The first test included a combination of different parameters except loss, optimiser, and activation function (Table 1). Every architecture was tested with the "Adam" optimiser and "Weighted-Cross-Entropy" loss function. The learning-rate of the first test-sequence was equal to 5E-4 with a decay of 5E-6 over 100 epochs.Multiple tests were conducted to identify the best combination of backbone, pretrained dataset, loss function, optimiser, and parameters (see Table 1), that would provide the most efficient model. The first test included a combination of different parameters except loss, optimiser, and activation function (Table 1). Every architecture was tested with the "Adam" optimiser and "Weighted-Cross-Entropy" loss function. The learning-rate of the first test-sequence was equal to 5E-4 with a decay of 5E-6 over 100 epochs.</p>
        <p>All architectures provided similar results (between 95.98% to 96.27% validation-accuracy), with 
            <rs type="software">DeepLabV3+</rs> (#3) having the highest accuracy (96.27%). For each architecture, the parameters used for the optimal model were: a) U-Net (#6): Optimized using 64 output filters, 0.0005 L2-Regularization, 0.25 dropout, Batch-Normalization, and "glorot-uniform" initializer. f) 
            <rs type="software">DeepLabV3+</rs> (#3): 16 OS (feature-extractor output ratio), "Xception" backbone, "Pascal-Voc" pretrained weights, and "Sigmoid" activation.
        </p>
        <p>The loss function was used to minimise the error during training and define the weights to reduce the loss during the next evaluation. Multiple loss-functions were tested to identify the most optimal for the current use-case. The loss functions tested were Focal-Loss (FL), Weighted-Cross-Entropy (WCE), F1-Loss (F1L), and Binary-Cross-Entropy (BCE). All cases used the "Adam" optimizer with learning-rate of 1E-4 and decay equal to 1E-6. Table 2 presents the best of three of each architecture/loss combinations. After evaluating all cases, the most efficient loss-function (highest validation-accuracy) was the BCE. However, it has been noticed that the highest validation-precision was typically acquired when using FL and the highest validation-recall when using WCE. Nonetheless, the target metric was the validation-accuracy. Thus, the optimal loss function was taken equal to the BCE. Furthermore, to exclude any error in the evaluation procedure of the loss-functions, a combination of different optimisers per loss-function was tested. However, the evaluation of the optimiser was only undertaken for the 
            <rs type="software">DeepLabV</rs>3+ architecture since it had the highest validation-accuracy for both tests. The remaining parameters were equal between the second and third tests. The two optimisers used herein were: a) Adam, Stochastic Gradient Descent (SGD); and b) RMSprop (RMSP).
        </p>
        <p>In Table 3 each optimiser provided the most efficient model with different loss function. So, the use of BCE as the optimal loss function was not universal. Using the SGD optimiser, the most efficient loss function was WCE (86.14% validation accuracy). With RMSP, the optimal loss function was F1L (96.72% validation accuracy). Using 
            <rs type="software">Adam</rs>, the highest score was obtained through BCE (96.65% validation accuracy). Also, from Table 3, it is concluded that the combinations (Optimiser/Loss) with the highest accuracy were the Adam-BCE and RMSP-F1L and had very similar accuracy. So, both have been considered for the development of the final model. In contrast, the SGD optimiser was disregarded due to the low accuracy score along all loss functions.
        </p>
        <p>The optimal learning-rate used to adjust the final model and decide on the utilisation of F1L and BCE loss functions. RMSP was selected as the target optimiser, since it obtained the highest score, see Table 3. Different learning-rate values were tested over 200 epochs. The decay used was equal to the Learning-Rate over the Max-Epoch. All models used the DLV3+ architecture with the Xception backbone, pretrained to the Pascal-VOC dataset, see Table 4.The optimal learning-rate used to adjust the final model and decide on the utilisation of F1L and BCE loss functions. RMSP was selected as the target optimiser, since it obtained the highest score, see Table 3. Different learning-rate values were tested over 200 epochs. The decay used was equal to the Learning-Rate over the Max-Epoch. All models used the DLV3+ architecture with the Xception backbone, pretrained to the Pascal-VOC dataset, see Table 4.</p>
        <p>The highest score using F1L loss function was obtained with 2E-4 learning rate with validation accuracy equal to 96.86% (Table 4). The highest score using BCE loss function was obtained with 1E-4 learning rate with validation accuracy equal to 96.87% (Table 4). The validation accuracy of both models was very similar. Thus, the selection of the final model considered the progression of the loss on the accuracy/loss graphs (Fig. 5) and the visual representation of the validation set (Fig. 6 and Fig. 7).The highest score using F1L loss function was obtained with 2E-4 learning rate with validation accuracy equal to 96.86% (Table 4). The highest score using BCE loss function was obtained with 1E-4 learning rate with validation accuracy equal to 96.87% (Table 4). The validation accuracy of both models was very similar. Thus, the selection of the final model considered the progression of the loss on the accuracy/loss graphs (Fig. 5) and the visual representation of the validation set (Fig. 6 and Fig. 7).</p>
        <p>The output graph of the BCE model shows moderate overfitting to the dataset provided (Increasing validation-accuracy and validation-loss), which reveals that the BCE model may not be generalising as well as the model with F1L. Moreover, from the samples provided, the model with F1L has reduced noise on complex locations (i.e., images 1-3 in Fig. 6 and Fig. 7). Although using BCE the validation score was slightly higher in the model, the reduction of noise assisted with the detection of individual blocks on more complex images. Also, both models provided very accurate results for images with adequate resolution per block. Moreover, both were able to recognise openings and backgrounds exceptionally well, even for bricks with varied colour (i.e., images 4 and 8 in Fig. 6 and Fig. 7). So, the model with F1L loss function was considered as the best model and adopted here. In more detail, the specified model has a classification error equal to 1.24% for the background and 1.52% for the blocks class (Fig. 8). Additionally, the model is aimed to be used for images that are simpler than the trained dataset. Thus, in practice, the CNN-output is expected to provide improved results.The output graph of the BCE model shows moderate overfitting to the dataset provided (Increasing validation-accuracy and validation-loss), which reveals that the BCE model may not be generalising as well as the model with F1L. Moreover, from the samples provided, the model with F1L has reduced noise on complex locations (i.e., images 1-3 in Fig. 6 and Fig. 7). Although using BCE the validation score was slightly higher in the model, the reduction of noise assisted with the detection of individual blocks on more complex images. Also, both models provided very accurate results for images with adequate resolution per block. Moreover, both were able to recognise openings and backgrounds exceptionally well, even for bricks with varied colour (i.e., images 4 and 8 in Fig. 6 and Fig. 7). So, the model with F1L loss function was considered as the best model and adopted here. In more detail, the specified model has a classification error equal to 1.24% for the background and 1.52% for the blocks class (Fig. 8). Additionally, the model is aimed to be used for images that are simpler than the trained dataset. Thus, in practice, the CNN-output is expected to provide improved results.</p>
        <p>As mentioned before, the crack detection algorithm adopted in this study used the most efficient model presented in Dais et al. [16]. Both patch classification and pixel wise segmentation was tested. However, for the purposes of this study, only the models for pixel wise segmentation were considered (Table 5). The architectures tested were: a)As mentioned before, the crack detection algorithm adopted in this study used the most efficient model presented in Dais et al. [16]. Both patch classification and pixel wise segmentation was tested. However, for the purposes of this study, only the models for pixel wise segmentation were considered (Table 5). The architectures tested were: a)</p>
        <p>To acquire the final output, image processing of the original image was undertaken. Initially the image was re-sized using the same methodology described in the Development of the database section (Eq. ( 5)). By combining the image slices directly to the image, distortion effects near the edges of the image-slice (Fig. 10: b) were observed. So, each slice assigned an overlap value. The best results were acquired using an overlap value of 50 pixels for an image slice of 224 × 224. The image was divided into sections of 124 × 124 pixels (224 -2 × 50) and included a white padding of 100 pixels (2 × 50). This effectively retained only the central section of each slice for use and improved the overall quality of the final output (Fig. 10: c). Furthermore, the use of the models to acquire the location of cracks and masonry elements shown satisfactory results. Fig. 11 presents outputs from both models CNN (blocks and cracks) that were used for verification purposes i.e., not used during the training/evaluation phase.To acquire the final output, image processing of the original image was undertaken. Initially the image was re-sized using the same methodology described in the Development of the database section (Eq. ( 5)). By combining the image slices directly to the image, distortion effects near the edges of the image-slice (Fig. 10: b) were observed. So, each slice assigned an overlap value. The best results were acquired using an overlap value of 50 pixels for an image slice of 224 × 224. The image was divided into sections of 124 × 124 pixels (224 -2 × 50) and included a white padding of 100 pixels (2 × 50). This effectively retained only the central section of each slice for use and improved the overall quality of the final output (Fig. 10: c). Furthermore, the use of the models to acquire the location of cracks and masonry elements shown satisfactory results. Fig. 11 presents outputs from both models CNN (blocks and cracks) that were used for verification purposes i.e., not used during the training/evaluation phase.</p>
        <p>One important use of the damage detection model proposed here is to assist engineers with the inspection and documentation of masonry structures in their care. Using image processing, each individual crack was identified and measured. The isolation of white elements from the CNN output was succeeded by using watershed segmentation to assign a unique label to each crack (Fig. 12: b). Using the individual labels of the segmentation, it was possible to acquire the area of each label by counting the total number of pixels. Each segmentation provided the linearization of its area (Fig. 12: c), which can be used to evaluate the length of the crack. Finally, the results obtained can be scaled to the real dimensions and provide realistic measurements of the crack properties by providing a scale factor (Table 6). The approximate-length (Skeleton (mm)) was acquired under the assumption that the length of each pixel is the average between its horizontal and diagonal distance.One important use of the damage detection model proposed here is to assist engineers with the inspection and documentation of masonry structures in their care. Using image processing, each individual crack was identified and measured. The isolation of white elements from the CNN output was succeeded by using watershed segmentation to assign a unique label to each crack (Fig. 12: b). Using the individual labels of the segmentation, it was possible to acquire the area of each label by counting the total number of pixels. Each segmentation provided the linearization of its area (Fig. 12: c), which can be used to evaluate the length of the crack. Finally, the results obtained can be scaled to the real dimensions and provide realistic measurements of the crack properties by providing a scale factor (Table 6). The approximate-length (Skeleton (mm)) was acquired under the assumption that the length of each pixel is the average between its horizontal and diagonal distance.</p>
        <p>The main use of the feature detection was aimed for the automatic development of geometrical models for documentation and numerical models for analysing the structural capacity of masonry structures. The methodology to convert binary images of masonry blocks and cracks is further explained in Loverdos et al. [32].The main use of the feature detection was aimed for the automatic development of geometrical models for documentation and numerical models for analysing the structural capacity of masonry structures. The methodology to convert binary images of masonry blocks and cracks is further explained in Loverdos et al. [32].</p>
        <p>The algorithms described in the previous study used binary images acquired using simple photogrammetric applications (i.e., image blurring, image thresholding, edge detection). However, the use of simple image processing applications found to be not reliable and, in some cases, unusable (i.e., for large variations on illumination and/or colour (Fig. 13: c, e, andg)). Furthermore, the process requires to adjust the parameters of each image-processing function manually. The use of CNN, for the feature detection of masonry micro-geometry (i.e., geometry of individual masonry units and mortar), improves the results of the feature extraction by providing a better binarized output and automating the procedure (Fig. 13: d, f, andh).The algorithms described in the previous study used binary images acquired using simple photogrammetric applications (i.e., image blurring, image thresholding, edge detection). However, the use of simple image processing applications found to be not reliable and, in some cases, unusable (i.e., for large variations on illumination and/or colour (Fig. 13: c, e, andg)). Furthermore, the process requires to adjust the parameters of each image-processing function manually. The use of CNN, for the feature detection of masonry micro-geometry (i.e., geometry of individual masonry units and mortar), improves the results of the feature extraction by providing a better binarized output and automating the procedure (Fig. 13: d, f, andh).</p>
        <p>For both applications (measurement of cracks and generation of geometrical model) the image is required to be either an orthorectified photo or an image captured vertically compared to the masonry element. This will ensure that the detected elements (i.e., blocks, cracks, openings) will have the same scale along the image used.For both applications (measurement of cracks and generation of geometrical model) the image is required to be either an orthorectified photo or an image captured vertically compared to the masonry element. This will ensure that the detected elements (i.e., blocks, cracks, openings) will have the same scale along the image used.</p>
        <p>Moreover, the simplified shapes acquired, using the binarized output from the trained models, demonstrated great improvement when compared to binarized images acquired using image-processing. The blocks were more evenly shaped and better aligned to the actual masonry bricks (Fig. 14). This did not only improve the reliability of the numerical analysis, but also the geometric representation of the structure when used for representation in CAD environments. It should be noted that the original image used in Fig. 13 and Fig. 14 was the same used for the generation of the numerical model in the previous study for comparison purposes. Additionally, the use of simple image-processing applications, for the feature detection, favours the specified image since the contrast between mortar and bricks is highly visible, without large changes to illumination/colour. For general use, the difference between the resultant output is expected to be larger.Moreover, the simplified shapes acquired, using the binarized output from the trained models, demonstrated great improvement when compared to binarized images acquired using image-processing. The blocks were more evenly shaped and better aligned to the actual masonry bricks (Fig. 14). This did not only improve the reliability of the numerical analysis, but also the geometric representation of the structure when used for representation in CAD environments. It should be noted that the original image used in Fig. 13 and Fig. 14 was the same used for the generation of the numerical model in the previous study for comparison purposes. Additionally, the use of simple image-processing applications, for the feature detection, favours the specified image since the contrast between mortar and bricks is highly visible, without large changes to illumination/colour. For general use, the difference between the resultant output is expected to be larger.</p>
        <p>The quality of the segmentation was evaluated to quantify the change between the ground truth and the output from either the CNN model or simple image-processing applications. The simple metrics included the accuracy, recall, precision, and F1-Score, as seen in the use of the CNN model.The quality of the segmentation was evaluated to quantify the change between the ground truth and the output from either the CNN model or simple image-processing applications. The simple metrics included the accuracy, recall, precision, and F1-Score, as seen in the use of the CNN model.</p>
        <p>However, those metrics tend to check the overall quality of the output. Since the model was aimed to be used for the generation of geometrical models and documentation, additional metrics were included to quantify the quality of the block-shapes. The shape quality was estimated by calculating the coverage, error of area, and quantity of undefined blocks (Fig. 15). Each segmentation of the ground-truth was compared with the output of either CNN or image-processing to identify the same object in both images. The first step was to calculate the common area between the two objects (Eq. ( 7)). The coverage (Eq. ( 8)) was calculated by comparing the common-area between both objects (ground-truth and output) while the Area-Error was calculated by comparing the area between both objects (Eq. ( 9)). The quantity of undefined objects was the number of objects that didn't match with a segmentation from the ground truth following certain conditions (i.e., the coverage must be similar to the area of the segmentation). The equations of the additional metrics are provided below:However, those metrics tend to check the overall quality of the output. Since the model was aimed to be used for the generation of geometrical models and documentation, additional metrics were included to quantify the quality of the block-shapes. The shape quality was estimated by calculating the coverage, error of area, and quantity of undefined blocks (Fig. 15). Each segmentation of the ground-truth was compared with the output of either CNN or image-processing to identify the same object in both images. The first step was to calculate the common area between the two objects (Eq. ( 7)). The coverage (Eq. ( 8)) was calculated by comparing the common-area between both objects (ground-truth and output) while the Area-Error was calculated by comparing the area between both objects (Eq. ( 9)). The quantity of undefined objects was the number of objects that didn't match with a segmentation from the ground truth following certain conditions (i.e., the coverage must be similar to the area of the segmentation). The equations of the additional metrics are provided below:</p>
        <p>Missing Error = Missing/AllObjects (10) where Object1 i denotes any segmentation on ground truth, Object2 i any segmentation on the output (CNN or Image-processing), Area1 the total area of the Object1 in pixels, and Area2 the total area of Object2 in pixels. Individual segmentations were obtained using watershed-segmentation with the binary image as mask. The conditions for segmentation, i.e., the same object as in the ground-truth, were:Missing Error = Missing/AllObjects (10) where Object1 i denotes any segmentation on ground truth, Object2 i any segmentation on the output (CNN or Image-processing), Area1 the total area of the Object1 in pixels, and Area2 the total area of Object2 in pixels. Individual segmentations were obtained using watershed-segmentation with the binary image as mask. The conditions for segmentation, i.e., the same object as in the ground-truth, were:</p>
        <p>where the Threshold is any value between 0 and 1 and denotes the difference between the common area and the total area of the segmentation. The shape-analysis in this case used a Threshold value of 0.2 (i.e., 80% of object area must be common). The first condition was used to verify that two objects have a common area (Eq. ( 11)). By using the second condition only (Eq. ( 12)), the evaluation considered objects that were erroneously merged (Fig. 15c), which increased the area-error significantly. By using the third condition only (Eq. ( 13)), the evaluation considered segmentations that were erroneously broken into smaller elements. If multiple objects satisfy the conditions, then they were all included in the final common area. For this study, both optional conditions were used. Thus, for an object to be considered, must have a common area of at least 80% of both segmentations (ground-truth and output). Fig. 15 shows the undefined objects that did not match both images (Fig. 15: a-b &amp; Fig. 15: a-c). Furthermore, it demonstrates that the CNN output has marginally fewer undefined objects, since imageprocessing is prone to noise caused by the change in illumination and colour within the same image. Although watershed-segmentation can close open-segmentations (Fig. 13: g), it is not always feasible. In Fig. 16, images were utilised during the validation phase of the model and are shown to compare the output acquired using image-processing. The method applied was the same as the one used to acquire the image in Fig. 13:c. The metrics for the complete evaluation of all images (Fig. 15 &amp; Fig. 16), are shown in Table 7.where the Threshold is any value between 0 and 1 and denotes the difference between the common area and the total area of the segmentation. The shape-analysis in this case used a Threshold value of 0.2 (i.e., 80% of object area must be common). The first condition was used to verify that two objects have a common area (Eq. ( 11)). By using the second condition only (Eq. ( 12)), the evaluation considered objects that were erroneously merged (Fig. 15c), which increased the area-error significantly. By using the third condition only (Eq. ( 13)), the evaluation considered segmentations that were erroneously broken into smaller elements. If multiple objects satisfy the conditions, then they were all included in the final common area. For this study, both optional conditions were used. Thus, for an object to be considered, must have a common area of at least 80% of both segmentations (ground-truth and output). Fig. 15 shows the undefined objects that did not match both images (Fig. 15: a-b &amp; Fig. 15: a-c). Furthermore, it demonstrates that the CNN output has marginally fewer undefined objects, since imageprocessing is prone to noise caused by the change in illumination and colour within the same image. Although watershed-segmentation can close open-segmentations (Fig. 13: g), it is not always feasible. In Fig. 16, images were utilised during the validation phase of the model and are shown to compare the output acquired using image-processing. The method applied was the same as the one used to acquire the image in Fig. 13:c. The metrics for the complete evaluation of all images (Fig. 15 &amp; Fig. 16), are shown in Table 7.</p>
        <p>Most metrics provide similar values for both test cases of the damaged wall (Fig. 15). More specifically the accuracy on the CNN output was slightly higher, which explains the better representation of the segmentation (95.7% vs 94.8%). Although, the coverage in the CNN image was slightly lower for the objects that were detected correctly (94.4% vs 96.7%). Nonetheless, the missing error of the CNN image was much lower than the thresholding case (5.5% vs 27.5%), which was caused by the presence of multiple open shapes in the thresholding image (Fig. 15: c). On the CNN case, the bottom-left block was undefined due to the damage not separating the blocks completely (Fig. 15: b), as it is on the ground truth image (Fig. 15: a) and detecting the three broken elements as a single object. Also, it should be noted that the large value of the undefined blocks, in the thresholding case, would decrease the coverage and increase the area error, if they were allowed in the evaluation (Fig. 15: c). Thus, a higher coverage does not correspond to an overall better quality of segmentation, since it relates to fewer elements. Furthermore, the image was favourable for block detection using thresholding, due to minimal noise, which explains the better fit of the validated objects. The results will vary depending on alterations to illumination and colour (Fig. 16).Most metrics provide similar values for both test cases of the damaged wall (Fig. 15). More specifically the accuracy on the CNN output was slightly higher, which explains the better representation of the segmentation (95.7% vs 94.8%). Although, the coverage in the CNN image was slightly lower for the objects that were detected correctly (94.4% vs 96.7%). Nonetheless, the missing error of the CNN image was much lower than the thresholding case (5.5% vs 27.5%), which was caused by the presence of multiple open shapes in the thresholding image (Fig. 15: c). On the CNN case, the bottom-left block was undefined due to the damage not separating the blocks completely (Fig. 15: b), as it is on the ground truth image (Fig. 15: a) and detecting the three broken elements as a single object. Also, it should be noted that the large value of the undefined blocks, in the thresholding case, would decrease the coverage and increase the area error, if they were allowed in the evaluation (Fig. 15: c). Thus, a higher coverage does not correspond to an overall better quality of segmentation, since it relates to fewer elements. Furthermore, the image was favourable for block detection using thresholding, due to minimal noise, which explains the better fit of the validated objects. The results will vary depending on alterations to illumination and colour (Fig. 16).</p>
        <p>The metrics acquired for Fig. 16 demonstrate that the quality of shapes when using simple thresholding was detrimental for the accuracy of the model. This can be observed initially from the median accuracy, which was equal to 94.5% vs 79.3%, for the CNN and thresholding methods respectively. In general, thresholding provided marginally fewer validated blocks, as it was observed by the median missing error, which was equal to 22.9% vs 69.1%, for the CNN and thresholding methods respectively (Fig. 16: c,d). Furthermore, the overall fitting of the shapes was higher in the CNN case since the coverage calculated was 96.7% vs 94.6% for the CNN and thresholding methods respectively.The metrics acquired for Fig. 16 demonstrate that the quality of shapes when using simple thresholding was detrimental for the accuracy of the model. This can be observed initially from the median accuracy, which was equal to 94.5% vs 79.3%, for the CNN and thresholding methods respectively. In general, thresholding provided marginally fewer validated blocks, as it was observed by the median missing error, which was equal to 22.9% vs 69.1%, for the CNN and thresholding methods respectively (Fig. 16: c,d). Furthermore, the overall fitting of the shapes was higher in the CNN case since the coverage calculated was 96.7% vs 94.6% for the CNN and thresholding methods respectively.</p>
        <p>Moreover, simple image-processing methods can not recognise openings, damage, or background (Fig. 16: d4 &amp; Fig. 16: d8). They are only able to identify either edges (edge detection) or pixel intensity (thresholding). Every image that contained locations of background, required modification before it was used. Thus, the use of CNN for the object detection was preferred for the current test-case, due to its higher accuracy and reliability to identify correctly almost every object (Fig. 15: b), except for highly complex images (Fig. 16: c1, c2, c3).Moreover, simple image-processing methods can not recognise openings, damage, or background (Fig. 16: d4 &amp; Fig. 16: d8). They are only able to identify either edges (edge detection) or pixel intensity (thresholding). Every image that contained locations of background, required modification before it was used. Thus, the use of CNN for the object detection was preferred for the current test-case, due to its higher accuracy and reliability to identify correctly almost every object (Fig. 15: b), except for highly complex images (Fig. 16: c1, c2, c3).</p>
        <p>This research is contributing towards automating procedures that engineers would require considerable amounts of effort, expertise, and time to perform documentation and structural inspection of masonry fabric, while at the same time minimises the human error. Also, the proposed approach is suitable for cases in which visual inspection is in locations difficult to reach by humans. The study demonstrates that both crack and block (i.e. masonry unit) detection can be achieved with adequately high accuracy by utilising deep learning approaches (i.e. block-detection model achieved a validation-accuracy of 96.86% and the crack detection model an F1-Score of 79.6%). The quality of the binarized output has also been assessed showing that the CNN output outperforms simple image-processing functions even for clean images. Especially considering that simple image-processing applications do not differentiate between detected elements and background/openings. Additionally, deep learning methods allow for the improvement of the model by increasing the dataset used for training and validation. Consequently, the performance of the model can always be enhanced by acquiring additional samples of the classified elements.This research is contributing towards automating procedures that engineers would require considerable amounts of effort, expertise, and time to perform documentation and structural inspection of masonry fabric, while at the same time minimises the human error. Also, the proposed approach is suitable for cases in which visual inspection is in locations difficult to reach by humans. The study demonstrates that both crack and block (i.e. masonry unit) detection can be achieved with adequately high accuracy by utilising deep learning approaches (i.e. block-detection model achieved a validation-accuracy of 96.86% and the crack detection model an F1-Score of 79.6%). The quality of the binarized output has also been assessed showing that the CNN output outperforms simple image-processing functions even for clean images. Especially considering that simple image-processing applications do not differentiate between detected elements and background/openings. Additionally, deep learning methods allow for the improvement of the model by increasing the dataset used for training and validation. Consequently, the performance of the model can always be enhanced by acquiring additional samples of the classified elements.</p>
        <p>The main limitation of the demonstrated application of deep learning, for the detection of features in masonry structures, is that a similar sample should be provided during training of the model to detect specific features. i.e., to be able to reliably identify irregular masonry units, images with irregular masonry should be included in the dataset. Furthermore, features not shown in the image will not be identified by the model (i.e., cracks of extremely small size). Thus, the engineer must ensure that the desired features should be visible on the image-slice passed through the network. Also, the use orthorectified images is important for the accurate evaluation of detected features (i.e., if used for numerical modelling, or crack measurements). Future work includes the implementation of the developed models to a framework that will automatically generate numerical models and analyse changes on the structure in real time. This includes a detailed report of the measurement of detected defects on the structure in a form of a geometrical digital twin. Moreover, currently only cracks are detected but additional classifications of defects are considered, such as spalling, vegetation, and discolouration. Additionally, the results obtained from the block/crack-detection can be coupled directly with algorithms for numerical modelling to automatically evaluate the crack patterns on the structure by performing numerical analysis or compare the results from inverse analysis by matching the outputs (from the block/crack detection models and evaluation method used, i.e., [1,3,22,35,45]). In all cases, the capture procedure can be replaced by remote sensing applications such as drones to remotely capture image/ video data paired with semantic segmentation for the identification of structural elements; hence, providing a digital twin of the structure considered for real time monitoring.The main limitation of the demonstrated application of deep learning, for the detection of features in masonry structures, is that a similar sample should be provided during training of the model to detect specific features. i.e., to be able to reliably identify irregular masonry units, images with irregular masonry should be included in the dataset. Furthermore, features not shown in the image will not be identified by the model (i.e., cracks of extremely small size). Thus, the engineer must ensure that the desired features should be visible on the image-slice passed through the network. Also, the use orthorectified images is important for the accurate evaluation of detected features (i.e., if used for numerical modelling, or crack measurements). Future work includes the implementation of the developed models to a framework that will automatically generate numerical models and analyse changes on the structure in real time. This includes a detailed report of the measurement of detected defects on the structure in a form of a geometrical digital twin. Moreover, currently only cracks are detected but additional classifications of defects are considered, such as spalling, vegetation, and discolouration. Additionally, the results obtained from the block/crack-detection can be coupled directly with algorithms for numerical modelling to automatically evaluate the crack patterns on the structure by performing numerical analysis or compare the results from inverse analysis by matching the outputs (from the block/crack detection models and evaluation method used, i.e., [1,3,22,35,45]). In all cases, the capture procedure can be replaced by remote sensing applications such as drones to remotely capture image/ video data paired with semantic segmentation for the identification of structural elements; hence, providing a digital twin of the structure considered for real time monitoring.</p>
        <p>Several photos obtained by engineers from Network Rail and Helifix, UK, and were kindly offered to expand our masonry dataset. Therefore, their support in this study is highly recognised and appreciated. Ihsan Bal, Eleni Smyrou and Dimitris Dais are acknowledged for their insightful comments on the implementation of the deep learning network for the crack detection algorithm (github.com/dimitrisdai s/crack_detection_CNN_masonry). This work was funded by the EPSRC project "Exploiting the resilience of masonry arch bridge infrastructure: a 3D multi-level modelling framework" (ref. EP/T001348/1). The financial contribution is very much appreciated.Several photos obtained by engineers from Network Rail and Helifix, UK, and were kindly offered to expand our masonry dataset. Therefore, their support in this study is highly recognised and appreciated. Ihsan Bal, Eleni Smyrou and Dimitris Dais are acknowledged for their insightful comments on the implementation of the deep learning network for the crack detection algorithm (github.com/dimitrisdai s/crack_detection_CNN_masonry). This work was funded by the EPSRC project "Exploiting the resilience of masonry arch bridge infrastructure: a 3D multi-level modelling framework" (ref. EP/T001348/1). The financial contribution is very much appreciated.</p>
        <p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
    </text>
</tei>
