<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-13T16:05+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Zero-shot 3D shape understanding aims to recognize ''unseen'' 3D categories that are not present in training data. Recently, Contrastive Language-Image Pre-training (CLIP) has shown promising open-world performance in zero-shot 3D shape understanding tasks by information fusion among language and 3D modality. It first renders 3D objects into multiple 2D image views and then learns to understand the semantic relationships between the textual descriptions and images, enabling the model to generalize to new and unseen categories. However, existing studies in zero-shot 3D shape understanding rely on predefined rendering parameters, resulting in repetitive, redundant, and low-quality views. This limitation hinders the model's ability to fully comprehend 3D shapes and adversely impacts the text-image fusion in a shared latent space. To this end, we propose a novel approach called Differentiable rendering-based multi-view Image-Language Fusion (DILF) for zero-shot 3D shape understanding. Specifically, DILF leverages large-scale language models (LLMs) to generate textual prompts enriched with 3D semantics and designs a differentiable renderer with learnable rendering parameters to produce representative multi-view images. These rendering parameters can be iteratively updated using a text-image fusion loss, which aids in parameters' regression, allowing the model to determine the optimal viewpoint positions for each 3D object. Then a group-view mechanism is introduced to model interdependencies across views, enabling efficient information fusion to achieve a more comprehensive 3D shape understanding. Experimental results can demonstrate that DILF outperforms state-of-the-art methods for zero-shot 3D classification while maintaining competitive performance for standard 3D classification. The 
            <rs type="software">code</rs> is available at 
            <rs type="url">https://github.com/yuzaiyang123/DILP</rs>.
        </p>
        <p>Three-dimensional (3D) shape understanding is a critical task in the field of computer vision and pattern recognition. It involves analyzing the geometric structure and spatial relationships of 3D data, which can be represented as point clouds, meshes, or volumetric data (voxels). The objective of this task is to classify or categorize 3D objects based on their shapes, distinguishing them by distinct features and characteristics [1]. Applications of 3D shape understanding are diverse and widespread across various fields, such as autonomous driving, robotics, virtual reality, and environmental monitoring.Three-dimensional (3D) shape understanding is a critical task in the field of computer vision and pattern recognition. It involves analyzing the geometric structure and spatial relationships of 3D data, which can be represented as point clouds, meshes, or volumetric data (voxels). The objective of this task is to classify or categorize 3D objects based on their shapes, distinguishing them by distinct features and characteristics [1]. Applications of 3D shape understanding are diverse and widespread across various fields, such as autonomous driving, robotics, virtual reality, and environmental monitoring.</p>
        <p>3D shape understanding encompasses both standard 3D shape understanding and zero-shot 3D shape understanding. For standard 3D understanding, there are two main strategies: the point-based strategy In contrast, DILF can introduce a differentiable rendering module (Right) that predicts optimal viewpoints in a data-driven manner. Therefore, this differentiable rendering process allows DILF to generate multi-view images that are more semantically representative.3D shape understanding encompasses both standard 3D shape understanding and zero-shot 3D shape understanding. For standard 3D understanding, there are two main strategies: the point-based strategy In contrast, DILF can introduce a differentiable rendering module (Right) that predicts optimal viewpoints in a data-driven manner. Therefore, this differentiable rendering process allows DILF to generate multi-view images that are more semantically representative.</p>
        <p>to unseen categories. Inspired by this, incorporating knowledge from other modalities, such as language, can be a powerful and straightforward approach to overcoming the challenges in zero-shot 3D shape classification [6]. By fusing the information from multiple modalities, the model can gain a more comprehensive understanding of 3D objects and improve its ability to classify unseen categories effectively.to unseen categories. Inspired by this, incorporating knowledge from other modalities, such as language, can be a powerful and straightforward approach to overcoming the challenges in zero-shot 3D shape classification [6]. By fusing the information from multiple modalities, the model can gain a more comprehensive understanding of 3D objects and improve its ability to classify unseen categories effectively.</p>
        <p>The language modality can provide valuable complementary information to the 3D modality [7]. By using text prompts to describe 3D scenes from a macroscopic perspective [8], the language modality can help the network achieve a more comprehensive understanding of unseen 3D categories in zero-shot learning scenarios [9]. Recent research has shown the potential of integrating language information into zero-shot learning through an ''information fusion'' strategy, where both the language and 3D modalities are combined to enhance the overall interpretation of the data. OpenAI's Contrastive Learning In Pretraining (CLIP) is a notable example [10], which bridges the domain gap between vision and language modalities by utilizing pre-trained text and image correlations to enable open-vocabulary recognition between the two modalities. A few extensions of CLIP have been proposed to handle the challenges in zero-shot 3D classification. For instance, PointCLIP [11] designs an inter-view adapter that utilizes text prompts to enhance the understanding of 3D multi-view images. Another method, ULIP [12] learns a unified representation of 2D RGB images, texts, and 3D point clouds by pre-training with object triplets from the three modalities. The previous methods demonstrate that the language modality can supplement extra information for unseen 3D categories, enhancing the potential of integrating the CLIP model in handling zero-shot 3D classification.The language modality can provide valuable complementary information to the 3D modality [7]. By using text prompts to describe 3D scenes from a macroscopic perspective [8], the language modality can help the network achieve a more comprehensive understanding of unseen 3D categories in zero-shot learning scenarios [9]. Recent research has shown the potential of integrating language information into zero-shot learning through an ''information fusion'' strategy, where both the language and 3D modalities are combined to enhance the overall interpretation of the data. OpenAI's Contrastive Learning In Pretraining (CLIP) is a notable example [10], which bridges the domain gap between vision and language modalities by utilizing pre-trained text and image correlations to enable open-vocabulary recognition between the two modalities. A few extensions of CLIP have been proposed to handle the challenges in zero-shot 3D classification. For instance, PointCLIP [11] designs an inter-view adapter that utilizes text prompts to enhance the understanding of 3D multi-view images. Another method, ULIP [12] learns a unified representation of 2D RGB images, texts, and 3D point clouds by pre-training with object triplets from the three modalities. The previous methods demonstrate that the language modality can supplement extra information for unseen 3D categories, enhancing the potential of integrating the CLIP model in handling zero-shot 3D classification.</p>
        <p>While applying CLIP to zero-shot 3D classification tasks shows promise, its success relies on the quality of rendered multi-view images since it can influence the effectiveness of the text-image fusion process. For effective zero-shot 3D classification, the model needs to understand and recognize 3D shapes from different viewpoints without direct training on these specific views. This requires having accurate and informative multi-view representations of the 3D objects. However, the exploration of a suitable rendering configuration for rendering optimal multi-view images has been lacking in previous works [6,[11][12][13]. As shown in Fig. 1, predefined rendering configurations may not adequately capture the variations and complexities present in the 3D shapes, leading to the limited coverage of viewpoints and potentially missing important details. On the other hand, most previous CLIP-based methods treat all views equally to generate the shape descriptors, which neglects the essential role that the content relationship and discriminative information among the views play in understanding the 3D modality. Additionally, they mainly adopt CLIP 2D prompt structure, e.g., ''a photo of a [CLASS]'', and append elementary domainspecific terms, such as ''3D object''. Nevertheless, these methods confront the challenge of Naive Textual Prompting (NTP), in which the simplistic textual prompt fails to represent 3D shapes adequately and adversely affects the pre-trained language-image fusion in the embedding space [13].While applying CLIP to zero-shot 3D classification tasks shows promise, its success relies on the quality of rendered multi-view images since it can influence the effectiveness of the text-image fusion process. For effective zero-shot 3D classification, the model needs to understand and recognize 3D shapes from different viewpoints without direct training on these specific views. This requires having accurate and informative multi-view representations of the 3D objects. However, the exploration of a suitable rendering configuration for rendering optimal multi-view images has been lacking in previous works [6,[11][12][13]. As shown in Fig. 1, predefined rendering configurations may not adequately capture the variations and complexities present in the 3D shapes, leading to the limited coverage of viewpoints and potentially missing important details. On the other hand, most previous CLIP-based methods treat all views equally to generate the shape descriptors, which neglects the essential role that the content relationship and discriminative information among the views play in understanding the 3D modality. Additionally, they mainly adopt CLIP 2D prompt structure, e.g., ''a photo of a [CLASS]'', and append elementary domainspecific terms, such as ''3D object''. Nevertheless, these methods confront the challenge of Naive Textual Prompting (NTP), in which the simplistic textual prompt fails to represent 3D shapes adequately and adversely affects the pre-trained language-image fusion in the embedding space [13].</p>
        <p>To address these limitations, we propose Differentiable renderingbased multi-view Image-Language Fusion (DILF), as shown in Fig. 2. DILF consists of three modules. (1) To produce informative multi-view images, DILF employs a differentiable renderer to iteratively update the rendering parameters, enabling end-to-end training of the zero-shot 3D classification task. DILF allows the language modality to supply additional information for the 3D modality, enabling the selection of appropriate rendering viewpoints under the guidance of the language prompts, as shown in Fig. 3(a). ( 2) To mine the content relationship and discriminative information among the multi-view images, a groupview mechanism is introduced into DILF for enhancing the network's ability to model complex interdependencies among these views, as shown in Fig. 3(b). (3) Motivated by automatic prompt designs [13], DILF proposes the large-scale language model LLM-assisted textual feature learning which leverages the descriptive capabilities of LLMs to generate 3D-specific prompts enriched with 3D semantics to overcome the NTP challenge, as shown in Fig. 2(a). The contributions of this study can be summarized as follows:To address these limitations, we propose Differentiable renderingbased multi-view Image-Language Fusion (DILF), as shown in Fig. 2. DILF consists of three modules. (1) To produce informative multi-view images, DILF employs a differentiable renderer to iteratively update the rendering parameters, enabling end-to-end training of the zero-shot 3D classification task. DILF allows the language modality to supply additional information for the 3D modality, enabling the selection of appropriate rendering viewpoints under the guidance of the language prompts, as shown in Fig. 3(a). ( 2) To mine the content relationship and discriminative information among the multi-view images, a groupview mechanism is introduced into DILF for enhancing the network's ability to model complex interdependencies among these views, as shown in Fig. 3(b). (3) Motivated by automatic prompt designs [13], DILF proposes the large-scale language model LLM-assisted textual feature learning which leverages the descriptive capabilities of LLMs to generate 3D-specific prompts enriched with 3D semantics to overcome the NTP challenge, as shown in Fig. 2(a). The contributions of this study can be summarized as follows:</p>
        <p>(1) We propose a novel approach called Differentiable renderingbased multi-view Image-Language Fusion (DILF) for zero-shot 3D shape understanding. DILF combines Differentiable Rendering (DR) with language prompts to generate multi-view images with more comprehensive information. It enables the iterative optimization of viewpoint selection by fusion among 3D and language modalities. This approach integrates the principles of DR, allowing the network to refine the rendering process under the explicit guidance of language descriptions. (2) For accurate text-image fusion, DILF introduces the group-view mechanism and LLM-assisted textual feature learning to bridge the domain gap. The group-view mechanism aims at modeling interdependencies across views, enabling a more comprehensive 3D shape understanding. The LLM-assisted textual feature learning utilizes the descriptive capabilities of LLMs, enabling a comprehensive interpretation of 3D objects. A 3D object includes its 3D and language modality, the group-view mechanism and LLM-assisted textual feature learning aim at making a complete interpretation of the 3D object at different levels and therefore facilitating accurate text-image fusion. performance in comparison to state-of-the-art (SOTA) for zeroshot 3D classification while maintaining competitive performance for standard 3D classification. The superior results validate the effectiveness of DILF for achieving accurate and comprehensive 3D shape understanding.(1) We propose a novel approach called Differentiable renderingbased multi-view Image-Language Fusion (DILF) for zero-shot 3D shape understanding. DILF combines Differentiable Rendering (DR) with language prompts to generate multi-view images with more comprehensive information. It enables the iterative optimization of viewpoint selection by fusion among 3D and language modalities. This approach integrates the principles of DR, allowing the network to refine the rendering process under the explicit guidance of language descriptions. (2) For accurate text-image fusion, DILF introduces the group-view mechanism and LLM-assisted textual feature learning to bridge the domain gap. The group-view mechanism aims at modeling interdependencies across views, enabling a more comprehensive 3D shape understanding. The LLM-assisted textual feature learning utilizes the descriptive capabilities of LLMs, enabling a comprehensive interpretation of 3D objects. A 3D object includes its 3D and language modality, the group-view mechanism and LLM-assisted textual feature learning aim at making a complete interpretation of the 3D object at different levels and therefore facilitating accurate text-image fusion. performance in comparison to state-of-the-art (SOTA) for zeroshot 3D classification while maintaining competitive performance for standard 3D classification. The superior results validate the effectiveness of DILF for achieving accurate and comprehensive 3D shape understanding.</p>
        <p>The remainder of this paper is organized as follows. Section 2 introduces related works. Section 3 presents the proposed DILF approach. Section 4 shows the related experiment results and analyses. Finally, we conclude the whole paper and direct our future research.The remainder of this paper is organized as follows. Section 2 introduces related works. Section 3 presents the proposed DILF approach. Section 4 shows the related experiment results and analyses. Finally, we conclude the whole paper and direct our future research.</p>
        <p>The research on 3D shape understanding can be broadly categorized into two main streams: point-based methods and view-based methods. The point-based methods involve defining the network directly on a point cloud or mesh representation of a 3D object. PointNet [14] and its variants [15][16][17][18][19] are examples of point-based networks used to learn representations from point clouds. These methods operate directly on the raw point data and have shown effectiveness in certain 3D tasks. The view-based methods, on the other hand, generate multiple views of a given 3D model by rendering the object from different viewpoints, resulting in multi-view images. These images are then used to solve downstream tasks such as 3D classification and segmentation. View-based methods have achieved state-of-the-art performance in these tasks [20]. Among these view-based researches, Wei et al. [2] claimed that view-based methods are effective at revealing the essential structure and contour changes of 3D objects. Su et al. [21] suggested that view-based methods align with how the human brain associates 2D appearances with prior knowledge of a 3D shape, which could contribute to their superior performance. Qi et al. [3] argued that view-based methods can complement each other's detailed features from multi-view images by setting different rendering viewpoints and thereby providing a complete interpretation of an occluded object.The research on 3D shape understanding can be broadly categorized into two main streams: point-based methods and view-based methods. The point-based methods involve defining the network directly on a point cloud or mesh representation of a 3D object. PointNet [14] and its variants [15][16][17][18][19] are examples of point-based networks used to learn representations from point clouds. These methods operate directly on the raw point data and have shown effectiveness in certain 3D tasks. The view-based methods, on the other hand, generate multiple views of a given 3D model by rendering the object from different viewpoints, resulting in multi-view images. These images are then used to solve downstream tasks such as 3D classification and segmentation. View-based methods have achieved state-of-the-art performance in these tasks [20]. Among these view-based researches, Wei et al. [2] claimed that view-based methods are effective at revealing the essential structure and contour changes of 3D objects. Su et al. [21] suggested that view-based methods align with how the human brain associates 2D appearances with prior knowledge of a 3D shape, which could contribute to their superior performance. Qi et al. [3] argued that view-based methods can complement each other's detailed features from multi-view images by setting different rendering viewpoints and thereby providing a complete interpretation of an occluded object.</p>
        <p>The 3D multi-view images can accurately capture the characteristics of non-rigid 3D objects, providing a robust depiction of their geometry and structure [3]. Consequently, DILF employs a view-based approach to extract representations from 3D models. The difference between DILF and prior 3D shape understanding methods lies in DILF's fusion of language modality knowledge, which offers an additional complement to 3D modality. This is attributed to the fact that multimodal learning facilitates the acquisition of cross-modality information about 3D objects, enabling a comprehensive interpretation of these objects [22]. Meanwhile, a few view-based methods [2,23] have explored the position of viewpoint for rendering optimal multi-view images. However, these previous methods rely on predefined configurations for rendering 3D objects, resulting in limited success and requiring an elaborate training process [20]. The difference between DILF and these previous methods is that DILF iteratively adjusts the rendering configuration by fusing explicit text guidance into the rendering process, thereby obtaining multi-view images that are more semantically representative in a data-driven manner.The 3D multi-view images can accurately capture the characteristics of non-rigid 3D objects, providing a robust depiction of their geometry and structure [3]. Consequently, DILF employs a view-based approach to extract representations from 3D models. The difference between DILF and prior 3D shape understanding methods lies in DILF's fusion of language modality knowledge, which offers an additional complement to 3D modality. This is attributed to the fact that multimodal learning facilitates the acquisition of cross-modality information about 3D objects, enabling a comprehensive interpretation of these objects [22]. Meanwhile, a few view-based methods [2,23] have explored the position of viewpoint for rendering optimal multi-view images. However, these previous methods rely on predefined configurations for rendering 3D objects, resulting in limited success and requiring an elaborate training process [20]. The difference between DILF and these previous methods is that DILF iteratively adjusts the rendering configuration by fusing explicit text guidance into the rendering process, thereby obtaining multi-view images that are more semantically representative in a data-driven manner.</p>
        <p>CLIP is a type of pretraining that uses contrastive learning to learn representations from large-scale image-text pairs [24]. CLIP bridges the gap between the language and image modalities, enabling it to capture rich semantic relationships between them [25]. The efficiency of the CLIP in understanding 3D scenes has been validated through various applications, such as 3D object generation [26][27][28], 3D recognition [29,30], and 3D editing [31,32]. Recently, there have been several attempts to extend the application of CLIP to the task of zeroshot 3D shape understanding. Among these attempts, PointCLIP [11,13] combines visual and textual information to achieve cross-modal language-image embedding, while ULIP [6,12] employs large multimodal models to generate detailed language descriptions of 3D objects, addressing limitations in existing 3D object datasets regarding the quality and scalability of language descriptions. These methods demonstrate the effectiveness of extending the CLIP-based strategy in zero-shot 3D shape understanding.CLIP is a type of pretraining that uses contrastive learning to learn representations from large-scale image-text pairs [24]. CLIP bridges the gap between the language and image modalities, enabling it to capture rich semantic relationships between them [25]. The efficiency of the CLIP in understanding 3D scenes has been validated through various applications, such as 3D object generation [26][27][28], 3D recognition [29,30], and 3D editing [31,32]. Recently, there have been several attempts to extend the application of CLIP to the task of zeroshot 3D shape understanding. Among these attempts, PointCLIP [11,13] combines visual and textual information to achieve cross-modal language-image embedding, while ULIP [6,12] employs large multimodal models to generate detailed language descriptions of 3D objects, addressing limitations in existing 3D object datasets regarding the quality and scalability of language descriptions. These methods demonstrate the effectiveness of extending the CLIP-based strategy in zero-shot 3D shape understanding.</p>
        <p>The common point between previous CLIP-based methods and DILF lies in their fusion of language modality into zero-shot learning to enhance the understanding of 3D objects. The distinction lies in the fact that previous CLIP-based methods either render multi-view images by setting a virtual camera by 30 â€¢ interval [6,12] or project the point cloud from 6 orthogonal views [11,13]. However, these rendering strategies rely on the assumption of homogeneous space (e.g., icosahedron) for predefining view configurations, resulting in leading to redundant and repetitive views [2]. In contrast to previous methods, DILF regresses suitable viewpoints with a differentiable renderer under the supervision of language modality. Consequently, this allows DILF to learn semantically representative views in a data-driven manner.The common point between previous CLIP-based methods and DILF lies in their fusion of language modality into zero-shot learning to enhance the understanding of 3D objects. The distinction lies in the fact that previous CLIP-based methods either render multi-view images by setting a virtual camera by 30 â€¢ interval [6,12] or project the point cloud from 6 orthogonal views [11,13]. However, these rendering strategies rely on the assumption of homogeneous space (e.g., icosahedron) for predefining view configurations, resulting in leading to redundant and repetitive views [2]. In contrast to previous methods, DILF regresses suitable viewpoints with a differentiable renderer under the supervision of language modality. Consequently, this allows DILF to learn semantically representative views in a data-driven manner.</p>
        <p>The paradigm of perception by predicting the optimal environment parameters that generated the rendering image is called Differentiable Rendering (DR) [33]. In the context of DR, appropriate viewpoint positions, lighting, and other physical properties serve as latent variables, which can be inferred to comprehend 3D scenes [34]. Recent DR approaches focus on making the graphics operations differentiable, allowing gradients to flow from the image to the rendering parameters directly [20]. Among these approaches, Abdullah et al. [20] introduce the Multi-View Transformation Network (MVTN) that regresses optimal viewpoints for 3D shape recognition, building upon advances in differentiable rendering. Tulsiani et al. [35] proposes Factored 3D Representation Learning(F3DRL) for learning disentangled 3D object representations by factorizing object shape, appearance, and viewpoint. Liu et al. [36] introduces a differentiable renderer that enables gradient-based optimization for 3D object understanding tasks. Previous research confirms that DR facilitates the examination of 3D objects from appropriate viewpoints, thereby offering a comprehensive interpretation of the 3D object.The paradigm of perception by predicting the optimal environment parameters that generated the rendering image is called Differentiable Rendering (DR) [33]. In the context of DR, appropriate viewpoint positions, lighting, and other physical properties serve as latent variables, which can be inferred to comprehend 3D scenes [34]. Recent DR approaches focus on making the graphics operations differentiable, allowing gradients to flow from the image to the rendering parameters directly [20]. Among these approaches, Abdullah et al. [20] introduce the Multi-View Transformation Network (MVTN) that regresses optimal viewpoints for 3D shape recognition, building upon advances in differentiable rendering. Tulsiani et al. [35] proposes Factored 3D Representation Learning(F3DRL) for learning disentangled 3D object representations by factorizing object shape, appearance, and viewpoint. Liu et al. [36] introduces a differentiable renderer that enables gradient-based optimization for 3D object understanding tasks. Previous research confirms that DR facilitates the examination of 3D objects from appropriate viewpoints, thereby offering a comprehensive interpretation of the 3D object.</p>
        <p>The method most closely related to our work is MVTN [20]. In contrast to the previous work, the MVTN employs the labels of 3D shape categories to implicitly regress optimal viewpoints, whereas DILF leverages language information as explicit guidance for iteratively updating rendering parameters. Recent studies [7][8][9] have demonstrated the potential of fusing language modality as a supplementary component to the 3D modality. Consequently, fusing language modality in DR processing enables a more comprehensive interpretation of 3D objects.The method most closely related to our work is MVTN [20]. In contrast to the previous work, the MVTN employs the labels of 3D shape categories to implicitly regress optimal viewpoints, whereas DILF leverages language information as explicit guidance for iteratively updating rendering parameters. Recent studies [7][8][9] have demonstrated the potential of fusing language modality as a supplementary component to the 3D modality. Consequently, fusing language modality in DR processing enables a more comprehensive interpretation of 3D objects.</p>
        <p>Our proposed DILF approach consists of two training stages. (1) Differentiable rendering-based multi-view Image-Language Fusion (DILF) (Section 3.1), which utilizes language prompts as supervisory signals, and iteratively selects appropriate rendering parameters via a differentiable renderer. (2) DILF for 3D classification (Section 3.2), which contains two downstream tasks, namely zero-shot 3D classification and standard 3D classification.Our proposed DILF approach consists of two training stages. (1) Differentiable rendering-based multi-view Image-Language Fusion (DILF) (Section 3.1), which utilizes language prompts as supervisory signals, and iteratively selects appropriate rendering parameters via a differentiable renderer. (2) DILF for 3D classification (Section 3.2), which contains two downstream tasks, namely zero-shot 3D classification and standard 3D classification.</p>
        <p>The DILF algorithm consists of three modules: (1) LLM-assisted textual feature learning (Section 3.1.1), which utilizes large-scale language models, i.e. GPT-3 [37], to generate language prompts that are rich 1) The large-scale language models (LLMs) are utilized to generate 3D-specific prompts (ğ¶) enriched with 3D semantics derived from the input language commands (ğ¿). ( 2) Viewpoint position learning (ğ¸ ğ‘‰ ) is utilized to learn the scene parameters (ğ‘¢) from the input 3D object, and then a differentiable renderer (ğ‘…) is utilized to generate the multi-view images (ğ‘€) based on the ğ‘¢. (3) The group-view mechanism (ğ¸ ğº ) is adopted to mine the interdependencies among ğ‘€, and to fuse ğ‘€ into a unified feature space. The snowflake represents the freeze of the model weights, while the flame denotes iterative the model weights. in 3D semantics. ( 2) Visual feature learning via differentiable renderer (Section 3.1.2), where the viewpoint position learning network is employed to estimate optimal scene parameters for rendering multiview images. Subsequently, the group-view mechanism combines the multi-view images into a unified representation space. (3) Fusion loss (Section 3.1.3) uses language prompts as supervisory signals to select suitable rendering parameters in a data-driven manner. By pre-training on tuples from both modalities, a unified representation space for information fusion among multi-view images and language is established. The network structure of DILF is illustrated in Fig. 2.The DILF algorithm consists of three modules: (1) LLM-assisted textual feature learning (Section 3.1.1), which utilizes large-scale language models, i.e. GPT-3 [37], to generate language prompts that are rich 1) The large-scale language models (LLMs) are utilized to generate 3D-specific prompts (ğ¶) enriched with 3D semantics derived from the input language commands (ğ¿). ( 2) Viewpoint position learning (ğ¸ ğ‘‰ ) is utilized to learn the scene parameters (ğ‘¢) from the input 3D object, and then a differentiable renderer (ğ‘…) is utilized to generate the multi-view images (ğ‘€) based on the ğ‘¢. (3) The group-view mechanism (ğ¸ ğº ) is adopted to mine the interdependencies among ğ‘€, and to fuse ğ‘€ into a unified feature space. The snowflake represents the freeze of the model weights, while the flame denotes iterative the model weights. in 3D semantics. ( 2) Visual feature learning via differentiable renderer (Section 3.1.2), where the viewpoint position learning network is employed to estimate optimal scene parameters for rendering multiview images. Subsequently, the group-view mechanism combines the multi-view images into a unified representation space. (3) Fusion loss (Section 3.1.3) uses language prompts as supervisory signals to select suitable rendering parameters in a data-driven manner. By pre-training on tuples from both modalities, a unified representation space for information fusion among multi-view images and language is established. The network structure of DILF is illustrated in Fig. 2.</p>
        <p>In this section, we leverage the descriptive capabilities of LLMs to generate 3D-specific prompts ğ¶ ğ‘– enriched with 3D semantics derived from the input language commands ğ¿ ğ‘– . Specifically, the LLMs are utilized to convert language commands ğ¿ ğ‘– into 3D-Specific prompts ğ¶ ğ‘– , and then the textual features ğ‘‡ ğ‘– are extracted from 3D-Specific prompts ğ¶ ğ‘– via a pre-trained text encoder ğ¸ ğ‘‡ .In this section, we leverage the descriptive capabilities of LLMs to generate 3D-specific prompts ğ¶ ğ‘– enriched with 3D semantics derived from the input language commands ğ¿ ğ‘– . Specifically, the LLMs are utilized to convert language commands ğ¿ ğ‘– into 3D-Specific prompts ğ¶ ğ‘– , and then the textual features ğ‘‡ ğ‘– are extracted from 3D-Specific prompts ğ¶ ğ‘– via a pre-trained text encoder ğ¸ ğ‘‡ .</p>
        <p>We leverage the 
            <rs type="software">GPT</rs>-3 [37] to convert language commands ğ¿ ğ‘– into 3D-Specific prompts ğ¶ ğ‘– . e.g., Input: ''Describe the shape of a 3D [plane] when rendering it.''; Output: ''Aerodynamic design with plane wings, tail, and fuselage depicted in 3D space''. By doing so, the representative character of the 3D target is fully described, making the textual prompt of the 3D target more complete and comprehensive. The transformation of the language commands ğ¿ ğ‘– into 3D-Specific prompts ğ¶ ğ‘– is formulated as:
        </p>
        <p>After that, DILF leverages the pre-trained vision-language model SLIP [38](ğ¸ ğ‘‡ ) to extract textual features ğ‘‡ ğ‘– from language commands ğ¶ ğ‘– . The extraction of the textual features ğ‘‡ ğ‘– is formulated as:After that, DILF leverages the pre-trained vision-language model SLIP [38](ğ¸ ğ‘‡ ) to extract textual features ğ‘‡ ğ‘– from language commands ğ¶ ğ‘– . The extraction of the textual features ğ‘‡ ğ‘– is formulated as:</p>
        <p>After the LLM-assisted textual feature learning, the language commands ğ¿ ğ‘– are transformed into textual features ğ‘‡ ğ‘– , providing a feature representation of the language modality for subsequent multi-view image-language fusion.After the LLM-assisted textual feature learning, the language commands ğ¿ ğ‘– are transformed into textual features ğ‘‡ ğ‘– , providing a feature representation of the language modality for subsequent multi-view image-language fusion.</p>
        <p>This section demonstrates the data flow of visual feature extraction, as shown in Fig. 3. The viewpoint position learning network ğ¸ ğ‘‰ learns scene parameters ğ‘¢ from the input 3D object ğ‘†. Subsequently, a differentiable renderer ğ‘… generates multi-view images ğ‘€ based on the scene parameters ğ‘¢. The multi-view features ğ‘“ ğ‘– ğ‘œğ‘Ÿğ‘– are then extracted from the multi-view images ğ‘€ ğ‘– using a pre-trained image encoder ğ¸ ğ¼ . Finally, the group-view mechanism ğ¸ ğº is utilized to obtain the global multi-view feature ğ‘ƒ by fusing the multi-view features ğ‘“ ğ‘– ğ‘œğ‘Ÿğ‘– . Differentiable renderer. The differentiable renderer ğ‘… allows gradients to flow from the loss of image-text alignment to the scene parameters ğ‘¢, and therefore enables obtaining suitable viewpoint position in a data-driven manner. The viewpoint position layer ğ¸ ğ‘‰ is utilized to calculate the scene parameters from the 3D object ğ‘†: ğ‘¢ = ğ¸ ğ‘‰ (ğ‘†) .This section demonstrates the data flow of visual feature extraction, as shown in Fig. 3. The viewpoint position learning network ğ¸ ğ‘‰ learns scene parameters ğ‘¢ from the input 3D object ğ‘†. Subsequently, a differentiable renderer ğ‘… generates multi-view images ğ‘€ based on the scene parameters ğ‘¢. The multi-view features ğ‘“ ğ‘– ğ‘œğ‘Ÿğ‘– are then extracted from the multi-view images ğ‘€ ğ‘– using a pre-trained image encoder ğ¸ ğ¼ . Finally, the group-view mechanism ğ¸ ğº is utilized to obtain the global multi-view feature ğ‘ƒ by fusing the multi-view features ğ‘“ ğ‘– ğ‘œğ‘Ÿğ‘– . Differentiable renderer. The differentiable renderer ğ‘… allows gradients to flow from the loss of image-text alignment to the scene parameters ğ‘¢, and therefore enables obtaining suitable viewpoint position in a data-driven manner. The viewpoint position layer ğ¸ ğ‘‰ is utilized to calculate the scene parameters from the 3D object ğ‘†: ğ‘¢ = ğ¸ ğ‘‰ (ğ‘†) .</p>
        <p>((</p>
        <p>The scene parameters ğ‘¢ contain azimuth angles ğ‘¢ ğ‘ and elevation angles ğ‘¢ ğ‘’ of each rendering camera. Azimuth and elevation angles are used in 3D rendering to describe the position of a rendering camera in a spherical coordinate system [39], as shown in Fig. 4. The azimuth angles ğ‘¢ ğ‘ and elevation angles ğ‘¢ ğ‘’ are formulated as:The scene parameters ğ‘¢ contain azimuth angles ğ‘¢ ğ‘ and elevation angles ğ‘¢ ğ‘’ of each rendering camera. Azimuth and elevation angles are used in 3D rendering to describe the position of a rendering camera in a spherical coordinate system [39], as shown in Fig. 4. The azimuth angles ğ‘¢ ğ‘ and elevation angles ğ‘¢ ğ‘’ are formulated as:</p>
        <p>The ğ¶â„ğ‘¢ğ‘›ğ‘˜ function is utilized to divide the ğ‘¢ into ğ‘¢ ğ‘ and ğ‘¢ ğ‘’ . The ğ‘¢ ğ‘ğ‘œğ‘¢ğ‘›ğ‘‘ is positive and it defines the permissible range for ğ‘¢ ğ‘ and ğ‘¢ ğ‘’ . We set ğ‘¢ ğ‘ğ‘œğ‘¢ğ‘›ğ‘‘ to 360 â€¢ . The azimuth and elevation angles can determine the position of the rendering camera in 3D coordinates. After that, the 3D object is rendered based on the position of the rendering camera in 3D coordinates:The ğ¶â„ğ‘¢ğ‘›ğ‘˜ function is utilized to divide the ğ‘¢ into ğ‘¢ ğ‘ and ğ‘¢ ğ‘’ . The ğ‘¢ ğ‘ğ‘œğ‘¢ğ‘›ğ‘‘ is positive and it defines the permissible range for ğ‘¢ ğ‘ and ğ‘¢ ğ‘’ . We set ğ‘¢ ğ‘ğ‘œğ‘¢ğ‘›ğ‘‘ to 360 â€¢ . The azimuth and elevation angles can determine the position of the rendering camera in 3D coordinates. After that, the 3D object is rendered based on the position of the rendering camera in 3D coordinates:</p>
        <p>During the rendering process, ğ‘… has two components: a rasterizer and a shader. First, the rasterizer transforms the 3D object ğ‘† from the world to view coordinates given the camera viewpoint and assigns faces to pixels. Using these face assignments, the shader creates multiple values for each pixel and then blends them. By doing so, the input 3D object ğ‘† is transformed into multi-view images ğ‘€.During the rendering process, ğ‘… has two components: a rasterizer and a shader. First, the rasterizer transforms the 3D object ğ‘† from the world to view coordinates given the camera viewpoint and assigns faces to pixels. Using these face assignments, the shader creates multiple values for each pixel and then blends them. By doing so, the input 3D object ğ‘† is transformed into multi-view images ğ‘€.</p>
        <p>Group-view mechanism. Rendering a 3D object often results in the generation of redundant and repetitive multi-view images [23]. To extract the content relationship and discriminative information from the views, we incorporate a residual connection into the group-view mechanism, as shown in Fig. 3(b). The residual structure enables the network to learn the residual mapping between the input and output of the layer, enhancing the network's ability to model complex interdependencies [40]. Consequently, incorporating the residual structure into the group-view mechanism enables a deeper exploration of the intrinsic connections among multi-view images. Specifically, we first --------~; ::: ----------------------------------------------( extract the multi-view features ğ‘“ ğ‘– ğ‘œğ‘Ÿğ‘– from multi-view images ğ‘€ ğ‘– by the pre-trained image encoder ğ¸ ğ¼ . Subsequently, the Multi-view features are concatenated by:Group-view mechanism. Rendering a 3D object often results in the generation of redundant and repetitive multi-view images [23]. To extract the content relationship and discriminative information from the views, we incorporate a residual connection into the group-view mechanism, as shown in Fig. 3(b). The residual structure enables the network to learn the residual mapping between the input and output of the layer, enhancing the network's ability to model complex interdependencies [40]. Consequently, incorporating the residual structure into the group-view mechanism enables a deeper exploration of the intrinsic connections among multi-view images. Specifically, we first --------~; ::: ----------------------------------------------( extract the multi-view features ğ‘“ ğ‘– ğ‘œğ‘Ÿğ‘– from multi-view images ğ‘€ ğ‘– by the pre-trained image encoder ğ¸ ğ¼ . Subsequently, the Multi-view features are concatenated by:</p>
        <p>where concate represents the vertical concatenation. After that, a residual layer is utilized, which improves the model's capacity to describe inter-dependencies between views. The global multi-view feature ğ‘ƒ âˆˆ R 1Ã—N are defined as:where concate represents the vertical concatenation. After that, a residual layer is utilized, which improves the model's capacity to describe inter-dependencies between views. The global multi-view feature ğ‘ƒ âˆˆ R 1Ã—N are defined as:</p>
        <p>The weight ğ‘¤ âˆˆ R 1Ã—N , ğ›¿ âˆˆ R 1Ã—N and ğ›½ âˆˆ R 1 are employed to fine-tune the weights of ğ‘“ ğ‘ âˆˆ R 1Ã—N . The unlearnable hyperparameter ğ›½ serves as the balance factor for the residual connection, similar to ResNext [41].The weight ğ‘¤ âˆˆ R 1Ã—N , ğ›¿ âˆˆ R 1Ã—N and ğ›½ âˆˆ R 1 are employed to fine-tune the weights of ğ‘“ ğ‘ âˆˆ R 1Ã—N . The unlearnable hyperparameter ğ›½ serves as the balance factor for the residual connection, similar to ResNext [41].</p>
        <p>The learnable weights ğ‘¤ and ğ›¿ of the residual connection are dot multiplied with ğ‘“ ğ‘ . This process harnesses the explanatory prowess of the residual structure, thereby directing the DILF model to focus on complex inter-dependencies. The weight ğ‘¤ and ğ›¿ are defined as:The learnable weights ğ‘¤ and ğ›¿ of the residual connection are dot multiplied with ğ‘“ ğ‘ . This process harnesses the explanatory prowess of the residual structure, thereby directing the DILF model to focus on complex inter-dependencies. The weight ğ‘¤ and ğ›¿ are defined as:</p>
        <p>where ğ‘Š 1 and ğ‘Š 2 are learning weights of linear layers, ğ‘“ ğºğ´ğ‘ƒ denotes the global average pooling.where ğ‘Š 1 and ğ‘Š 2 are learning weights of linear layers, ğ‘“ ğºğ´ğ‘ƒ denotes the global average pooling.</p>
        <p>After the differentiable rendering and group-view mechanism, the input 3D object ğ‘† is transformed into global multi-view features ğ‘ƒ , providing a feature representation of the 3D modality for subsequent multi-view image-language fusion.After the differentiable rendering and group-view mechanism, the input 3D object ğ‘† is transformed into global multi-view features ğ‘ƒ , providing a feature representation of the 3D modality for subsequent multi-view image-language fusion.</p>
        <p>Following the extraction of textual features ğ‘‡ and global multiview features ğ‘ƒ , DILF pre-trained to fuse these two modalities' representations into a unified representation space. Throughout the network training, we maintain the pre-trained vision-language models, i.e., SLIP [38](ğ¸ ğ‘‡ , ğ¸ ğ¼ ) in a frozen state and train the viewpoint position learning(ğ¸ ğ‘‰ ) and group-view mechanism(ğ¸ ğº ) by aligning an object's textual features ğ‘‡ with its corresponding multi-view features ğ‘ƒ . This approach enables DILF to learn scene rendering parameters through natural language supervision, thereby obtaining suitable multi-view images in a data-driven manner. Specifically, the fusion loss in contrastive -------------------------------------------------Following the extraction of textual features ğ‘‡ and global multiview features ğ‘ƒ , DILF pre-trained to fuse these two modalities' representations into a unified representation space. Throughout the network training, we maintain the pre-trained vision-language models, i.e., SLIP [38](ğ¸ ğ‘‡ , ğ¸ ğ¼ ) in a frozen state and train the viewpoint position learning(ğ¸ ğ‘‰ ) and group-view mechanism(ğ¸ ğº ) by aligning an object's textual features ğ‘‡ with its corresponding multi-view features ğ‘ƒ . This approach enables DILF to learn scene rendering parameters through natural language supervision, thereby obtaining suitable multi-view images in a data-driven manner. Specifically, the fusion loss in contrastive -------------------------------------------------</p>
        <p>where ğ‘– and ğ‘— are index of the textual features ğ‘‡ and multi-view features ğ‘ƒ respectively. A positive pair in the training batch is indicated when ğ‘– = ğ‘—. Conversely, when ğ‘– â‰  ğ‘—, this signifies that the textual features ğ‘‡ ğ‘– do not match the multi-view features ğ‘ƒ ğ‘— , as shown in Fig. 5. We use a learnable temperature parameter ğœ as well, similar to CLIP [10].where ğ‘– and ğ‘— are index of the textual features ğ‘‡ and multi-view features ğ‘ƒ respectively. A positive pair in the training batch is indicated when ğ‘– = ğ‘—. Conversely, when ğ‘– â‰  ğ‘—, this signifies that the textual features ğ‘‡ ğ‘– do not match the multi-view features ğ‘ƒ ğ‘— , as shown in Fig. 5. We use a learnable temperature parameter ğœ as well, similar to CLIP [10].</p>
        <p>During pre-training, we find that if we update SLIP's [38] image and text encoders (ğ¸ ğ‘‡ , ğ¸ ğ¼ ) will result in catastrophic forgetting due to limited data size. This will lead to a significant performance drop when applying DILF to downstream tasks. Therefore we freeze the weights of ğ¸ ğ‘‡ and ğ¸ ğ¼ during the entire pre-training and only update ğ¸ ğ‘‰ and ğ¸ ğº .During pre-training, we find that if we update SLIP's [38] image and text encoders (ğ¸ ğ‘‡ , ğ¸ ğ¼ ) will result in catastrophic forgetting due to limited data size. This will lead to a significant performance drop when applying DILF to downstream tasks. Therefore we freeze the weights of ğ¸ ğ‘‡ and ğ¸ ğ¼ during the entire pre-training and only update ğ¸ ğ‘‰ and ğ¸ ğº .</p>
        <p>In the process of multi-view image-language fusion, we utilize language as explicit guidance to obtain suitable rendering scene parameters in a data-driven manner. This approach allows DILF to benefit from the large-scale pretrained vision-language encoders, which enable the language modality to provide supplementary information for the 3D modality, and therefore augmenting zero-shot 3D shape understanding.In the process of multi-view image-language fusion, we utilize language as explicit guidance to obtain suitable rendering scene parameters in a data-driven manner. This approach allows DILF to benefit from the large-scale pretrained vision-language encoders, which enable the language modality to provide supplementary information for the 3D modality, and therefore augmenting zero-shot 3D shape understanding.</p>
        <p>By leveraging the pre-trained weights of DILF as described in Section 3.1, we implement both zero-shot and standard 3D classification tasks separately based on the pre-trained weights of DILF, as shown in Fig. 6. The weight training status of DILF's different modules during pretraining, zero-shot classification, and standard classification stage, as shown in Table 1. ShapeNet [42] is employed as the pretraining dataset, while ModelNet40 [43] and 
            <rs type="software">ScanObjectNN</rs> [44] are used for 3D classification. To facilitate zero-shot learning, we removed overlapping classes from ShapeNet, ModelNet40, and 
            <rs type="software">ScanObjectNN</rs>, thereby ensuring that the unseen objects were not part of the pretraining samples. In the zero-shot classification process, we freeze the weights of DILF and input the unseen category of 3D objects into the network. Conversely, during the standard 3D classification, we freeze the weights of ğ¸ ğ‘‰ and ğ¸ ğ¼ and fine-tune the weights of ğ¸ ğº and ğ¸ ğ¶ .
        </p>
        <p>In this section, we first outline the experimental settings, encompassing the downstream datasets, implementation details, evaluationIn this section, we first outline the experimental settings, encompassing the downstream datasets, implementation details, evaluation</p>
        <p>To facilitate zero-shot learning, we employ different datasets for DILF pretraining and classification tasks. Specifically, ShapeNet [42] serves as the pretraining dataset, while ModelNet40 [43] and 
            <rs type="software">ScanOb-jectNN</rs> [44] are used for zero-shot learning. We adopts the segmentation rule of ULIP [12] for the training and evaluation dataset to ensure performing a fair comparison with previous zero-shot methods. Specifically, we eliminate overlapping classes among ShapeNet, ModelNet40, and 
            <rs type="software">ScanObjectNN</rs> to ensure that the unseen objects are not included in the training samples.
        </p>
        <p>ShapeNet is a richly annotated, large-scale shape repository of 3D CAD models representing various objects, with extensive annotations. The 3D models in ShapeNet cover a broad spectrum of semantic categories, organized according to the WordNet taxonomy. ShapeNet is widely employed in tasks such as point cloud classification, ShapeNet comprises more than 3 million 3D models, encompassing 55 prevalent object categories and 12,000 distinct object classes.ShapeNet is a richly annotated, large-scale shape repository of 3D CAD models representing various objects, with extensive annotations. The 3D models in ShapeNet cover a broad spectrum of semantic categories, organized according to the WordNet taxonomy. ShapeNet is widely employed in tasks such as point cloud classification, ShapeNet comprises more than 3 million 3D models, encompassing 55 prevalent object categories and 12,000 distinct object classes.</p>
        <p>ModelNet40 is a synthetic dataset of 3D CAD models, with a total of 9843 samples designated for training and 2468 samples for testing, spanning across 40 distinct categories.ModelNet40 is a synthetic dataset of 3D CAD models, with a total of 9843 samples designated for training and 2468 samples for testing, spanning across 40 distinct categories.</p>
        <p>
            <rs type="software">ScanObjectNN</rs> is a dataset comprising of real-world 3D scanned objects, encompassing 2902 objects classified into 15 distinct categories. The dataset is available in three variants: 'OBJ-ONLY', which includes ground truth segmented objects extracted from scene meshes datasets; 'OBJ-BJ', which features objects coupled with background noise; and 'PB-T50-RS', which introduces perturbations such as translation, rotation, and scaling to the dataset.
        </p>
        <p>DILF's pretraining and fine-tuning experiments are conducted on 4 NVIDIA RTX 5000 GPUs with 16 GB of memory each. The network structure is implemented using 
            <rs type="software">Pytorch</rs> [45]. The differentiable renderer is implemented using 
            <rs type="software">Pytorch3D</rs> [46]. During the differentiable rendering, we established the rendering configurations of camera position and light position as unlearnable parameters, while the azimuth and elevation angles of the rendering camera were set as learnable parameters, in line with previous differentiable rendering-based studies [20,47,48].
        </p>
        <p>During the multi-view rendering, the original point clouds in the ModelNet40 and 
            <rs type="software">ScanObjectNN</rs> datasets do not include color information. Consequently, colors were manually selected for the point clouds before their rendering into multi-view images. To ensure a fair comparison with prior 3D shape classification studies [20,21,23,49,50], the 3D objects are rendered in gray, aligning with the color configuration used in previous methods.
        </p>
        <p>For the network input, we uniformly sample 3072 points from each 3D object. As mentioned in Section 3.1, we choose the differentiable renderer ğ‘… from 
            <rs type="software">Pytorch3D</rs> in our pipeline for its speed and compatibility rendering. The powerful language model, 
            <rs type="software">GPT-3</rs> [37], is adopted to convert the language commands into 3D-Specific prompts as described in Section 3.1.1. As mentioned in Section 3.1.2, the PointNet [14] is leveraged as the backbone of the viewpoint position learning network ğ¸ ğ‘‰ . During pretraining, we utilize an advanced version of CLIP, namely SLIP [38], to perform as the visual and text encoder(ğ¸ ğ‘‡ , ğ¸ ğ¼ ). We freeze the weights of ğ¸ ğ‘‡ and ğ¸ ğ¼ and fine-tune the weights of ğ¸ ğ‘‰ and ğ¸ ğº for 100 epochs, with the 64 as the batch size, 10 -3 as the learning rate, and AdamW as the optimizer.
        </p>
        <p>In accordance with [12], the process of zero-shot 3D classification is performed by quantifying the distances between the features of a 3D object and the textual features of potential categories, as depicted in Fig. 6. It is notable that the categories utilized in zero-shot 3D classification are excluded from the pretraining phase, thereby preserving the purpose of zero-shot learning. The category that yields the minimal distance is chosen as the predicted category, as delineated in Section 3.1.3. Our pretrained models are directly utilized in zero-shot classification, without the necessity for a fine-tuning stage.In accordance with [12], the process of zero-shot 3D classification is performed by quantifying the distances between the features of a 3D object and the textual features of potential categories, as depicted in Fig. 6. It is notable that the categories utilized in zero-shot 3D classification are excluded from the pretraining phase, thereby preserving the purpose of zero-shot learning. The category that yields the minimal distance is chosen as the predicted category, as delineated in Section 3.1.3. Our pretrained models are directly utilized in zero-shot classification, without the necessity for a fine-tuning stage.</p>
        <p>In Standard 3D classification, we introduced the softmax layer as the classification head ğ¸ ğ¶ . As mentioned in Section 3.2, ğ¸ ğº and ğ¸ ğ¶ are fine-tuned, while the rest of the modules are frozen, as depicted in Fig. 6. We set the learning rate to 10 -4 and fine-tune our model for 60 epochs, with a batch size of 64, and AdamW as the optimizer.In Standard 3D classification, we introduced the softmax layer as the classification head ğ¸ ğ¶ . As mentioned in Section 3.2, ğ¸ ğº and ğ¸ ğ¶ are fine-tuned, while the rest of the modules are frozen, as depicted in Fig. 6. We set the learning rate to 10 -4 and fine-tune our model for 60 epochs, with a batch size of 64, and AdamW as the optimizer.</p>
        <p>In accordance with 3D classification community standards, we evaluated each method using top-1 accuracy for the zero-shot 3D classification task; mean Average Precision (mAP), and Cumulative Matching Characteristic (CMC) curves for the standard 3D classification task.In accordance with 3D classification community standards, we evaluated each method using top-1 accuracy for the zero-shot 3D classification task; mean Average Precision (mAP), and Cumulative Matching Characteristic (CMC) curves for the standard 3D classification task.</p>
        <p>We record the number of floating-point operations (GFLOPs) and the time of a forward pass for a single input sample. In the zero-shot classification evaluation stage, the floating point operations (FLOPs) of DILF when retrieving one 3D object are limited to 0.132 GFLOPs, with a per-computation cost of 13.23 ms. In the standard classification evaluation stage, the FLOPs of DILF when retrieving one 3D object are limited to 0.104 GFLOPs, with a per-computation cost of 11.62 ms. The computation cost of DILF validates that DILF is affordable when implemented in deployment scenarios. It is notable that previous zeroshot classification works [6,[11][12][13]51] have not accounted for the computational overhead of rendering 3D objects into multi-view images in their FLOPs reporting. In contrast, DILF incorporates the differentiable renderer into its network architecture, enabling end-to-end training. Consequently, DILF calculates the complete computational overhead of model inference, inclusive of the computational overhead of differentiable rendering.We record the number of floating-point operations (GFLOPs) and the time of a forward pass for a single input sample. In the zero-shot classification evaluation stage, the floating point operations (FLOPs) of DILF when retrieving one 3D object are limited to 0.132 GFLOPs, with a per-computation cost of 13.23 ms. In the standard classification evaluation stage, the FLOPs of DILF when retrieving one 3D object are limited to 0.104 GFLOPs, with a per-computation cost of 11.62 ms. The computation cost of DILF validates that DILF is affordable when implemented in deployment scenarios. It is notable that previous zeroshot classification works [6,[11][12][13]51] have not accounted for the computational overhead of rendering 3D objects into multi-view images in their FLOPs reporting. In contrast, DILF incorporates the differentiable renderer into its network architecture, enabling end-to-end training. Consequently, DILF calculates the complete computational overhead of model inference, inclusive of the computational overhead of differentiable rendering.</p>
        <p>This section offers a comparative analysis between DILF and current methods within the zero-shot 3D and standard 3D classification. DILF achieves state-of-the-art performance in zero-shot 3D classification and exhibits comparable performance in standard 3D classification with the current methods.This section offers a comparative analysis between DILF and current methods within the zero-shot 3D and standard 3D classification. DILF achieves state-of-the-art performance in zero-shot 3D classification and exhibits comparable performance in standard 3D classification with the current methods.</p>
        <p>In Table 2, we compare the zero-shot classification performance with existing methods. DILF achieves 67.7%/53.3%/47.6%/38.5% accuracy(top-1) on ModelNet40 and 
            <rs type="software">ScanObjectNN</rs>, respectively, surpassing previous methods by at least +3.5%/+3.2%/+6.4%/+3.1%. The result demonstrates the superiority of DILF in handling zero-shot classification challenges, the reasons can be summarized as follows:
        </p>
        <p>(1) The previous zero-shot classification methods [6,11,13,51,52] utilize predefined rendering parameters to generate multi-view images. However, the predefined rendering parameters lead to redundant and repeated multi-view images when rendering a 3D object, which further leads to network overfitting [53]. With the difference from previous methods, DILF employs a differentiable renderer that fuses explicit text guidance into the rendering process to achieve iterative updates of rendering parameters, thereby guaranteeing optimal views as network input. (2) The domain gap exists in the multi-view images during 3D object rendering, as different views contain distinct attributes of the 3D object [54]. However, the previous zero-shot classification methods [6,11,13,51,52] treat all views equally to generate the shape descriptor, neglecting the significant role that the content relationship and discriminative information of the views play in understanding 3D modality. With the difference from previous methods, a groupview mechanism is adopted to mine the interdependencies among views, therefore facilitating a deeper exploration of the intrinsic connections among multi-view images. In summary, the introduction of the differentiable rendering mechanism and grouping-view mechanism augments the perceptual ability of 3D objects and, therefore enhances the performance of DILF in zero-shot 3D classification.(1) The previous zero-shot classification methods [6,11,13,51,52] utilize predefined rendering parameters to generate multi-view images. However, the predefined rendering parameters lead to redundant and repeated multi-view images when rendering a 3D object, which further leads to network overfitting [53]. With the difference from previous methods, DILF employs a differentiable renderer that fuses explicit text guidance into the rendering process to achieve iterative updates of rendering parameters, thereby guaranteeing optimal views as network input. (2) The domain gap exists in the multi-view images during 3D object rendering, as different views contain distinct attributes of the 3D object [54]. However, the previous zero-shot classification methods [6,11,13,51,52] treat all views equally to generate the shape descriptor, neglecting the significant role that the content relationship and discriminative information of the views play in understanding 3D modality. With the difference from previous methods, a groupview mechanism is adopted to mine the interdependencies among views, therefore facilitating a deeper exploration of the intrinsic connections among multi-view images. In summary, the introduction of the differentiable rendering mechanism and grouping-view mechanism augments the perceptual ability of 3D objects and, therefore enhances the performance of DILF in zero-shot 3D classification.</p>
        <p>The recent zero-shot 3D Classification methods always fall short when applied to standard 3D classification tasks. In terms of this, we validate the effectiveness of DILF on standard 3D Classification to validate its adaptability in handling standard 3D classification challenges, as shown in Table 3. Two kinds of methods are compared: methods concentrate on standard 3D classification [20,[55][56][57][58][59][60][61][62][63] and methods concentrate on zero-shot 3D classification [6,51]. The results indicate that DILF outperforms the previous zero-shot 3D classification on standard 3D classification tasks. This is because the proposed method allows the network to obtain accurate text-image fusion in a data-driven manner, enabling a comprehensive perception of the 3D object by leveraging the generalizing power of language modality. Meanwhile, DILF shows slightly inferior to standard 3D classification methods on standard 3D classification tasks. Since the CLIP is a vision-language model, directly utilizing the CLIP in 3D shape understanding requires rendering the 3D object into multi-view images and then aligning between the text and images. However, the conversion from a 3D object into multi-view images exists in information loss, as the positional relationships between points in the 3D object are not explicitly represented in the multi-view images [64], therefore leading to a decline in network performance. Despite these limitations, DILF outperforms the average performance of standard 3D classification methods by + 0.6%/+ 0.9%/+ 3.9%/+ 5.4% in terms of Rank-1/mAP on ModelNet40 and 
            <rs type="software">ScanObjectNN</rs>, and the average performance of zero-shot 3D classification methods by + 0.1%/+ 0.0%/+ 0.5%/+ 0.4% in terms of Rank-1/mAP on ModelNet40 and 
            <rs type="software">ScanObjectNN</rs>. The above results validate the robustness of DILF on standard 3D classification tasks.
        </p>
        <p>DILF comprises three components, namely LLM-assisted textual feature learning, differentiable renderer, and group-view mechanism. In this section, we perform ablation studies on the zero-shot classification task to analyze each component of DILF.DILF comprises three components, namely LLM-assisted textual feature learning, differentiable renderer, and group-view mechanism. In this section, we perform ablation studies on the zero-shot classification task to analyze each component of DILF.</p>
        <p>In this section, we conduct the ablation study for the differentiable renderer, as shown in Table 4. Specifically, we compare the effect of different rendering configurations on zero-shot classification accuracy, and then we explore the suitable viewpoints numbers in zero-shot learning.In this section, we conduct the ablation study for the differentiable renderer, as shown in Table 4. Specifically, we compare the effect of different rendering configurations on zero-shot classification accuracy, and then we explore the suitable viewpoints numbers in zero-shot learning.</p>
        <p>To evaluate the effectiveness of the differentiable renderer in 3D shape understanding, we employ the alternative rendering configurations for producing multi-view images, while maintaining the remaining components of the DILF module. We chose PointCLIP V2 [13] rendering configuration(i.e. project the point cloud from 6 orthogonal views, index-1) and ULIP-2 [6] rendering configuration(i.e. rendering a multi-view image for every 30 â€¢ interval, index-2) as alternative rendering configurations for comparison with the differentiable renderer, as presented in Fig. 1. As shown in Table 4, DILF(index-4) surpasses previous rendering configurations(index-1âˆ¼2) by at least +19.1%/+15.1%/+10.3%/ +4.8% accuracy(top-1) on ModelNet40 and 
            <rs type="software">ScanObjectNN</rs>. The reason is that the previous zero-shot classification methods [6,13] are based on predefined rendering parameters. However, the predefined rendering parameters lead to redundant and repeated multi-view images when rendering a 3D object, which further leads to network overfitting [23]. With the difference between previous methods, DILF employs a differentiable renderer that fuses explicit text guidance into the rendering process, thereby DILF is able to leverage a data-driven approach to mining views with more discriminative information.
        </p>
        <p>To evaluate the effect of the number of views on the performance of DILF. We perform an ablation study to examine the impact of varying the number of rendering viewpoints, as depicted in Table 4. The experiment results(index-3âˆ¼6) demonstrate that an excessive or insufficient number of viewpoints does not augment the DILF's capacity in 3D shape classification. The reason is that an excess of rendering viewpoints can lead to redundant and repetitive images, while a shortage might not capture the key information of 3D objects [2]. In light of the experimental findings, DILF employs 6 rendering viewpoints for multi-view image rendering (index-4).To evaluate the effect of the number of views on the performance of DILF. We perform an ablation study to examine the impact of varying the number of rendering viewpoints, as depicted in Table 4. The experiment results(index-3âˆ¼6) demonstrate that an excessive or insufficient number of viewpoints does not augment the DILF's capacity in 3D shape classification. The reason is that an excess of rendering viewpoints can lead to redundant and repetitive images, while a shortage might not capture the key information of 3D objects [2]. In light of the experimental findings, DILF employs 6 rendering viewpoints for multi-view image rendering (index-4).</p>
        <p>In this section, we conduct the ablation study for LLM-assisted textual feature learning. To fully adapt the descriptive capabilities of LLMs to generate 3D-Specific prompts enriched with 3D semantics, we propose the following four series of language commands: Words to Sentence. Input: ''Make a sentence using these words: a [plane], 3D shape, rendering.''; Output: ''A 3D plane's 3D shape includes aerodynamic design with wings and tail when rendering it''. DILF by at least +17.5%/+11.6%/+6.4%/+4.0% on ModelNet40 and 
            <rs type="software">ScanObjectNN</rs>, compared to the use of naive textual input(index-1). The reason is that the naive textual input cannot fully describe 3D shapes and harms the pre-trained language-image fusion in the embedding space [13]. To overcome the above challenge, we propose LLM-assisted textual feature learning to generate 3D-specific prompts that are rich in 3D semantics as described in Section 3.1. Based on the experimental results, DILF employs the language command structure ''Describe the shape of a 3D [CLASS] when rendering it''. and leverages LLMs to convert these language commands into 3D-Specific prompts.
        </p>
        <p>To evaluate each component of DILF, we perform ablation experiments including differential render(îˆ°), the LLM-assisted textual feature learning (îˆ¸), and the group-view mechanism(îˆ³), as shown in Table 6. When îˆ° is not adopted, we adopt the ULIP [12] rendering setting. When îˆ¸ is not adopted, We do not utilize LLMs to process input text prompts, but directly input the text prompts into DILF. When îˆ³ is not adopted, we vertically concanate the multi-view features, and then a linear layer is adopted to resize the concentration features into specified dimensions.To evaluate each component of DILF, we perform ablation experiments including differential render(îˆ°), the LLM-assisted textual feature learning (îˆ¸), and the group-view mechanism(îˆ³), as shown in Table 6. When îˆ° is not adopted, we adopt the ULIP [12] rendering setting. When îˆ¸ is not adopted, We do not utilize LLMs to process input text prompts, but directly input the text prompts into DILF. When îˆ³ is not adopted, we vertically concanate the multi-view features, and then a linear layer is adopted to resize the concentration features into specified dimensions.</p>
        <p>For index-3 and index-6, the performance of DILF is improved by +20.3%/+16.8%/+11.8%/+9.2% accuracy(top-1) on ModelNet40 and 
            <rs type="software">ScanObjectNN</rs> when utilizing the îˆ° to generate the multi-view images. With the comparison of predefined rendering parameters(index-3), the differential render(index-6) utilizes language prompts as supervisory signals to iteratively select appropriate rendering. The experimental result demonstrates that îˆ° has the potential to obtain more informative rendering images and contribute to accurate text-image fusion. From index-2 and index-4, we can also observe that with the îˆ°, the performance is improved by +0.6%/+4.0%/+3.7%/+2.0% accuracy(top-1) on 
            <rs type="software">ModelNet</rs>40 and ScanObjectNN, because the îˆ° enables a data-driven manner to obtain suitable rendering viewpoint positions.
        </p>
        <p>For index-4 and index-6, the performance is improved by +24.5%/+15.1%/+9.9%/ +8.8% accuracy(top-1) on ModelNet40 and 
            <rs type="software">ScanObjectNN</rs>. The reason for the decline in performance is that when directly inputting text prompts into the CLIP-based network due to the challenge of Naive Textual Prompting (NTP) [13]. NTP stems from the simplistic textual prompt's inability to adequately represent 3D shapes, which consequently impairs the pre-established language-image alignment in the embedding space. From index-2 and index-3, we can also observe that with the îˆ¸, the performance is improved by +4.8%/+2.3%/+2.4%/ +1.6% accuracy(top-1) on ModelNet40 and ScanObjectNN because îˆ¸ enables to leverage the descriptive capabilities of LLMs to generate language prompts enriched with 3D semantics derived from the input textual prompts.
        </p>
        <p>For index-5 and index-6, the performance is improved by +13.5%/+10.9%/+9.5%/+8.4% accuracy(top-1) on ModelNet40 and 
            <rs type="software">ScanObjectNN</rs>. The reason is that the îˆ³ introduces a residual structure for feature fusion, which bolsters the network's capability to model complex interdependencies among multi-view features [40]. Therefore, incorporating the residual structure into the îˆ³ facilitates a deeper exploration of the intrinsic connections among multi-view images. From index-1 and index-2, we can also observe that with the îˆ¸, the performance is improved by +1.5%/+2.3%/+2.7%/+2.5% accuracy(top-1) on 
            <rs type="software">ModelNet</rs>40 and ScanObjectNN. This enhancement is attributed to the extension of îˆ³ into DILF, which enables DILF to extract discriminative information while preserving representative information among multi-view images.
        </p>
        <p>In this section, we provide the visualization result of differentiable rendering, as shown in Fig. 7. It can be observed that as the number of epochs increases, there is progressively less redundant and repetitive information in the multi-view image. The reason is that DILF introduces a differentiable rendering module that predicts optimal viewpoints in a data-driven manner. Therefore, this differentiable rendering process allows DILF to generate multi-view images that are more semantically representative.In this section, we provide the visualization result of differentiable rendering, as shown in Fig. 7. It can be observed that as the number of epochs increases, there is progressively less redundant and repetitive information in the multi-view image. The reason is that DILF introduces a differentiable rendering module that predicts optimal viewpoints in a data-driven manner. Therefore, this differentiable rendering process allows DILF to generate multi-view images that are more semantically representative.</p>
        <p>We propose DILF, which fuses explicit text guidance into the rendering process for iteratively updating rendering parameters to produce informative multi-view images. DILF allows the network to obtain accurate text-image fusion in a data-driven manner, enabling a comprehensive perception of the 3D object by leveraging the generalizing power of language modality. Empirical results demonstrate the advantages of DILF in both standard and zero-shot 3D shape classification.We propose DILF, which fuses explicit text guidance into the rendering process for iteratively updating rendering parameters to produce informative multi-view images. DILF allows the network to obtain accurate text-image fusion in a data-driven manner, enabling a comprehensive perception of the 3D object by leveraging the generalizing power of language modality. Empirical results demonstrate the advantages of DILF in both standard and zero-shot 3D shape classification.</p>
        <p>Future work may explore optimizing rendering settings. The DILF requires a predefined number of rendering viewpoints before the training process. However, the required number of viewpoints generally varies depending on the size of the 3D object. Therefore, it is crucial to dynamically acquire appropriate numbers of rendering viewpoints instead of relying on static, predefined ones. Furthermore, the proposed method can potentially tackle other zero-shot tasks, e.g., zero-shot 3D part segmentation and zero-shot 3D object detection.Future work may explore optimizing rendering settings. The DILF requires a predefined number of rendering viewpoints before the training process. However, the required number of viewpoints generally varies depending on the size of the 3D object. Therefore, it is crucial to dynamically acquire appropriate numbers of rendering viewpoints instead of relying on static, predefined ones. Furthermore, the proposed method can potentially tackle other zero-shot tasks, e.g., zero-shot 3D part segmentation and zero-shot 3D object detection.</p>
        <p>Shape Description.Shape Description.</p>
        <p>aa</p>
        <p>We thank all reviewers and editors for their valuable feedback. This work is supported by the National Natural Science Foundation of China No. 6237334, Beijing Natural Science Foundation No. L233036.We thank all reviewers and editors for their valuable feedback. This work is supported by the National Natural Science Foundation of China No. 6237334, Beijing Natural Science Foundation No. L233036.</p>
        <p>Github link is shared in the paper.Github link is shared in the paper.</p>
        <p>The authors declare no conflict of interestsThe authors declare no conflict of interests</p>
    </text>
</tei>
