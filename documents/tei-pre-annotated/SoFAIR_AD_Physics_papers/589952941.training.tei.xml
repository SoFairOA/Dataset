<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-25T06:33+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Artificial intelligence is paving the way for a new era of algorithms focusing directly on the information contained in the data, autonomously extracting relevant features for a given application. While the initial paradigm was to have these applications run by a server hosted processor, recent advances in microelectronics provide hardware accelerators with an efficient ratio between computation and energy consumption, enabling the implementation of artificial intelligence algorithms 'at the edge'. In this way only the meaningful and useful data are transmitted to the end-user, minimising the required data bandwidth, and reducing the latency with respect to the cloud computing model. In recent years, European Space Agency is promoting the development of disruptive innovative technologies on-board Earth Observation missions. In this field, the most advanced experiment to date is the Φ-sat-1, which has demonstrated the potential of Artificial Intelligence as a reliable and accurate tool for cloud detection on-board a hyperspectral imaging mission. The activities involved included demonstrating the robustness of the Intel Movidius Myriad 2 hardware accelerator against ionising radiation, developing a Cloudscout segmentation neural network, run on Myriad 2, to identify, classify, and eventually discard on-board the cloudy images, and assessing of the innovative Hyperscout-2 hyperspectral sensor. This mission represents the first official attempt to successfully run an AI Deep Convolutional Neural Network (CNN) directly inferencing on a dedicated accelerator on-board a satellite, opening the way for a new era of discovery and commercial applications driven by the deployment of on-board AI.</p>
        <p>T HE space industry is growing at an incredible speed.</p>
        <p>The increasing number of new private ventures that complement the traditionally public sector agencies in the space arena shows how this sector is attractive and rich with new opportunities [1]- [3]. However, the adoption of new technology on-board satellites is still strongly limited by the requirements of reliability and availability, which traditionally have imposed the use of components with flight-heritage and extensive qualification. This is why on-board processing on space borne data systems still relies on old components that do not provide enough computational power to run most of the innovative state-of-the-art algorithms [4], [5].</p>
        <p>Artificial Intelligence (AI) shows the ability to solve very complex problems exploiting only the intrinsic information contained within data, reducing the pre-and post-processing that is required by standard on-board techniques. In this sense, AI can provide the needed boost in actual performance that will allow new applications to be realised [6]. AI algorithms, and especially those related to image processing (such as Convolutional Neural Networks (CNNs) and Deep Neural Networks (DNNs)), are not suited for the typical class of processors used on-satellite due to their limited computational power and memory resources [7]- [9]. Furthermore, flight hardware has to tolerate failures and faults caused by ionizing radiation in orbit [10]. The radiation exposure is dependent on the orbital altitude, so that the Total Ionizing Dose (TID) for Low Earth Orbit (LEO) missions is much less than that for Geostationary Earth Orbit (GEO) and interplanetary missions [11]. The rapid and continuous advancement in semiconductor technology is resulting in commercial processors that are increasingly compute-powerful. The performance of these Commercial Off-The-Shelf (COTS) devices, and in particular their efficiency and ability to implement state-of-the-art AI algorithms with low-power consumption at the edge, leads to an increasingly large gap between their capabilities and those of traditional, reliable space flight hardware [12]. In the latest years, as part of its initiative to promote the development of radically innovative technologies such as AI capabilities onboard Earth Observation (EO) missions, the European Space Agency (ESA) developed the first Φ-sat mission [13], [14], leveraging the development of the Technology and Quality Department of the Agency on a new processor board for LEO missions, named Eyes of Things (EoT) [15]. The EoT board features the Intel Movidius Myriad 2 Vision Processing Unit (VPU) capable of performing fast inferences while maintaining the power consumption well below 2 W [16]. Among the activities supported by ESA to assess and trial the Myriad 2, one of the most important is the successful radiation characterisation of the device at several European test facilities, including CERN [17]. This provided the confidence to progress to an in-fight demonstration of AI applied onsatellite on Φ-Sat 1. This paper presents the design and the first in-flight results of the Φ-Sat 1 mission.</p>
        <p>The aim of the mission is to demonstrate in-flight the capability, robustness and accuracy of AI acceleration using the Myriad 2 device, and the suitability of AI algorithms for elaborating and handling raw L0 data or L0 to L1 data processing directly on-board. Specifically, the goal is to demonstrate accurate band co-registration and precise cloud detection to increase the efficiency of the downlink in terms of ratio between useful images vs non useful images acquired by an innovative hyperspectral camera called HyperScout-2 [18].</p>
        <p>At the time of the Φ-Sat 1 mission, insufficient data volume from the HyperScout ® product line were available to train the CNN to classify each pixel as cloudy or not-cloudy. This is a common problem with space imagers where bespoke cameras are developed for each new mission. The approach was to derive synthesised images starting from the Sentinel-2 dataset [19] after proper processing to emulate the new camera sensor characteristics, and use those for training and initial test. This mission represents the first use of Deep CNN inference on a dedicated COTS VPU processor on-board a satellite, with the aim to autonomously identify hyperspectral images from 
            <rs type="software">HyperScout</rs>-2 that contain a percentage of cloudy pixels less than a given threshold.
        </p>
        <p>Φ-sat-1, the first experiment of the Φ-sat mission series, is part of the ESA Earth Observation Directorate initiative to promote the space-oriented development and adaptation of radically innovative technologies such as AI [20]. The Φsat mission series objective is to address brand new mission concepts, fostering novel architectures or sensing modalities that enable to meet user-driven science and/or applications by means of on-board processing. Specifically, the primary objective of Φ-sat-1 was to demonstrate in flight the ability of a DNN inference running on a dedicated COTS AI accelerator to reliably detect clouds on acquired hyperspectral images, allowing the removal of the cloudy pixels and thereby reducing the amount of data to be downloaded while increasing the information content of this data. The performance of an onboard inference engine based on a machine learning (ML) algorithm for cloud detection was validated in flight. This is the first time such an experiment has been conducted in space.</p>
        <p>Recent advances in space avionics have led to more decentralised on-board compute. COTS edge processors are ideally positioned to deliver low-latency and distributed edge compute at source for value added services from orbit [21]. Furthermore, rapid mission design cycles are possible using COTS devices that incorporate suitable mitigation strategies. Mission lifetime extensions, as well as improvement by means of delta training, are also immediately feasible for AI solutions via dynamic reconfigurability of the neural networks. This paradigm opens new prospects and new opportunities enabled by robust and accurate on-board processing, and L1 to L2 product generation, with respect to the classical approach to download on ground mainly raw data for subsequent processing. DNNs have demonstrated remarkable results in several space applications, such as scene classification [22] object recognition [23], pose-estimation [24], change-detection [25], and others [26]. This capability of performing complex tasks with credible and robust precision has pushed researchers to investigate the possibility to move the application of DNNs on board satellites [2].</p>
        <p>Moving AI to the edge can have a twofold benefit: I) it enables new remote sensing techniques, and II) it enables new types of applications such as those requiring minimal latency direct downlink to the final user, or those optimising the downlink bandwidth by transmitting to ground only useful data or only meta-information [27], [28]. In particular, the deployment of DNNs on-board can help to reduce mission/application bandwidth requirements by filtering out non-useful data [2], [29]. This ability becomes particularly relevant when high revisit times and limited budgets are pushing the increased adoption of small and nano satellites, and CubeSats, which feature extremely limited downlink data-rates [3]. It is worth noting that the robustness and reliability of the processing is of paramount importance, since a false detection can lead to an ultimate and definitive loss of data. In order to port DNNs on board spacecrafts, authors in [1], [17] propose to perform inference using COTS hardware accelerators, which feature improved energy efficiency and low costs and mass.</p>
        <p>Furthermore, COTS accelerators are capable of exploiting the regular structure of neural networks that, regardless of the specific layers, share the same structure and require the repeated execution of the same type of building block operations, such as Multiply and Accumulate (MAC). Thanks to this feature, the use of COTS devices has strong potential to enable the use of the same hardware for different applications, with advantages in terms of reduced mission set-up times, greater market access, and reduced costs [1], [17], [23].</p>
        <p>HyperScout ® 2, shown in Fig. 1, is a miniaturized spectral camera developed by cosine Remote Sensing with a hyperspectral channel in the Visible to Near-Infrared (VNIR) offering 45 bands from 400 to 1000 nm and a multispectral channel in the Thermal Infrared Range (TIR) [12] with 3 bands from 8 µm to 14 µm, as detailed in Tab. I. HyperScout ® 2 is the second generation of the HyperScout ® product line, with the first generation launched and demonstrated in orbit in 2018 [5]. The instrument, detailed in Fig. 2, is based on a two set of 2D sensors used in push broom mode exploiting the orbital motion of the satellite for the acquisition and reconstruction of the entire sensor spectral coverage for each ground pixel.</p>
        <p>The 
            <rs type="software">HyperScout</rs> ® 2 includes a number of subsystems among which the telescope, the VNIR and TIR focal plane arrays, Instrument Control Unit (ICU) and back-end electronics. For data processing the system is equipped with a Central Processing Unit (CPU) based On-Board Data Handling and the EoT as described in the following section. 
            <rs type="software">HyperScout</rs> has been equipped with a dedicated interface board for control and latch-up protection of the EoT.
        </p>
        <p>The telescope is an athermal system based on a monolithical structure. The VNIR Focal Panel Assembly (FPA) is based on The latter are reconstructed unprocessed information at full space-time resolution coming from the imager payload with all available supplemental information to be used in subsequent processing appended. These data are then stored in the payload Mass Memory Unit (MMU).</p>
        <p>The OBDH hardware serves multiple purposes, the most distinct being the platform for both the acquisition and the processing modes. During the acquisition mode, data will be transferred from the BEE into the memory of the OBDH, which is then written to the MMU via SATA. During processing mode, the data is retrieved from the MMU and processed in memory on the OBDH. Both the acquired L0 image data and processed data are stored on board the payload's MMUs.</p>
        <p>Standard methods of constructing spectral cubes from pushbroom sensors rely on a multitude of satellite platform dependent instrumentation. These on-board instruments include GPS, star trackers, and other attitude determination systems. This method inevitably makes the critical interface between platform and instrument very complex to manage in terms of synchronisation, and additionally imposes requirements on the platform that need to be managed, verified and validated. Furthermore, the available instrumentation makes the respective spectral cubing algorithm platform dependent, which could be a disadvantage for push-broom satellites constituting a constellation on multiple platforms where it is desirable for all the satellites to behave in a similar manner. cosine's HyperScout ® are intended to be flown by various customers on different platforms and could indeed be used to create a constellation in the near future. Consequently, a spectral cubing algorithm completely based on machine vision techniques was developed. This algorithm, which is currently in the process of being patented, can construct spectral cubes without the use of Attitude Determination and Control System (ADCS) data allowing the HyperScout instruments, in principle, to operate in a plethora of environments, e.g., on various space based platforms, on-board airplanes, or as part of a Unmanned Aerial Vehicle (UAV), while all using the same code base.</p>
        <p>As introduced in Section I, the AI processing engine on Φ-Sat-1 is a custom build of a Myriad 2-based EoT development board from Ubotica Technologies. Initially developed as part of the 'Eyes of Things' H2020 project [15], the EoT board is a low-power vision-enabled Internet of Things (IoT) edge processing platform. All EoT processing and control is performed by the Intel Movidius Myriad 2 VPU, positioning the board ideally as a readily available Myriad 2 hardware platform for the inference task. The Myriad 2 VPU is a System on Chip (SoC) with integrated DRAM that has been designed from the ground up considering high performance edge compute for vision applications. It is a heterogeneous 14-core SoC, with 2 RISC-V LEON processors managing functionality and controlling the 12 integrated vector processors. These Streaming Hybrid Architecture Vector Engines (SHAVEs) are 128-bit Very Long Instruction Word (VLIW) processors that have concurrent access to a 2MB multi-ported RAM, with 400GB/s of sustained internal memory bandwidth supported between the SHAVEs and RAM. The SHAVE processors contain wide and deep register files controlling multiple functional units including extensive Single Instruction Mutliple Data (SIMD) capability for high parallelism and throughput at both the functional unit and processor level. Firmware on the Myriad 2 utilises the 12 SHAVEs to efficiently perform parallelised Neural Network (NN) inference, including memory management and Direct Memory Access (DMA) for fast network weight loading to the multi-ported memory, providing exceptional and highly sustainable NN inference performance.</p>
        <p>Key to its selection for Φ-Sat-1 was the Myriad 2's compute efficiency. With a core voltage of 0.9V (that guarantees a good level of robustness against the most dangerous destructive radiation effects), it can operate at 600MHz nominally consuming only 1W. 20 independently controllable internal power islands help to minimise power dissipation. Further efficiencies for image processing operations are achieved via the Computer Vision (CV) and Image Signal Processing (ISP) hardware acceleration blocks. Together, these features provide Myriad 2 with in excess of 1 TFLOPs of compute.</p>
        <p>In order to support its deployment on Φ-Sat-1, and in satellite applications in general, the Myriad 2 has undergone radiation characterisation via a range of test campaigns in European test facilities. During these campaigns the device was assessed for susceptibility to latch-up, and to determine radiation cross-sections across a range of energies [17]. Myriad 2 has demonstrated no Single Event Latchup (SEL) effects at energies up to 8.8MeVcm 2 /mg, with further results for recent tests at higher energies pending analysis. The in-package DRAM of the device was shown during Single Event Upset (SEU) campaigns to have high immunity to bit upsets (per device cross-section of 2e -14 at the above energy), indicating its suitability for code and NN model storage, and providing a level of inherent protection against functional upsets. Total Ionising Dose (TID) testing was conducted up to 49krad, with the device found to have no sensitivity to cumulative Co-60 radiation effects up to this dose.</p>
        <p>The EoT board was designed to expose the wide range of Myriad 2 interfaces and peripherals in a compact 76mm x 68mm form factor, facilitating broad application development. USB (2.0 and 3.0) is the high-speed control and data interface to the board, while multiple low speed interfaces (I2C, SPI, UART) for peripheral attachment are exposed, alongside serial and parallel image sensor interfaces. The EoT board, being Myriad-centric, was selected as a suitable AI accelerator for the Φ-Sat-1 mission as it provides an ideal base platform from which to build a complete inference engine, providing a payload-compatible, host controlled, reconfigurable, low power, low heat generation, high-speed interfaced device, in a form-factor that integrates into the available space atop the sensor payload. However, although functionally capable for Φ-Sat-1, the board was not designed with the harsh conditions of launch and space operation in mind, and its design consists entirely of COTS components. Consequently, a thorough analysis of the board design was conducted wherein non-essential functionalities were identified, thermal and vibration factors were considered, and a board-wide component risk analysis was conducted. Out of this a robust version of the board consisting of a custom-assembled COTS EoT mounted on a protection PCB (Fig. 4) was produced for Φ-Sat-1.</p>
        <p>All active components associated with EoT board functionalities that were not required for Φ-Sat-1 were excluded where possible from the board assembly, along with debug indicators and unused interface headers. In final preparation for integration, the board was conformally coated to protect against tin whiskers. The flight configuration of the Φ-Sat-1 EoT board build is shown in Fig. 4.</p>
        <p>Inference functionality is enabled on the EoT board via Ubotica and Intel Movidius firmware [30] executing on the Myriad 2, with host-side control of inference achieved via inference libraries that were custom built in order to target the payload OBC. The libraries expose a compact API for downloading NN models to the EoT, and subsequently for submitting input tensors to the inference engine and receiving corresponding inference results. Asynchronous inference is supported via input and output tensor queues in the firmware. A Built In Self Test (BIST), with board-level and device-level self-test coverage, is executable on-demand from the OBC, enabling health monitoring of internal memories and processors, device interfaces and board peripherals (see Sec.V-D), and detection of SEU effects. The OBC is also responsible for booting the Myriad 2 via USB (either with BIST or inference boot images).</p>
        <p>Most of the ESA missions implement either a new sensor or new sensing technique, implying a general lack of data for the construction of the data sets. Φ-Sat-1, as a highly innovative mission, is no less affected by this lack of pre-flight sensor data, which implied the need to construct and label synthetic data sets based on proxy off-the-shelf data sets (in this case the Sentinel-2 archive). Of course, the authors appreciate that this is a potential source of inaccuracy in the in-flight inference results.</p>
        <p>In order to prepare a representative sample of Sentinel-2 data, the whole archive of Sentinel-2 tiles, available through Sentinel-Hub services [31], was randomly sampled. Fig. 5 shows the locations of the randomly sampled Sentinel-2 images.</p>
        <p>For each sample, the 13 bands of the Sentinel-2, at 10 m/px resolution were acquired. To simulate the behaviour of the HyperScout-2 imager, the original samples were re-binned from 10 m/px to 70 m/px, which corresponds the ground resolution of the HyperScout ® 2 imager. Resampling was done using the default interpolation method of the Sentinel-Hub: nearest neighbour. Although nearest neighbour is not the best solution for image resampling, it represents a good trade-off between dataset management and computational effort. The data was then retrieved and stored in the Coordinate Reference System (CRS) of the corresponding Sentinel-2 tile, so as to remove the need to re-project data. To prepare the dataset to train the DNN, the associated cloud mask was added to each image as produced by the 
            <rs type="software">s2</rs>cloudless package. The 
            <rs type="software">s2</rs>cloudless [32] is an automated cloud-detection algorithm for Sentinel-2 imagery based on a gradient boosting algorithm. The algorithm is mono-temporal, it does not take into account any spatial context and can be executed at any resolution. The input features are Sentinel-2 Level-1C Top-Of-Atmosphere (TOA) reflectance values of the following 10 bands: B01, B02, B04, B05, B08, B8A, B09, B10, B11, B12 and output of the algorithm is a cloud probability map. Users of the algorithm can convert the cloud probability map to a cloud mask by thresholding the cloud probability map. The masks were produced at a 70m/px resolution, the same resolution used to download the Sentinel-2 data, using cloud probability threshold of 0.4. The distribution of the cloud ratio coverage of the data generated is shown in Fig. 6. Clearly, although images were randomly selected, the nature of the observed phenomenon within the instrument swath is such that the vast majority of the images were either almost fully covered by clouds or almost cloud free. This drives the decision on the value of the cloud coverage threshold used to declare the image cloudy and not download it. The distribution of the cloud ratio coverage of the data generated is shown in Fig. 6. Clearly, although images were randomly selected, the nature of the observed phenomenon within the instrument swath is such that the vast majority of the images were either almost fully covered by clouds or almost cloud free. This drives the decision on the value of the cloud coverage threshold used to declare the image cloudy and not download it. From Fig. 6 it is clear that choosing a threshold of 70% enables the detection of fully cloudy images.
        </p>
        <p>HyperScout ® 2 is able to sense 10 of the 13 bands available on the Sentinel-2 satellites. However, using high number of bands for inference would require a large pre-processing directly on-board that would place significant memory and energy demands on the satellite system. In fact, the data processing chain, illustrated by the first 3 blocks Fig. 3, is executed on the HyperScout ® 2 OBDH before the extracted spectral bands are fed into the EoT board for executing the inference step. During the first pre-processing, the raw frames are radiometrically corrected for gain and offset and are geometrically corrected to compensate for the distortions created by instrument's optical train. Next, for the spectral cube construction step, the corrected frames are stack on one another and aligned, using computer vision techniques, to form a hyperspectral data cube. The appropriate bands from this cube are then extracted and normalized during the second preprocessing step.</p>
        <p>To overcome this problem, we performed Principal Component Analysis (PCA) on the 10 bands that HyperScout 2 has in common with Sentinel-2 to select the best three bands to use directly on-board. Using the three most important components from PCA, a sample image that would be submitted to Myriad for inference is shown in Fig. 7 (left), while reconstruction of the true RGB image from the 3 PCA components is shown in Fig. 8 (left), with the original RGB Sentinel-2 image on the right.</p>
        <p>Nevertheless, in-flight reduction of the 10 Sentinel-2 bands into three using PCA, although possible using the CPU on board HyperScout-2, was deemed prohibitive due to the necessary power required. The chosen alternative is to extract from the HyperScout-2 acquired data cube the three most important bands as determined by the S2cloudless model. The feature importance analysis of the S2cloudless model highlighted that the three most important Sentinel-2 bands that have a highly sensitive corresponding bands on HyperScout-2 are the B01, B02 and the B8A, i.e., bands with central wavelengths of 450nm, 494nm and 862nm, respectively.</p>
        <p>After the selection of the three most useful Sentinel-2 bands, the HyperScout-2 data was simulated. The Sentinel-2 Level-1C data (digital numbers), Sun zenith angles, per-band solar irradiances and Sun-Earth distances were used to calculate TOA radiances. For each pixel, the per-band radiance and HyperScout-2 imager per-band noise characteristics were used to calculate (per-band) root-mean-square (RMS) values, and simulated Gaussian noise with zero mean and RMS value as standard deviation was added to the radiances. In case the radiance value for given band was above HyperScout-2 imager saturation thresholds, the value was capped. Finally, all the images produced were normalised to range [0, 1] and stored using 16-bit floating point precision. This normalisation allowed us to fully exploit the input channel of EoT board i.e. 16-bit FP per pixel.</p>
        <p>A custom segmentation NN architecture was developed for the Φ-Sat-1 mission in order to achieve high detail and good granularity in the network result. A semantic segmentation network (Fig. 9) is generally composed of two parts: Encoder and Decoder [33]. The Encoder extracts only the most relevant features from the input image and propagates them through the entire network, increasing the level of details within each feature. At the end of the encoding phase, a non-human-readable vector is extracted that captures a compressed version of the input image. This vector is input to the Decoder, and, helped by the concatenation with the same dimension Encoder layers, reconstructs only the most valuable information, creating the segmented images.</p>
        <p>As already mentioned in Section III, the hardware accelerator used in this mission is the Myriad 2 VPU. Due to limitations on its maximum intra-layer memory, particular attention had to be devoted to the implementation of the convolutional/deconvolutional layers, and to the quantization of the weights to the 16-bit floating point arithmetic available in the VPU. Furthermore, to avoid the saturation of the memory, an input size reduction was performed. In contrast to the binary classification model described in [34], the segmentation network input tensor size is 192 x 192 x 3. This input reduction allows increasing the number of deconvolutional layer within the network model, although the output size has been halved to better handling and post-process the output data by the onboard processor.</p>
        <p>The 
            <rs type="software">CloudScout</rs> network, shown in Fig. 10, was inspired by U-Net [25] which is a network used to segment different scenes, with particular attention to False Negative values [35]. Moreover, this network owes its success to the low number of training images required compared to the mean Intersection over Union (mIoU) obtained. Exploiting the same criteria, the 
            <rs type="software">CloudScout</rs> network uses only the lowest section of the U-Net.
        </p>
        <p>In particular, the network is composed of convolutional, de-convolutional, and max-pooling layers. The convolutional layers have kernel size of 3x3 and stride 1, while the deconvolutional layers are of two types: i) doubling the input image size by exploiting a stride of 2 and kernel of 2x2; ii) increase the input image size of two pixels per axes, exploiting stride 1 and kernel of 3x3. All layers use ReLU activation functions. The training phase, detailed in Fig. 11, was conducted exploiting the Binary Cross Entropy loss function, starting from 0.01 with 
            <rs type="software">AdaDelta</rs> optimiser. Furthermore, in order to reduce the memory footprint and to avoid excessive memory demand during the de-convolutional phase, the reconstructed images, and consequently the number of channels per layer, were reduced in size.
        </p>
        <p>The Receiver Operating Characteristic (ROC) analysis in Fig. 12 shows the variation of the performance with respect to each pixel confidence score threshold value of the last layer. This threshold represents the minimum confidence score needed by the network to define pixels cloudy or not cloudy, providing a fine control of the final output. Furthermore, it is not applied by the NN, but it is computed by the on-board processor and can be changed to adjust the percentage of FP/FN without retrain the network. The red dot represents the best trade-off for the CloudScout network in terms of pixel-wise accuracy and False Positive Rate, as shown by the Confusion Matrix in Table II. This is obtained using a threshold of 0.6 on the output mask of the last layer. The final pixel-wise accuracy is 88.4%. Although this is indicative of the overall quality of the inference, the main parameter that sets the performance and represents the actual index of quality is the false positive (FP) rate. The main reason for this is that using this inference in an operative mission to decide which of the images are worth downloading to ground and which can be discarded on-satellite, the false positives, being images actually not cloudy but detected as cloudy, represent the net loss of good data, containing useful information, that is discarded. Therefore, the chosen quality index is the percentage of FP which for pixel classification is equal to 5.6% (Tab.II). Moving from pixel classification back to image classification, with the definition of cloudy images as the images whose percentage of cloudy pixels is higher than 70%, the associated confusion matrix is that of Table III. It is worth noting that the 88.4% of accuracy of the segmentation algorithm corresponds to 95.1% of tiles accuracy with only 0.8% of the images were classified as False Positive within the synthetic dataset. It is possible to further reduce the number of FPs by increasing via software the threshold of the last layer, at the expenses of some percentage reduction in the pixel wise accuracy. The inference time is approximately 102 ms exploiting 8 of the 12 available SHAVE vector processors of the 
            <rs type="software">Myriad 2</rs>, and consuming only 62.9 KB of memory footprint.
        </p>
        <p>V. RESULTS</p>
        <p>The 
            <rs type="software">HyperScout</rs>-2 pre-processing chain was run on 17 datasets acquired during the Φ-Sat-1 mission, whose coordinates and acquisition times are summarised in Tab. IV. For each of these datasets, three 1152 x 1152 pixels bands with central wavelengths of 450, 494 and 862 nm (see Section III) were produced. 9 of these three-bands sets, combined to form colour images, are shown in Fig. 13.
        </p>
        <p>For every band produced, a saturation mask composed of Boolean True/False values was made so that the impact on the relative band radiances could be assessed. Each pixel which exceeds the saturation threshold was marked as True and all other pixels were assigned False values. Overall, 1.2% of the pixels were saturated due to bright clouds. The percentage of saturated pixels per acquisition is plotted in Fig. 14. It can be seen that acquisition 02CE is considerably more saturated than the other datasets, accounting for 50.6% of the total saturated pixels.</p>
        <p>Since significant feature misalignment between the bands can affect the quality of the NN's inference, the inter-band alignment precision was also assessed. To quantify the precision, key points were identified with the ORB algorithm [20]. The distance between corresponding key points was then calculated and averaged for each band set. At least 100 suitable key point pairs were found for each acquisition except for 038B / Tumbarumba, where only 33 pairs were identified due to the lack of sharp features in the image. All key point pairs with distances of over 10 pixels were discarded because these pairs were observed to be the result of incorrectly identified key points, and misalignments of this magnitude were not visually observed. Overall, the mean separation between the key points for all pairs was calculated as 1.14 ± 1.33 pixels.</p>
        <p>The key point separation per acquisition is plotted in Fig. 15.</p>
        <p>In order to train an NN capable of segmenting the images produced by the HyperScout 2 camera, a dedicated dataset reflecting the characteristics of the sensor itself is needed. As described in Sec.IV-A, the dataset has been synthesized from Sentinel-2 images, which, although similar, differ in some aspects, namely I) radiance vs reflectance; and II) relative SNR per band.</p>
        <p>Simulating the behaviour of radiance from reflectance images may introduce additional noise. This noise is important for the NN which uses the intrinsic characteristics to classify pixel values. To this aim, for each image from the generated synthetic radiance images, we added and removed 5% of the noise from the nominal noise expected for the selected band in HyperScout-2. The process involved adding an Additive White Gaussian Noise (AWGN) to increase the baseline value, and a denoiser algorithm to reduce it. Increasing the SNR values allows us to train a more robust NN capable of working correctly in different situations, and possibly with different bands. Furthermore, to challenge the network and avoid wasting potential good images, some images containing clouds over salt lakes, snow, etc. were included in the synthetic dataset. These images, even if they do not represent the main goal of the ΦSat-1 mission, lead to a reduction of the overall accuracy in the classification of the synthetic dataset. On the other hand, these challenging images improve the ability of the network to recognise boundary situations. In operational missions improvements can be easily obtained adding special location information. Finally, in order to evaluate these capabilities, we mixed some bands obtained from a cube generated by the HyperScout-2 sensor as shown in Fig. 16.</p>
        <p>The classification result is almost the same for all cases, achieving about 97% pixel accuracy for each image. This dataset is a real-world example of generating a synthetic dataset for cloud detection, demonstrating that it is possible to use/generate synthetic images for new sensors to be exploited directly on board.</p>
        <p>The 
            <rs type="software">CloudScout</rs> Segmentation has gone through two completely different testing phases. The first was conducted during the design phase at the end of the training and validation stages, using the synthetized data set, and the second one was conducted in-flight using the available images acquired by 
            <rs type="software">HyperScout</rs>-2 during the Φ-Sat-1 mission. While the first testing phase aimed to assess the network's capabilities and performances using the synthetic dataset to validate the inference against the requirements, the second phase was a direct evaluation of the performance exploiting the hypercubes acquired in-flight during the Φ-Sat-1 mission by the HyperScout-2 camera. The results of the first phase have been already presented and summarised in the confusion matrix in Table III. As already highlighted they respect the most stringent requirement: false positive under 1% on images for the entire test set.
        </p>
        <p>After the launch, 17 cubes acquired by the HyperScout-2 hyperspectral camera have been used for the assessment of the performances of the NN. As already highlighted above, each cube is pre-processed to extract a portion of 1152 by 1152 pixels with only the 3 bands and tiled in 6 by 6 images of size 192 by 192 by 3 bands which makes a total of 612 images tested. An example of original input images and the respectively NN output mask is shown in Fig. 17, Fig. 18.</p>
        <p>The entire test executed on the HyperScout-2 images produced the results shown in Tab.V. Although the result with 0% false positives is very promising, it should be highlighted that the acquisitions of 
            <rs type="software">HyperScout</rs>-2 were not planned to challenge the NN but rather to perform actual applications. In particular, the 
            <rs type="software">HyperScout 2</rs> acquisitions have been planned and executed according to requirements related to Earth Observation applications. All the acquisitions have been used for the PhiSat in orbit demonstration. This implies, for example, that no images of clouds on snow or clouds on Salt Lake have been acquired, while the training and synthetic test data sets randomly chosen from the Sentinel-2 archive contained many configurations quite challenging for the NN. This is also observable in the overall accuracy obtained by the NN model against the synthetic test set which was 95.1% (considering the images results representation) against an accuracy of 96% obtained in the test with in-flight HyperScout-2 acquired images.
        </p>
        <p>In addition to the CloudScout results acquired during Φ-Sat-1's mission, in-flight performance data was also acquired for the EoT inference engine. Four separate EoT hardware test phases were executed over a 70 day period of the mission. During each phase the EoT BIST routine was initiated, which performed both EoT chip-level and board-level diagnostics and reported back results. Chip-level tests coverage encompassed memories, caches, interfaces, and functional tests to dynamically exercise the SHAVEs and multi-ported memory. Board-level diagnostics evaluated the PMU, flash and SD card. The executed diagnostics and their results are summarised in Table VI. It is seen that every diagnostic test passed at each phase, with the exception of the SD card test. 2 and 3 bit errors were observed in two of the data readbacks from the SD card, where the temporal gap between write and readback was 41 seconds. Note that the SD card functionality was not used on the Φ-Sat-1 mission. Test 027A included an additional 240 second test run during which NN inference with an exemplar TinyYOLO [36] model was continuously executed, with all inference outputs exactly matching the reference values. The in-flight diagnostics tests for the EoT inference engine indicate that the device performed as expected onboard Φ-Sat-1 without experiencing any functional upsets, or any functional degradation effects due to radiation. All future installations of the Myriad VPU in space will be equipped with this Built In Test (BIST) that will allow to monitor correct performance of the hardware in time. The 
            <rs type="software">HyperScout</rs>-2 (Sec.III-A), developed and produced by Cosine Remote Sensing B.V., is a hyperspectral camera based on a 2D sensor used in push broom mode. This Hyper-Scout model provides hyperspectral imaging in the visible and near infrared to analyze the Earth composition, along with three thermal infrared bands to retrieve the temperature distribution, boosting and improving the number of Earth Observation applications. As part of the Φ-Sat-1 mission it has been demonstrated that it is possible to run the full preprocessing chain before inference onboard the HyperScout OBDH, including computing spectral data cubes at pixel accuracy without relying on platform ADCS data but instead solely on machine vision techniques, allowing robustness and independence from small satellites' performance.
        </p>
        <p>As shown in Fig. 19, at the bottom of its structure, there is the EoT hardware accelerator, developed by Ubotica (Sec.III-B), latch-up protected and controlled by cosine's 
            <rs type="software">HyperScout</rs> subsystems. The EoT, powered by the Myriad 2 Vision Processing Unit, accelerates both computer vision and artificial intelligence while operating in a low power envelope. Acceleration is achieved via a unique combination of parallel processing and high-bandwidth multi-ported memory on the multi-core Myriad 2 SoC. The NN inference acceleration provided by EoT on Φ-Sat-1 is enabled by a compact, hostintegrated API, wherein the EoT acts as an inference server, supporting frame-by-frame inference requests over a highspeed USB interface. In-flight test and self-test data from Φ-Sat-1 demonstrates the ability of the board to reliably, accurately and robustly perform the inference task throughout the duration of the mission. The success of this board as an AI accelerator on Φ-Sat-1 has led to the development of a successor board (the UB0100 board) for enabling future AIbased cubesat missions to advance on the success of Φ-Sat-1.
        </p>
        <p>In order to demonstrate the potential of using AI directly on board, the 
            <rs type="software">CloudScout</rs> segmentation neural network was developed by the 
            <rs type="creator">University of Pisa</rs> (Sec.IV-B). It assigns to each pixel a binary classification: cloudy or not cloudy. The 
            <rs type="software">CloudScout</rs> NN exploits only 3 of the 10 bands available from the 
            <rs type="software">HyperScout 2</rs> for two main reasons: memory constraints on the Myriad 2, and power limits derived from the satellite power budget. Hence, to perform the NN training, a synthetic dataset was developed by Sinergise (IV-A), starting from the Sentinel 2 images. The dataset was built following three phases: I) Principal component analysis to select the best bands combination for our goal; II) re-binning the Sentinel-2 images from 10m GSD to 70m GSD; III) using the Sinergise s2cloudless algorithm to construct a label/ground truth mask for the input images.
        </p>
        <p>The training of the network aimed to obtain the highest accuracy while maintaining a low number of false positives. Maintaining a low false positive rate is of paramount importance for the application, as images wrongly classified as cloudy would be not transmitted to ground, resulting in a potential loss of interesting data. The 
            <rs type="software">CloudScout</rs> neural network was tested on both synthetic images on ground, and subsequently in-orbit on Φ-Sat-1 for live images acquired from the HyperScout-2 sensor. In both cases the solution demonstrated an accuracy in excess of 95.9% with respect to the tile level and a commensurate low FP rate, achieving a 96% accuracy when performing cloud detection on live images on-satellite as stated in Tab.V.
        </p>
        <p>The Φ-Sat-1 mission represents the first AI on-board demonstrator able to autonomously select non-cloudy images for transmission to ground. Thanks to its in-flight measured performances, Φ-Sat-1 has demonstrated the capability of AI to perform reliable and accurate on-board image processing. This technological advancement in the field of space AI, and the use of low-power COTS hardware-accelerated inference, paves the way for the exploitation of on-board AI in future EO and remote sensing applications, enabling the development of smarter and more efficient satellites for Earth Observation.</p>
        <p>Fig. 12. ROC analysis.</p>
        <p>IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. , NO. ,</p>
        <p>The authors would like to thank the ESRIN team for their support and collaboration throughout the project and Tyvak International Srl for their support during the mission</p>
        <p>Aubrey Dunne is Co-Founder and CTO at Ubotica Technologies, a Dublin-based developer of Computer Vision and AI edge compute solutions. He performs strategic technology planning, liaises with international and multi-national customers and partners, and project manages across the engineering team. Aubrey leads Ubotica's space activities, where his focus is on COTS processors, low-power in-orbit vision and AI processing, AI systems integration, and neural network lifecycle management for spacebased edge compute. Prior to co-founding Ubotica, he completed 10 years of professional engineering consultancy in the core areas of computer vision and embedded systems design Gianluca Furano PhD in Microelectronics Engineering, works in European Space Agency's Data System Division since 2003. He is in charge of research and development activities and he coordinates ESA activities on on-board artificial intelligence. Among Gianluca's interest in ESA are on-board data handling systems and their major components, such as space grade microprocessors and support electronics, meeting very stringent requirements in terms of radiation tolerance, reliability, availability, and safety; key avionics building blocks such as platform mass memories, remote terminal units, on-board buses and data networks; onboard and space to ground data communication protocols including protocol security aspects. Gianluca also provides support to European standardisation (CCSDS, ECSS) in areas such as telemetry, telecommand and on-board data, wireless and monitoring control interfaces. He has authored or co-authored more than 100 publications.</p>
    </text>
</tei>
