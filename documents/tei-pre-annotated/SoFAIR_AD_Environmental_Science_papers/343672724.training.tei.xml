<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-14T08:19+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>This is a PDF file of an article that has undergone enhancements after acceptance, such as the addition of a cover page and metadata, and formatting for readability, but it is not yet the definitive version of record. This version will undergo additional copyediting, typesetting and review before it is published in its final form, but we are providing this version to give early visibility of the article. Please note that, during the production process, errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.This is a PDF file of an article that has undergone enhancements after acceptance, such as the addition of a cover page and metadata, and formatting for readability, but it is not yet the definitive version of record. This version will undergo additional copyediting, typesetting and review before it is published in its final form, but we are providing this version to give early visibility of the article. Please note that, during the production process, errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.</p>
        <p>Ecological Niche and Species Distribution Models (ENMs and SDMs, respectively), are widely applied in ecology, providing important basal information for the most diverse fields, such as conservation (e.g. Keppel et al., 2012;Razgour et al., 2018), biological invasions (Peterson, 2003;Campos et al., 2014;Lins et al., 2018), phylogenetic/evolutionary studies (e.g. Carstens &amp; Richards, 2007;Chifflet et al., 2016) and disease management (Peterson &amp; Shaw, 2003). While there are theoretical differences among ENMs and SDMs (see Peterson &amp; Soberón, 2012), we will adopt the nomenclature ENM from now on as most studies are closer to estimating species' niche. Such broad applicability is related to two significant properties of ENMs: (i) a simple underlying model that requires only occurrence data and environmental variables, and (ii) a huge effort employed by researchers to develop robust methods and software. There is a significant change in methods from first ENMs studies, that uses one to few algorithms and do not explore other steps that could influence the result (Peterson &amp; Holt, 2003), to current studies, which use several algorithms and diverse steps to fit models, such as pseudo-absence allocation and accessible area definition (e.g., Velazco et al., 2019).Ecological Niche and Species Distribution Models (ENMs and SDMs, respectively), are widely applied in ecology, providing important basal information for the most diverse fields, such as conservation (e.g. Keppel et al., 2012;Razgour et al., 2018), biological invasions (Peterson, 2003;Campos et al., 2014;Lins et al., 2018), phylogenetic/evolutionary studies (e.g. Carstens &amp; Richards, 2007;Chifflet et al., 2016) and disease management (Peterson &amp; Shaw, 2003). While there are theoretical differences among ENMs and SDMs (see Peterson &amp; Soberón, 2012), we will adopt the nomenclature ENM from now on as most studies are closer to estimating species' niche. Such broad applicability is related to two significant properties of ENMs: (i) a simple underlying model that requires only occurrence data and environmental variables, and (ii) a huge effort employed by researchers to develop robust methods and software. There is a significant change in methods from first ENMs studies, that uses one to few algorithms and do not explore other steps that could influence the result (Peterson &amp; Holt, 2003), to current studies, which use several algorithms and diverse steps to fit models, such as pseudo-absence allocation and accessible area definition (e.g., Velazco et al., 2019).</p>
        <p>One of the major assets of ENMs is its community, with several researchers dedicated to delving into specific methodological aspects of the modeling process. Some noteworthy aspects involve the control of collinearity among environmental variables (De Marco &amp; Nóbrega, 2018), different strategies for the allocation of pseudo-absences (Engler et al., 2004;Barbet-Massin et al., 2012;Senay et al., 2013); careful definition of the accessible area (Peterson et al., 2001;Soberón, 2010;Barve et al., 2011;Cooper &amp; Soberón, 2018); ensemble of different algorithms (Marmion et al., 2009;Thuiller et al., 2009;Hao et al., 2019); different evaluation metrics (Allouche et al., 2006;Leroy et al., 2018) and diverse methods to partition the occurrence data for fitting and evaluating the model (Muscarella et al., 2014;Roberts et al., 2017). Given the wide variety of methods for each one of the several steps of fitting ENMs and the possible interactions that may arise, the number of models produced for a single species may easily surpass a thousand.One of the major assets of ENMs is its community, with several researchers dedicated to delving into specific methodological aspects of the modeling process. Some noteworthy aspects involve the control of collinearity among environmental variables (De Marco &amp; Nóbrega, 2018), different strategies for the allocation of pseudo-absences (Engler et al., 2004;Barbet-Massin et al., 2012;Senay et al., 2013); careful definition of the accessible area (Peterson et al., 2001;Soberón, 2010;Barve et al., 2011;Cooper &amp; Soberón, 2018); ensemble of different algorithms (Marmion et al., 2009;Thuiller et al., 2009;Hao et al., 2019); different evaluation metrics (Allouche et al., 2006;Leroy et al., 2018) and diverse methods to partition the occurrence data for fitting and evaluating the model (Muscarella et al., 2014;Roberts et al., 2017). Given the wide variety of methods for each one of the several steps of fitting ENMs and the possible interactions that may arise, the number of models produced for a single species may easily surpass a thousand.</p>
        <p>The great diversity of choices creates a duality in ENMs: while models are simple to fit and the required data is easily available, several decisions should be made regarding methodological steps that must be done judiciously and are not as readily available as the data. As a result, studies that rely on ENMs usually do not have the same methodological rigor as studies that focus on developing ENMs, i.e., several studies still apply (Area Under the Curve) AUC as an evaluation metric, even though it has been demonstrated for over 10 years that the metric is deeply affected by prevalence (Lobo et al., 2008) or the extent of the accessible area (Peterson et al., 2008;Barve et al., 2011). On the other side, there has been a great effort to develop alternatives for the AUC and several other methodological aspects, which have been implemented in several 
            <rs type="software">R</rs> packages and 
            <rs type="software">ENMs</rs> software (Thuiller et al., 2009;Guo &amp; Liu, 2010;Naimi &amp; Araújo, 2016;Hijmans et al., 2017;Golding et al., 2018;Kass et al., 2018;Sánchez-Tapia et al., 2018;Cobos et al., 2019).
        </p>
        <p>Ideally, ENMs should be fit-for-purpose, which means that fitting ENMs is a process that must be thought carefully, as there is not a single correct way to fit models (Guillera-Arroita et al., 2015;Qiao et al., 2015). Due to the great variety of methodological choices and the velocity that new alternatives arise, it may be hard to keep up with novelties within the ENMs' field. As a result, people who are not involved in the methodological developments within the field or do not have connections to developers have small participation in all the published papers (Ahmed et al., 2015). We introduce here ENMTML, a new 
            <rs type="software">R</rs> package to fit ENMs. The main objective of this package is to put together all this methodological diversity developed within the ENM field and present it to users simply and transparently. Despite being an 
            <rs type="software">R</rs> package, we also made it friendly for non-programmers and summarized the whole fitting process into a single function with several arguments that correspond to the methodological alternatives.
        </p>
        <p>The ENMTML package and its processes can be divided into three major stages: preprocessing, processing, and post-processing. This division in three stages is familiar to most ENMs routines. Identifying the stage in which each methodological step will be performed may help users to understand the connections among the different methodological steps and provides an overview that assists the decision-making process (Figure 1).The ENMTML package and its processes can be divided into three major stages: preprocessing, processing, and post-processing. This division in three stages is familiar to most ENMs routines. Identifying the stage in which each methodological step will be performed may help users to understand the connections among the different methodological steps and provides an overview that assists the decision-making process (Figure 1).</p>
        <p>In the pre-processing stage, the data is input (species occurrences and predictors variables), and a series of steps can be performed before fitting the model. Occurrence data is input as a tab-separated text file (TXT). The program automatically uses unique occurrences per cell. In addition, the user can control two steps acting over the occurrence dataset: i) the minimum number of occurrences valid for model fitting and ii) perform a thinning process to reduce sampling bias. Regarding predictors, there are three methods to control for collinearity and the possibility to include predictors for other time or geographic windows. As for preprocessing steps, there are five different strategies for pseudo-absence allocation with the option to control for presence-absence ratio; four methods to partition the data into subsets, with the possibility to provide a specific dataset for independent evaluation (a useful asset when studying biotic invasions); two methods to create species-specific accessible areas; and it is also possible to identify extrapolation areas based on a Mobility-Oriented Parity analysis (Owens et al., 2013).In the pre-processing stage, the data is input (species occurrences and predictors variables), and a series of steps can be performed before fitting the model. Occurrence data is input as a tab-separated text file (TXT). The program automatically uses unique occurrences per cell. In addition, the user can control two steps acting over the occurrence dataset: i) the minimum number of occurrences valid for model fitting and ii) perform a thinning process to reduce sampling bias. Regarding predictors, there are three methods to control for collinearity and the possibility to include predictors for other time or geographic windows. As for preprocessing steps, there are five different strategies for pseudo-absence allocation with the option to control for presence-absence ratio; four methods to partition the data into subsets, with the possibility to provide a specific dataset for independent evaluation (a useful asset when studying biotic invasions); two methods to create species-specific accessible areas; and it is also possible to identify extrapolation areas based on a Mobility-Oriented Parity analysis (Owens et al., 2013).</p>
        <p>The processing stage is when algorithms will fit models, and the suitability maps generated.The processing stage is when algorithms will fit models, and the suitability maps generated.</p>
        <p>For starters, the user can choose if both partial and final suitability maps will be generated or not. There are thirteen algorithms available for model fitting: Bioclim (Nix, 1986), Mahalanobis Distance (Farber &amp; Kadmon, 2003), Domain (Carpenter et al., 1993), Ecological Niche Factor Analysis (Hirzel et al., 2002), Generalized Linear Models (McCullagh &amp; Nelder, 1989), Generalized Additive Models (Hastie &amp; Tibshirani, 1990), Boosted Regression Tree (Friedman, 2001), Random Forests (Prasad et al., 2006), Support Vector Machine (Guo et al., 2005), Maximum Entropy with quadratic and linear (Anderson &amp; Gonzalez, 2011) and default features (Phillips et al., 2006;Phillips, 2017), Maximum Likelihood (Royle et al., 2012) and Gaussian Process (Golding &amp; Purse, 2016).For starters, the user can choose if both partial and final suitability maps will be generated or not. There are thirteen algorithms available for model fitting: Bioclim (Nix, 1986), Mahalanobis Distance (Farber &amp; Kadmon, 2003), Domain (Carpenter et al., 1993), Ecological Niche Factor Analysis (Hirzel et al., 2002), Generalized Linear Models (McCullagh &amp; Nelder, 1989), Generalized Additive Models (Hastie &amp; Tibshirani, 1990), Boosted Regression Tree (Friedman, 2001), Random Forests (Prasad et al., 2006), Support Vector Machine (Guo et al., 2005), Maximum Entropy with quadratic and linear (Anderson &amp; Gonzalez, 2011) and default features (Phillips et al., 2006;Phillips, 2017), Maximum Likelihood (Royle et al., 2012) and Gaussian Process (Golding &amp; Purse, 2016).</p>
        <p>Finally, in the post-processing stage, the suitability maps generated from the different algorithms are evaluated using seven different metrics (AUC, True Skill Statistics (TSS), Kappa, Jaccard, Sorensen, Boyce, and F pb ). When multiple models are fitted for the same species (i.e., several replicates or geographical partitions), the evaluation output result is the mean and standard deviation of the partial models. Other post-processing options include the creation of binary maps based on five different thresholds; six different ways to generate ensemble models; and the application of spatial restrictions to reduce model commission and bring the result closer to an estimation of the species realized distribution (MSDM).Finally, in the post-processing stage, the suitability maps generated from the different algorithms are evaluated using seven different metrics (AUC, True Skill Statistics (TSS), Kappa, Jaccard, Sorensen, Boyce, and F pb ). When multiple models are fitted for the same species (i.e., several replicates or geographical partitions), the evaluation output result is the mean and standard deviation of the partial models. Other post-processing options include the creation of binary maps based on five different thresholds; six different ways to generate ensemble models; and the application of spatial restrictions to reduce model commission and bring the result closer to an estimation of the species realized distribution (MSDM).</p>
        <p>All features are organized in a single R function with multiple arguments the user needs to fill according to the specific purpose. We chose not to establish default arguments, so users must think carefully about the choices. To provide support, we briefly explain the methodological steps and indicate relevant studies for each alternative.All features are organized in a single R function with multiple arguments the user needs to fill according to the specific purpose. We chose not to establish default arguments, so users must think carefully about the choices. To provide support, we briefly explain the methodological steps and indicate relevant studies for each alternative.</p>
        <p>Arguments involved: (occ_file/ Sp / x / y / min_occ / thin_occ) Occurrence data is imported as a tab-separated TXT file that needs to be specified by the user as the file path of the file in the argument occ_file. This file must contain information about species name, longitude, and latitude (in decimal degrees), and the name of those columns must be provided in the arguments Sp, x, y.Arguments involved: (occ_file/ Sp / x / y / min_occ / thin_occ) Occurrence data is imported as a tab-separated TXT file that needs to be specified by the user as the file path of the file in the argument occ_file. This file must contain information about species name, longitude, and latitude (in decimal degrees), and the name of those columns must be provided in the arguments Sp, x, y.</p>
        <p>The user must also provide the minimum number of unique occurrences valid for model fitting in the argument min_occ, species below this number will be excluded from the analysis. There is not a rule for the definition of a minimum number of occurrences, but there are several studies that indicate that model accuracy is directly related to sample size (Wisz et al., 2008). There are several factors that affect the viable minimum number of occurrences (Mateo et al., 2010), but a good framework for exploring this subject is the one developed by van Proosdij et al. (2015).The user must also provide the minimum number of unique occurrences valid for model fitting in the argument min_occ, species below this number will be excluded from the analysis. There is not a rule for the definition of a minimum number of occurrences, but there are several studies that indicate that model accuracy is directly related to sample size (Wisz et al., 2008). There are several factors that affect the viable minimum number of occurrences (Mateo et al., 2010), but a good framework for exploring this subject is the one developed by van Proosdij et al. (2015).</p>
        <p>Finally, users might opt to reduce autocorrelation in occurrence data and possible sampling bias by a thinning technique (argument thin_occ), performed using the package 
            <rs type="software">spThin</rs> (Aiello-Lammens et al., 2015). There are three alternatives for defining the thinning distance: i) based on the distance of a Moran's I Variogram that minimizes the spatial autocorrelation;
        </p>
        <p>ii) retaining unique cells that fall within a grid two times greater than the original cellsize; and iii) based on a minimum distance defined by the user (Table 1). For a better comprehension of the topic see (Aiello-Lammens et al., 2015).ii) retaining unique cells that fall within a grid two times greater than the original cellsize; and iii) based on a minimum distance defined by the user (Table 1). For a better comprehension of the topic see (Aiello-Lammens et al., 2015).</p>
        <p>Arguments involved: (pred_dir / proj_dir / colin_var)Arguments involved: (pred_dir / proj_dir / colin_var)</p>
        <p>Predictors are imported in the argument pred_dir, which specifies the folder path of the predictors, and should be in any of the given formats: BIL, TIF, ASC, TXT. Predictors for projection also accept the same formats and should be included in nested folders, with a major folder including all the projections datasets each with its respective sub-folder (Figure 2).Predictors are imported in the argument pred_dir, which specifies the folder path of the predictors, and should be in any of the given formats: BIL, TIF, ASC, TXT. Predictors for projection also accept the same formats and should be included in nested folders, with a major folder including all the projections datasets each with its respective sub-folder (Figure 2).</p>
        <p>Collinearity in predictors can be controlled using three different strategies: i) Pearson correlation with a threshold defined by the user; ii) Variance Inflation Factor (VIF; Marquaridt, 1970) and; Principal Component Analysis (PCA), using the axis that account for 95% of the total variance in the predictors as the new predictors (Heikkinen et al., 2006;De Marco &amp; Nóbrega, 2018). Predictors eliminated by the Pearson and VIF will also be eliminated for projections datasets. When users choose to perform a PCA and have datasets for projection, the linear relationship between the predictors and the principal components is projected onto the new datasets to create the principal components for the projection datasetsCollinearity in predictors can be controlled using three different strategies: i) Pearson correlation with a threshold defined by the user; ii) Variance Inflation Factor (VIF; Marquaridt, 1970) and; Principal Component Analysis (PCA), using the axis that account for 95% of the total variance in the predictors as the new predictors (Heikkinen et al., 2006;De Marco &amp; Nóbrega, 2018). Predictors eliminated by the Pearson and VIF will also be eliminated for projections datasets. When users choose to perform a PCA and have datasets for projection, the linear relationship between the predictors and the principal components is projected onto the new datasets to create the principal components for the projection datasets</p>
        <p>Arguments involved: (pseudoabs_method / pres_abs_ratio)Arguments involved: (pseudoabs_method / pres_abs_ratio)</p>
        <p>The program allocates pseudo-absences and background points within the area used to calibrate the models (Table 3). Such allocation will be particular for those geographical partitioning method (such us block-and band-cross validation) in which pseudo-absences and background points are created after performing such partition, in order to maintain a homogeneous distribution of background points between partitions, as well as a constant prevalence (conceived here as the relationship between presences and pseudo-absences).The program allocates pseudo-absences and background points within the area used to calibrate the models (Table 3). Such allocation will be particular for those geographical partitioning method (such us block-and band-cross validation) in which pseudo-absences and background points are created after performing such partition, in order to maintain a homogeneous distribution of background points between partitions, as well as a constant prevalence (conceived here as the relationship between presences and pseudo-absences).</p>
        <p>Since algorithm's performance may be sensible to the way pseudo-absences are distributed throughout the calibration area (Wisz &amp; Guisan, 2009;Barbet-Massin et al., 2012), the program offers five pseudo-absences allocation methods: i) 'single random' distribution (Zaniewski et al., 2002); ii) 'geographically constrained method', i.e., pseudo-absences are allocated outside a buffer around presences (Barbet-Massin et al., 2012); iii) 'environmental constrained methods' based on the lowest suitable region predicted by a Bioclim model (Engler et al., 2004); iv) 'geographical and environmental constrained method' (Lobo et al., 2010) and; v) a three-step method which combine environmental and geographical approach plus a k-mean non-agglomerative cluster process to distribute homogeneously on environmental space (Senay et al., 2013).Since algorithm's performance may be sensible to the way pseudo-absences are distributed throughout the calibration area (Wisz &amp; Guisan, 2009;Barbet-Massin et al., 2012), the program offers five pseudo-absences allocation methods: i) 'single random' distribution (Zaniewski et al., 2002); ii) 'geographically constrained method', i.e., pseudo-absences are allocated outside a buffer around presences (Barbet-Massin et al., 2012); iii) 'environmental constrained methods' based on the lowest suitable region predicted by a Bioclim model (Engler et al., 2004); iv) 'geographical and environmental constrained method' (Lobo et al., 2010) and; v) a three-step method which combine environmental and geographical approach plus a k-mean non-agglomerative cluster process to distribute homogeneously on environmental space (Senay et al., 2013).</p>
        <p>The program also allows for the user to define the ratio between presences and absences (argument pres_abs_ratio), a methodological step that received considerable focus from researchers and affects algorithm performance (Barbet-Massin et al., 2012).The program also allows for the user to define the ratio between presences and absences (argument pres_abs_ratio), a methodological step that received considerable focus from researchers and affects algorithm performance (Barbet-Massin et al., 2012).</p>
        <p>Arguments involved: (sp_accessible_area)Arguments involved: (sp_accessible_area)</p>
        <p>A crucial decision at the moment to construct ENMs is the hypothesized accessible area, i.e., the geographical region used by a species throughout a relevant period of time (Barve et al., 2011), also known as the movement component of the BAM diagram (Soberon &amp; Peterson, 2005). Such an accessible area can be delimited based on the knowledge of species ecology, dispersal ability, geographical barriers, and ancient region were species inhabited (Soberón, 2010;Peterson et al., 2011). Nonetheless, this information is often missing for most species; therefore, different techniques act as an approximation of the accessible area. ENMTML account with four option to define accessible areas: i) no restriction, i.e., the entire predictors extent will be used as accessible area; ii) define an accessible area based on a buffer around occurrence data; iii) define the accessible area based on a mask, e.g., using a shapefile for biogeographical ecoregions, or; iv) accessible are defined by the user (supported formats: SHP/TIF/BIL/ASC/TXT; Table 4).A crucial decision at the moment to construct ENMs is the hypothesized accessible area, i.e., the geographical region used by a species throughout a relevant period of time (Barve et al., 2011), also known as the movement component of the BAM diagram (Soberon &amp; Peterson, 2005). Such an accessible area can be delimited based on the knowledge of species ecology, dispersal ability, geographical barriers, and ancient region were species inhabited (Soberón, 2010;Peterson et al., 2011). Nonetheless, this information is often missing for most species; therefore, different techniques act as an approximation of the accessible area. ENMTML account with four option to define accessible areas: i) no restriction, i.e., the entire predictors extent will be used as accessible area; ii) define an accessible area based on a buffer around occurrence data; iii) define the accessible area based on a mask, e.g., using a shapefile for biogeographical ecoregions, or; iv) accessible are defined by the user (supported formats: SHP/TIF/BIL/ASC/TXT; Table 4).</p>
        <p>Arguments involved: (eval_occ / part)Arguments involved: (eval_occ / part)</p>
        <p>An ideal model evaluation requires a dataset in which occurrences are independent of the ones used to fit the model; this independent dataset can be supplied as the path to a TXT file in the argument eval_occ.An ideal model evaluation requires a dataset in which occurrences are independent of the ones used to fit the model; this independent dataset can be supplied as the path to a TXT file in the argument eval_occ.</p>
        <p>Nevertheless, the most common evaluation method is to partition occurrence data in two subsets, one to fit the model and another for evaluation. For this option (argument part), the package offers four methods for data partitioning, two based on random partitions and two on geographical partitions (Table 5). Among random partition methods the user can choose: i) bootstrap, in which users specify the number of replicates and proportion of the dataset used for fitting the model, e.g., 10 replicates each with 70% for training models, the remaining 30% is used for validation; and ii) k-fold, in which the dataset is split into a chosen number of folds, and on each run the model is fit using k-1 folds and evaluated on the folder left out. As alternatives for geographical partitions, the dataset can be split based on bands (latitudinal/longitudinal) or based on a checkerboard (blocks), with occurrence data being split into two subsets, alternatively used for fitting and evaluating the model. The optimal band or checkerboard is found based on the size which presents (i) the lower spatial autocorrelation, based on Moran's I, (ii) the maximum environmental similarity, based on Multivariate Environmental Similarity Surface metric (MESS) and (iii) the minimum difference in the number of records between subsets (Velazco et al., 2019). The importance of carefully delimiting blocks for fitting and evaluating the models is discussed by Roberts et al. (2017).Nevertheless, the most common evaluation method is to partition occurrence data in two subsets, one to fit the model and another for evaluation. For this option (argument part), the package offers four methods for data partitioning, two based on random partitions and two on geographical partitions (Table 5). Among random partition methods the user can choose: i) bootstrap, in which users specify the number of replicates and proportion of the dataset used for fitting the model, e.g., 10 replicates each with 70% for training models, the remaining 30% is used for validation; and ii) k-fold, in which the dataset is split into a chosen number of folds, and on each run the model is fit using k-1 folds and evaluated on the folder left out. As alternatives for geographical partitions, the dataset can be split based on bands (latitudinal/longitudinal) or based on a checkerboard (blocks), with occurrence data being split into two subsets, alternatively used for fitting and evaluating the model. The optimal band or checkerboard is found based on the size which presents (i) the lower spatial autocorrelation, based on Moran's I, (ii) the maximum environmental similarity, based on Multivariate Environmental Similarity Surface metric (MESS) and (iii) the minimum difference in the number of records between subsets (Velazco et al., 2019). The importance of carefully delimiting blocks for fitting and evaluating the models is discussed by Roberts et al. (2017).</p>
        <p>Arguments involved: (extrapolation)Arguments involved: (extrapolation)</p>
        <p>ENMs are fitted based on conditions found in occurrences and absence/pseudoabsence/background data. When making predictions, it is not uncommon for models to predict onto new conditions (non-analog climates), especially when performing projections to other time periods or geographical regions. In those situations, models will perform extrapolations, which means that there is some uncertainty as models were not fitted on those environmental conditions (Fitzpatrick &amp; Hargrove, 2009). To identify geographical locations in which models are performing extrapolations, we included a Mobility-Oriented Parity analysis (MOP; Owens et al., 2013), which is based on the defined accessible area for each species. If there is no accessible area, the program calculates MOP based on all conditions within the geographical extent of predictors. Example of articles that discuss the main issues caused by model extrapolation are discussed by Elith et al. (2010) and Owens at al. (2013).ENMs are fitted based on conditions found in occurrences and absence/pseudoabsence/background data. When making predictions, it is not uncommon for models to predict onto new conditions (non-analog climates), especially when performing projections to other time periods or geographical regions. In those situations, models will perform extrapolations, which means that there is some uncertainty as models were not fitted on those environmental conditions (Fitzpatrick &amp; Hargrove, 2009). To identify geographical locations in which models are performing extrapolations, we included a Mobility-Oriented Parity analysis (MOP; Owens et al., 2013), which is based on the defined accessible area for each species. If there is no accessible area, the program calculates MOP based on all conditions within the geographical extent of predictors. Example of articles that discuss the main issues caused by model extrapolation are discussed by Elith et al. (2010) and Owens at al. (2013).</p>
        <p>Arguments involved: (algorithm)Arguments involved: (algorithm)</p>
        <p>As one of the primary sources of ENMs/SDMs uncertainty is the method used to construct them (Watling et al., 2015;Thuiller et al., 2019), and assuming that no single methods can lead with all modeling situation (Qiao et al., 2015), our ENMTML package fit 13 algorithms that range different statistical techniques and type of data used to fit the models (Table 5).As one of the primary sources of ENMs/SDMs uncertainty is the method used to construct them (Watling et al., 2015;Thuiller et al., 2019), and assuming that no single methods can lead with all modeling situation (Qiao et al., 2015), our ENMTML package fit 13 algorithms that range different statistical techniques and type of data used to fit the models (Table 5).</p>
        <p>Model evaluation is performed using seven different metrics: Area Under the Curve (AUC, (Fielding &amp; Bell, 1997), Kappa (Cohen, 1960), True Skill Statistic (Allouche et al., 2006), Jaccard (Leroy et al., 2018), Sorensen (Leroy et al., 2018), F pb (Li &amp; Guo, 2013), Boyce (Boyce et al., 2002), partial ROC and its respective p-value (Peterson et al., 2008), omission rate (OR; Fielding &amp; Bell, 1997) and proportion of the total area in which species is considered to be present (Peterson, 2001). The values at the table are an average of the several replicates (if the bootstrap partition was chosen), folds (if random k-folds were chosen), or geographical subsets (if bands or block partition was chosen), accompanied by the respective standard deviation. Metrics are given for each algorithm used to fit models for each species, and each threshold chosen to create binary maps. The type of partition used to create occurrence subsets is also indicated (Table 7).Model evaluation is performed using seven different metrics: Area Under the Curve (AUC, (Fielding &amp; Bell, 1997), Kappa (Cohen, 1960), True Skill Statistic (Allouche et al., 2006), Jaccard (Leroy et al., 2018), Sorensen (Leroy et al., 2018), F pb (Li &amp; Guo, 2013), Boyce (Boyce et al., 2002), partial ROC and its respective p-value (Peterson et al., 2008), omission rate (OR; Fielding &amp; Bell, 1997) and proportion of the total area in which species is considered to be present (Peterson, 2001). The values at the table are an average of the several replicates (if the bootstrap partition was chosen), folds (if random k-folds were chosen), or geographical subsets (if bands or block partition was chosen), accompanied by the respective standard deviation. Metrics are given for each algorithm used to fit models for each species, and each threshold chosen to create binary maps. The type of partition used to create occurrence subsets is also indicated (Table 7).</p>
        <p>Arguments involved: (thr)Arguments involved: (thr)</p>
        <p>The different thresholds are used to create binary maps, being that more than one option can be chosen, which results in different sets of binary maps created within a single script run (Table 8). The thresholds are chosen based on the suitability value that maximizes a given metric. For instance, the MAX_TSS threshold uses the suitability value that gives the highest TSS value to create binary maps. This is the common threshold at which the sum of Specificity and Sensitivity is maximum. The same logic stands for all the other alternatives, except for Lowest Presence Threshold (LPT; Pearson, 2007) and Sensitivity. LPT threshold establishes a threshold value in which suitability is the lowest among all occurrence data.The different thresholds are used to create binary maps, being that more than one option can be chosen, which results in different sets of binary maps created within a single script run (Table 8). The thresholds are chosen based on the suitability value that maximizes a given metric. For instance, the MAX_TSS threshold uses the suitability value that gives the highest TSS value to create binary maps. This is the common threshold at which the sum of Specificity and Sensitivity is maximum. The same logic stands for all the other alternatives, except for Lowest Presence Threshold (LPT; Pearson, 2007) and Sensitivity. LPT threshold establishes a threshold value in which suitability is the lowest among all occurrence data.</p>
        <p>Sensitivity requires users to specify a desired sensitivity value for the resulting binary map (Table 8).Sensitivity requires users to specify a desired sensitivity value for the resulting binary map (Table 8).</p>
        <p>The major source of model uncertainty is caused by the different algorithms used to fit ENMs (Diniz-Filho et al., 2009;Thuiller et al., 2019). A commonly used method to deal with this is to create an ensemble model of different algorithms (Araújo &amp; New, 2007;Marmion et al., 2009). ENMTML offers six ensemble methods, three based on different ways to calculate models` average and three based on PCA derived from the models. Average-based ensembles can be created using: i) a simple average of all models, ii) weighted average, in which models` suitability is weighted by how well that algorithm performed and iii) superior average, in which a simple average is calculated only for those algorithms that performed better than the average of all algorithms. PCA-based ensemble performs a principal components analysis on suitability maps and uses the first component as the final map, this can be performed: i) using all models, ii) using only the superior models, selected similarly to the superior average, and iii) principal components are calculated using only suitability values above the threshold for each algorithm, values below the threshold are set to zero (Table 9).The major source of model uncertainty is caused by the different algorithms used to fit ENMs (Diniz-Filho et al., 2009;Thuiller et al., 2019). A commonly used method to deal with this is to create an ensemble model of different algorithms (Araújo &amp; New, 2007;Marmion et al., 2009). ENMTML offers six ensemble methods, three based on different ways to calculate models` average and three based on PCA derived from the models. Average-based ensembles can be created using: i) a simple average of all models, ii) weighted average, in which models` suitability is weighted by how well that algorithm performed and iii) superior average, in which a simple average is calculated only for those algorithms that performed better than the average of all algorithms. PCA-based ensemble performs a principal components analysis on suitability maps and uses the first component as the final map, this can be performed: i) using all models, ii) using only the superior models, selected similarly to the superior average, and iii) principal components are calculated using only suitability values above the threshold for each algorithm, values below the threshold are set to zero (Table 9).</p>
        <p>There is an underlying difference between ecological niche models (ENMs) and species distribution models (SDMs), being that both the niche and the distribution are more suitable to answer different questions (Peterson &amp; Soberón, 2012). Usually, models' output represents the niche (ENMs), being that methods that bring ENMs closer to SDMs, called here MSDM, is a topic lightly treated on species distribution (Mendes et al., in prep). MSDM procedures are grouped in two approaches, a priori and a posteriori methods. The first set of techniques creates geographic variables that are incorporated as predictors for ENMs fitting (Allouche et al., 2008). The second set of methods constrains generated species suitability patterns using estimates of site accessibility, not being included as predictors while fitting models (Mendes et al., in prep).There is an underlying difference between ecological niche models (ENMs) and species distribution models (SDMs), being that both the niche and the distribution are more suitable to answer different questions (Peterson &amp; Soberón, 2012). Usually, models' output represents the niche (ENMs), being that methods that bring ENMs closer to SDMs, called here MSDM, is a topic lightly treated on species distribution (Mendes et al., in prep). MSDM procedures are grouped in two approaches, a priori and a posteriori methods. The first set of techniques creates geographic variables that are incorporated as predictors for ENMs fitting (Allouche et al., 2008). The second set of methods constrains generated species suitability patterns using estimates of site accessibility, not being included as predictors while fitting models (Mendes et al., in prep).</p>
        <p>Arguments involved: (cores)Arguments involved: (cores)</p>
        <p>The 
            <rs type="software">ENMTML</rs> package has the option to fit models using parallel processing, which accelerates the process. However, as this is computation-intensive, we chose to leave it open for users to decide the number of computer cores allocated for fitting ENMs. If the users do not specify the number of cores, only a single core will be used.
        </p>
        <p>Arguments involved: (save_part / save_final)Arguments involved: (save_part / save_final)</p>
        <p>There are several possible outputs for a single run of the ENMTML package. All the outputs produced by the fitting process are within a Result folder, which is created at the same level as the Predictors folders (Figure 2). Within the Result folder, there is a sub-folder namedThere are several possible outputs for a single run of the ENMTML package. All the outputs produced by the fitting process are within a Result folder, which is created at the same level as the Predictors folders (Figure 2). Within the Result folder, there is a sub-folder named</p>
        <p>Algorithm that contains the suitability and binary maps produced for each algorithm for each species. If the user chose to create ensemble models, there is another subfolder named 
            <rs type="software">Ensemble</rs>, with the combined maps created for each ensemble type chosen by the user. If the user chose to perform projections to different geographical regions or time periods there will also be a sub-folder named Projection, within which are the sub-folders for each projection scenario, with contains suitability maps generated for all the algorithms and the ensemble of those algorithms, if the user-specified an ensemble method. Users can control if partial and final models will be saved, altering the arguments save_part and save_final (TRUE/FALSE).
        </p>
        <p>Files generated at the pre-processing stage are also within the Results folder. Accessible area masks for each species are found within the Extent_Masks sub-folder. Masks used to constrain pseudo-absence allocation are also saved within Results, i.e., if the user chose to restrict pseudo-absences allocation using an environmental constraint, there will be a subfolder named Env_Constrain which indicates valid areas for pseudo-absence allocation.Files generated at the pre-processing stage are also within the Results folder. Accessible area masks for each species are found within the Extent_Masks sub-folder. Masks used to constrain pseudo-absence allocation are also saved within Results, i.e., if the user chose to restrict pseudo-absences allocation using an environmental constraint, there will be a subfolder named Env_Constrain which indicates valid areas for pseudo-absence allocation.</p>
        <p>Finally, if the user chose to perform a geographical partition of the occurrence dataset, there will be a corresponding sub-folder named BLOCK or BANDS, with the areas used to delimit each occurrence subset.Finally, if the user chose to perform a geographical partition of the occurrence dataset, there will be a corresponding sub-folder named BLOCK or BANDS, with the areas used to delimit each occurrence subset.</p>
        <p>Other than the folders, there is also a series of TXT (tab-delimited) files within the ResultsOther than the folders, there is also a series of TXT (tab-delimited) files within the Results</p>
        <p>We used the ENMTML package to fit current and future distribution for five virtual species.We used the ENMTML package to fit current and future distribution for five virtual species.</p>
        <p>We only present here the results produced for a single species (full models' outputs can be found in Appendix A). For this example, we used five bioclimatic variables (bio1, bio3, bio4, bio12, and bio15) from the WorldClim database v2.0 (https://www.worldclim.org). We projected the models to 2080 climatic conditions with a Representative Concentration Pathway (RCP) of 8.5. We used the MOHC HadGEM2-ES model and the same bioclimatic variables used in current conditions sourced by GCM Downscaled Data Portal (http://ccafsclimate.org). Current and future variables had ten arcmins of resolution. We performed a Principal Component Analysis (PCA) in the environmental data in order to reduce predictors collinearity (see the details of this procedure in the Methods sub-section "Predictors input and collinearity reduction"). We employed Support Vector Machine (SVM), Random Forests (RDF), and Maximum Entropy with default tuning (MXD) as algorithms. We used an equal number of absences and presences (i.e., presences/absences ratio equal to 1), which were randomly allocated within a calibration area (i.e., species accessible area) delimited by a buffer of 500 km around the presences. Models were validated by spatial block crossvalidation. For the current condition we constrained the models using the method MCP-B (see Methods sub-section "Methods to constrain ENMs") with a buffer of 200 km around the MCP. Final models were constructed by ensembling all the algorithms with a PCA (see details in Methods sub-section "Ensemble methods"). We calculated models' extrapolation for current and future conditions based on Mobility-Oriented Parity (MOP) metric. The total time used for fitting and processing the models of five species employing four cores was 2.545 minutes.We only present here the results produced for a single species (full models' outputs can be found in Appendix A). For this example, we used five bioclimatic variables (bio1, bio3, bio4, bio12, and bio15) from the WorldClim database v2.0 (https://www.worldclim.org). We projected the models to 2080 climatic conditions with a Representative Concentration Pathway (RCP) of 8.5. We used the MOHC HadGEM2-ES model and the same bioclimatic variables used in current conditions sourced by GCM Downscaled Data Portal (http://ccafsclimate.org). Current and future variables had ten arcmins of resolution. We performed a Principal Component Analysis (PCA) in the environmental data in order to reduce predictors collinearity (see the details of this procedure in the Methods sub-section "Predictors input and collinearity reduction"). We employed Support Vector Machine (SVM), Random Forests (RDF), and Maximum Entropy with default tuning (MXD) as algorithms. We used an equal number of absences and presences (i.e., presences/absences ratio equal to 1), which were randomly allocated within a calibration area (i.e., species accessible area) delimited by a buffer of 500 km around the presences. Models were validated by spatial block crossvalidation. For the current condition we constrained the models using the method MCP-B (see Methods sub-section "Methods to constrain ENMs") with a buffer of 200 km around the MCP. Final models were constructed by ensembling all the algorithms with a PCA (see details in Methods sub-section "Ensemble methods"). We calculated models' extrapolation for current and future conditions based on Mobility-Oriented Parity (MOP) metric. The total time used for fitting and processing the models of five species employing four cores was 2.545 minutes.</p>
        <p>All these procedures are expressed in R command line below:All these procedures are expressed in R command line below:</p>
        <p>We present the release of the 
            <rs type="software">ENMTML</rs> package, but we already have in mind ideas for future implementations. As the main objective of the package is to approach complex methodological developments to people that rely on ENMs but do not focus the development of new methods and are not comfortable using 
            <rs type="software">R</rs>, in the next update we expect to launch a web platform using 
            <rs type="software">Shiny</rs>. On the other hand, we also believe that ENMTML package might be of great use for the whole ENMs' community, as it centers on methodological developments scattered around the literature, and not always implemented in 
            <rs type="software">R</rs>, in one single location. With that in mind, we also look forward to providing further options for people who are interested in the fine-tuning of models. One of the first additions already planned is the possibility for users to change algorithms parameters. In addition, we also plan to explore indepth the ensemble field and include more ensemble alternatives and uncertainty maps.
        </p>
        <p>Finally, we believe an important aspect of ENMs is to be clear about model uncertainty;Finally, we believe an important aspect of ENMs is to be clear about model uncertainty;</p>
        <p>therefore, in the upcoming update, we will implement metrics to calculate source of uncertainty for each species in a way similar to Watling et al. (2015). Other than the already planned improvements, users can expect novel methodological approaches published in the literature to be implemented in the future versions of the package and are welcome to contribute with the development of the package and suggest new features.therefore, in the upcoming update, we will implement metrics to calculate source of uncertainty for each species in a way similar to Watling et al. (2015). Other than the already planned improvements, users can expect novel methodological approaches published in the literature to be implemented in the future versions of the package and are welcome to contribute with the development of the package and suggest new features.</p>
        <p>We like to use this chance to thank the many people who helped us by testing the package throughout its development. There is not enough space to name them all, but we are greatlyWe like to use this chance to thank the many people who helped us by testing the package throughout its development. There is not enough space to name them all, but we are greatly</p>
        <p>clustercluster</p>
        <p>Senay et al. (2013)Senay et al. (2013)</p>
        <p>There are several 
            <rs type="software">R</rs> packages to fit ENMs. We performed a literature search and found seven alternatives: 
            <rs type="software">biomod</rs> (Thuiller et al., 2009), 
            <rs type="software">ModEco</rs> (Guo &amp; Liu, 2010), 
            <rs type="software">sdm</rs> (Naimi &amp; Araújo, 2016), 
            <rs type="software">Model-R</rs> (Sánchez-Tapia et al., 2018), 
            <rs type="software">Wallace</rs> (Kass et al., 2018), 
            <rs type="software">ZOON</rs> (Golding et al., 2018), and 
            <rs type="software">kuenm</rs> (Cobos et al., 2019). We summarize those packages in a table, highlighting each package features and contrasting them with the features available at ENMTML (Table 10). Most packages focus on the development of a specific aspect of the modeling process, e.g., the package 
            <rs type="software">biomod</rs> was proposed as a platform for creating ensemble models, while the package 
            <rs type="software">kuenm</rs> is heavily focused towards accurately developing Maxent models; therefore a crucial aspect of software/package selection lies on the study objective.
        </p>
        <p>We introduce the package ENMTML, which proposes to integrate complex methodological developments in the ENMs' field, published from several different sources, in a single package and make them visible for users, which are not accustomed to the methodological details of ENMs. Our secondary objective was to make the package user-friendly, even for people not comfortable with the programming environment; therefore, we summarized the whole process into one single function with arguments that must be filled by the user according to the study objectives. We covered the majority of the ENMs process, from preprocessing occurrences and predictors to post-processing suitability models into ensembles or MSDM and provided several methodological alternatives to the different modeling steps (Table 10).We introduce the package ENMTML, which proposes to integrate complex methodological developments in the ENMs' field, published from several different sources, in a single package and make them visible for users, which are not accustomed to the methodological details of ENMs. Our secondary objective was to make the package user-friendly, even for people not comfortable with the programming environment; therefore, we summarized the whole process into one single function with arguments that must be filled by the user according to the study objectives. We covered the majority of the ENMs process, from preprocessing occurrences and predictors to post-processing suitability models into ensembles or MSDM and provided several methodological alternatives to the different modeling steps (Table 10).</p>
        <p>• We present ENMTML, an open source 
            <rs type="software">R</rs> package to fit ecological niche models (ENMs) • The package covers a wide variety of methodological aspects gathered from several studies • Complex methodological features, which were not readily available in 
            <rs type="software">R</rs>, are now easily accessible to users • We condense all this complexity in a single function to make it easier for users to follow a workflow • We demonstrate an example of fitting models for four species with complex methodological choices and its interactions
        </p>
        <p>☒ The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.☒ The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
        <p>☐The authors declare the following financial interests/personal relationships which may be considered as potential competing interests:☐The authors declare the following financial interests/personal relationships which may be considered as potential competing interests:</p>
    </text>
</tei>
