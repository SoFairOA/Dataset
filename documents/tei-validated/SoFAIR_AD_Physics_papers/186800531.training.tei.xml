<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
<teiHeader>
<fileDesc xml:id="_1"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-25T07:02+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text xml:lang="en">
<body>
<p>RGB-D cameras, also known as range imaging cameras, are a recent generation of sensors. As they are suitable for measuring distances to objects at high frame rate, such sensors are increasingly used for 3D acquisitions, and more generally for applications in robotics or computer vision. This kind of sensors became popular especially since the Kinect v1 (Microsoft) arrived on the market in November 2010. In July 2014, Windows has released a new sensor, the Kinect for Windows v2 sensor, based on another technology as its first device. However, due to its initial development for video games, the quality assessment of this new device for 3D modelling represents a major investigation axis. In this paper first experiences with Kinect v2 sensor are related, and the ability of close range 3D modelling is investigated. For this purpose, error sources on output data as well as a calibration approach are presented.</p>
<p>Capturing three dimensional environments and objects is a necessary task for many applications in different fields. This is nowadays widely made possible thanks to various technologies such as laser scanners, stereo-vision or triangulation-based systems for example. Regarding some aspects like price or computation time, the recent RGB-D cameras offer new possibilities for the modelling of complex structures particularly in cultural heritage documentation. This type of sensors presents several benefits demanded initially by the gaming and entertainment industry. They are easy to use and some of them are low-cost compared with laser scanners. Regarding these advantages, their use was extended to many other areas such as 3D scene reconstructions or pattern recognition (Kolb et al., 2009) as well as in the robotic field (May et al., 2006). Kinect v1 and Kinect v2 are based on two different imaging principles. As the accuracy of Kinect v1 limits its use for some engineering measurement tasks, Kinect v2 will probably give better results considering the new technology.</p>
<p>The aim of this paper is first to present the new sensor and its functionalities. In a second part, several tests are carried out and a calibration approach is proposed in order to improve output data. This will highlight not only the errors related to environment and captured object but also errors related to the system itself. Finally, the benefits of the new sensor are listed.</p>
<p>Even though Kinect for Windows v2 sensor relies on a different technology than Kinect v1, it also allows the acquisition of three different output streams. It is composed of two cameras, namely a RGB and an infrared (IR) camera, from which just one can be seen on the left side of the sensor (Figure 1). The active illumination of the observed scene is insured by three IR projectors.</p>
<p>Figure 1. Kinect v2 sensor on a photographic tripod</p>
<p>The RGB camera captures color information with a resolution of 1920x1080 pixels, whereas the IR camera is used for the real-time acquisition of depthmaps and also IR data with a 512x424 pixels resolution. The whole acquisitions can be carried out with a framerate up to 30 Hz. A last feature to be mentioned is the field of view for depth sensing of 70 degrees horizontally and 60 degrees vertically.</p>
<p>The technical specifications provided by Microsoft announce an operative measurement range from 0,5 m to 4,5 m. This last characteristic will be verified through different experiments presented in this paper.</p>
<p>To enable the use of the sensor for developers and researchers, the official <rs xml:id="12970087" type="publisher" corresp="12970088">Microsoft</rs> <rs xml:id="12970088" type="software">SDK</rs> <rs xml:id="12970089" type="version" corresp="12970088">2.0</rs> (<rs xml:id="12970090" type="software">Software Development Kit</rs>) is free downloadable. It provides not only the drivers, but also a set of functions or code samples that can be used for own implementations. One should also note the introduction of the <rs xml:id="12970091" type="software" subtype="component" corresp="12970092">Kinect Fusion</rs> tool in this <rs xml:id="12970092" type="software" subtype="environment">SDK</rs>, which was initially released by <rs xml:id="12970093" type="publisher" corresp="12970092">Microsoft</rs> for the Kinect v1 because of the success of its device. This tool enables thanks to ICP algorithms the direct creation of 3D meshes under different formats (.stl, .obj or .ply) by using the Kinect as a scanning device moved slowly around the object.</p>
<p>As mentioned on Microsoft website and on a Canesta patent, the Kinect v2 sensor is based on time-of-flight principle, whereas the previous Kinect device uses structured light to reconstruct the third dimension. Even though time-of-flight range imaging is a recent technology, many books deal with its principles and its applications (Grzegorzek et al., 2013, Hansard et al., 2012, Kolb and Koch, 2009, Remondino and Stoppa, 2013). Moreover in the last decade, a few companies developed range imaging cameras based on this measurement principle, which can be direct or indirect. The distinction between these two different measurement types is based on the light source used, either light pulses (direct ToF) or amplitude modulated light (indirect ToF). Nevertheless, the basic principle stays unchanged: knowing the speed of light, the distance to be measured is proportional to the time needed by the active illumination source to travel from emitter to target.</p>
<p>As Canesta made use of indirect time-of-flight before Kinect v2 sensor development, we can assume that this methodology also appears in the new sensor investigated here. Indeed, direct timeof-flight requires very costly clocks to accurately determine the runtime of single pulses. Therefore it seems unlikely that such a component be part of a low-cost sensor. For indirect time-offlight systems based on light modulation, a phase shift between emitted and received signal is measured, replacing the direct measurement of runtime. In that case, the estimated distance between sensor and captured object depends on the determined phase shift ∆φ by equation (1) (Kolb and Koch, 2009).</p>
<p>(1) where f = modulation frequency c = speed of light It should be noticed that, unlike other time-of-flight sensors, it is impossible to act on the modulation frequency or on the integration time of the input parameters with Kinect v2 sensor.</p>
<p>Sensors based on the digital imaging technology introduced above deliver a measurement of the distances between an entire scene and the sensor. These distances values are directly stored in a matrix which size corresponds to the depth sensor resolution. Indeed, for each pixel of the 512x424 depth images, the measuring device estimates in real-time a distance value to the corresponding object point. This output data is the so-called depthmap (see an example Figure 2), a two-dimensional image from which the 3D-coordinates of the scene can be calculated. After a few post-processing steps, it is then possible to obtain indirectly point clouds of the captured scene or object.</p>
<p>Assuming that Kinect sensor uses a standard lens, camera intrinsic parameters such as focal length f and principal point coordinates (c x , c y ) can be determined. The Z-coordinate for a pixel is already known because it corresponds to the value stored in the depthmap. Using these parameters and the perspective projection relationship (see Dal Mutto et al., 2013), a 3D point X in the camera coordinate system can be mapped from the homogenous image coordinates of a pixel p = [u, v, 1] T through equation ( 2). As each pixel of the depthmap represents a value for the corresponding point cloud, the calculated point clouds count a number of 217088 points.</p>
<p>(2)</p>
<p>Because of the recent release of the Kinect v2 sensor, so far only a little amount of algorithms allows an off-the-shelf use of the sensor.</p>
<p>A good knowledge of sources of errors affecting the measurements of a system is needed to quantify the accuracy of the data provided by it. Even if a large number of these sources are interdependent, we can however try to classify them into four groups, as it is done for terrestrial laser scanners (Reshetyuk, 2009): instrumental errors, errors related to the scanned object, environmental errors and methodological errors.</p>
<p>Instrumental errors are usually systematic errors assigned to the system design. They should potentially be removed by system design improvement or by calibration. The environment in which the acquisitions are performed has an influence (temperature, brightness, humidity, etc.), as well as the characteristics of the observed object (e.g. albedo, reflectivity, geometry). Once the camera is on its tripod, the user intervention is limited to choosing settings (e.g. time interval, types of output data), therefore the automation level seems to be relatively high. However, errors can occur according to the distance-to-object chosen for the acquisition, the incidence angle, etc. The point cloud quality depends also strongly on the algorithms used for creating the output data (mapping functions, calibration parameters…). It is essential to determine the influence of these parameters on the quality of the distance measurement. To do that, a few tests are presented in this section.</p>
<p>Since individual frame acquisitions suffer from noise inherent to the sensor and its technology, averaging successive frames is a possible improvement step to overcome this phenomenon. This part deals with the influence of the number of acquisitions on the precision of the final measurement. Considering a static scene, several repeated measurements with 500 ms interval are carried out from one unique camera location. A statistical analysis over a large sample of 500 acquired depthmaps has been performed. We also analyzed the behavior observed with 20, 50, 100 and 200 successive depthmaps.</p>
<p>First, acquisitions have been made from an office scene which mainly contains objects of squared shape (Figure 3a). Using the acquired depthmaps, for each pixel the mean value and the standard deviation of distance measurements were calculated.</p>
<p>For this non planar scene, the computed standard deviations shown in Figure 3b are particularly wide at the objects boundaries, but also on reflective surfaces such as metallic parts. This phenomenon can be explained by the measurement noise which appears in these specific cases. It can also be associated to speckle noise present in infrared data. Moreover, considering the different sizes of samples, one should notice a low impact on the computed standard deviations from the 50 depthmaps. Indeed, for 20 successive frames the result seems to be less smooth than for the other samples sizes. However the effect observed at objects borders does not change a lot between the 50 and 500 samples. While frames are added, the main differences arise at far ranges (from about 3 meters), where depth inhomogeneities are reduced and the visual results smoothed. We will see in section 4 that far range measurements are not reliable. For this reason, they will not be treated and considering 50 successive depthmaps seems to be enough.</p>
<p>Secondly, the same test was repeated by positioning the Kinect device in front of a planar wall, at a distance of approximately 1,15 m. Histograms of the distance measurements performed for a central area of one pixel and its neighbors (3 x 3 pixels) were realized for the different samples mentioned before. Figure 4 shows the result obtained with 100 successive measurements. In that case, the standard deviation is lower than 1 mm (approximately 0,7 mm). It can be noticed that this value is almost constant for all sizes of samples we considered. In conclusion, as for the experiment presented above, a larger sample does not provide a better precision. The future tests will thus be carried out with a smaller amount of acquisitions.</p>
<p>Previous studies have shown that some RGB-D sensors need a pre-heating time before providing reliable range measurements (Mittet et al., 2013). For checking this phenomenon, the time delay necessary for Kinect v2 to achieve constant measurements has been determined. The sensor was placed parallel to a white planar wall and measurements were carried out each 15 seconds during one and a half hour (360 depthmaps recorded). For each recorded depthmap, the distance from the sensor to the wall was determined in a central area of ten per ten pixels. This area was chosen according to the maximal intensities in the corresponding infrared data.</p>
<p>This test permits to establish the value of distance measurements as a function of time. Figure 5 shows that the distance varies from 5 mm up to 30 minutes and becomes then almost constant (more or less 1 mm). It can be noticed that the sensor's ventilator starts after 20 minutes. For future tests, the pre-heating time will be respected even if the distance variation is quite low.</p>
<p>Figure 5. Value of distance measurements as a function of time</p>
<p>To assess the effects of different materials on intensity as well as on distance measurements, samples characterized by different albedos and roughness were measured. The sensor was placed parallel to the plane regrouping the samples (Figure 6a). As shown in Figure 6b and Figure 7, reflective and dark materials stand out among all the samples. Indeed, the intensity provided for very reflective as well as very dark surfaces is particularly low. Thus, the corresponding distances in the depthmaps are larger than expected. This effect in the depthmap is mainly important for a compact disk, i.e. high reflective material as illustrated in Figure 6b. For this sample, distances vary up to 6 cm. It is also visible in Figure 7, which presents a checkerboard of grey levels and the corresponding intensity image. The intensity increases with the brightness of the object (except when it is also very reflective or transparent). On the white parts of this checkerboard, a speckle phenomenon appears and illustrates the measurement noise. Concerning the level of roughness of the grainy textures on the test panel in Figure 6a, their influence is not clearly visible because of the noise inherent to the point cloud.</p>
<p>As the previous Kinect was not adapted for sunny outdoor acquisitions, the influence of the brightness conditions on the measurements was also studied. Outdoor acquisitions were performed during a sunny day.</p>
<p>This experiment shows that the sensor is able to work during a sunny day provided that the light does not directly illuminate the sensor. Indeed, strong backlighting conditions cause sensor's disconnections from the computer.</p>
<p>Considering the quality of the acquisitions carried out during a sunny day, two phenomena appear: the number of "flying pixels" increases particularly on the edges of the sensor field of view and the number of points decreases with the light intensity.</p>
<p>The "flying pixels" effect is shown Figure 8.</p>
<p>As the lenses used in time-of-flight cameras are standard, the acquired data suffer from distortion effects that can be estimated and corrected through successive calibration steps. Since we hope to use the Kinect v2 sensor as a measuring device, calibration should enable the acquisition of reliable 3D metric information from the acquired point clouds. Because of the specificity of range imaging cameras, the distinction between two successive approaches has to be made.</p>
<p>It appears in lots of works e.g. (Hansard et al., 2014) that timeof-flight cameras can be geometrically calibrated with standard methods. For this purpose, as for common 2D sensors, several images of a planar checkerboard were taken under different points of view. It is worth noting that the infrared data were used to handle this geometric calibration, because depth and infrared output streams result from the same sensor.</p>
<p>To determine the necessary intrinsic parameters, our dataset was treated by a
<rs xml:id="12970094" type="software" subtype="component" corresp="12970096">Camera Calibration Toolbox</rs> proposed by (<rs xml:id="12970095" type="publisher" subtype="person" corresp="12970096">Bouguet, 2013</rs>) under the
<rs xml:id="12970096" type="software" subtype="environment">Matlab</rs> software. This algorithm is mainly based on well-known camera models. However, one should underline the fact that changing or removing some of the images makes the computed results vary from a few pixels. This phenomenon is only due to the low sensor resolution. As a matter of fact, best calibration results are considered regarding the lower uncertainties on the parameters.
</p>
<p>Note that the <rs xml:id="12970097" type="publisher" corresp="12970098">Microsoft</rs> <rs xml:id="12970098" type="software" subtype="environment">SDK</rs> mentioned earlier also provides a function that returns all these intrinsic parameters (namely <rs xml:id="12970099" type="software" subtype="component" corresp="12970098">GetDepthCameraIntrinsics()</rs> function). Deviations towards selfcomputed results are shown in Table 9.</p>
<p>Values (pixels) Std.</p>
<p>Focal length (x)</p>
<p>Distortion parameters (radial, then tangential)</p>
<p>Table 9. Estimated intrinsic parameters using self-calibration, with Microsoft function, and related deviations</p>
<p>The application of these distortion coefficients directly on our point clouds computation algorithm allows the gaining of calibrated three dimensional dataset, which is no more affected by the actual distortions of the initial depthmap. It can also be noticed that the point clouds obtained by applying directly the Microsoft mapping functions seem to be pre-calibrated at least for radial distortions of the lens. Indeed, considering acquisitions of a planar surface, the results are visually similar between a point cloud computed through our method with corrections and a point cloud directly obtained with mapping functions. This phenomenon appears especially on the corners of the captured plane, which are not right angles but rather distorted. As a matter of fact, the intrinsic parameters stored in the device are certainly considered in the point clouds mapping functions.</p>
<p>Moreover, a geometric calibration could also be processed for the RGB sensor, which uses a different lens with a different resolution. However, as we do not need the colorimetric information for the rest of the experiments, this calibration has not yet been performed.</p>
<p>A systematic depth-related deformation, also depicted as wiggling error by some authors (e.g. Lindner et al., 2010) can be observed while working with time-of-flight devices. This deformation is due in part to inhomogeneities in modulation process. Lots of works have been devoted to the understanding of error sources in order to minimize their effects. As a matter of fact, calibration of time-of-flight sensors was a major issue in many studies in the last decade. Most of the time, the introduced approaches are based either on Look-Up Tables (Kahlmann et al., 2006) or on curve approximations with Bsplines (Lindner et al., 2010). In both cases, the aim is the storage of depth errors depending on the measured distance.</p>
<p>A common way to assess the distance inhomogeneity consists on positioning the camera parallel to a white planar wall at different well-known distances. In our study, the wall has been surveyed beforehand with a laser scanner (FARO Focus3D) in order to assess his planarity with a device of assumed higher accuracy as the investigated sensor. After that, a line has been implanted by tachometry perpendicularly to the wall. Marks have then been fixed on this line at predetermined distances, from which the acquisitions were made with the Kinect placed on a static tripod, as shown in Figure 10. The marks were implanted at ranges from 1m up to 6m from the reference wall, with 25 cm steps to progressively move the sensor away from this wall. An additional position at 0,8m was also set up, even though Microsoft announces a minimal range of 0,5 m. Indeed, measurements acquired at a range of 0,8 m already present a high standard deviation compared to other ranges, as can be seen on Figure 11. This phenomenon will be more important for measurements at a closer range. Moreover to limit the influence of noise, at each sensor position 50 successive depthmaps of the wall were acquired with intervals of one second. Finally, to insure the parallelism between sensor and wall and thus reduce the possible rotation effect, the distances were also surveyed by tachometry at each new position, using little prisms at the two sensor extremities.</p>
<p>Inspired by the work of (Lindner &amp; Kolb, 2006), a two steps distance calibration based on B-splines approximation was carried out. Firstly, the accuracy of distance measurement for a small central matrix of 10x10 pixels has been investigated. Secondly, the deformations extended to the whole sensor array have been assessed, in order to highlight the need of processing a potential per-pixel correction.</p>
<p>First aim of this experiment was to determine the deviations between real and measured distances as a function of the depth, to produce the result presented in Figure 11. As depicted in this graph, distortions of the averaged central area vary from about 0,5 cm to 2,7 cm. Knowing the curve parameters allows the correction of the measured distances. Another interesting information arises from the observation of the standard deviations calculated over the 50 depthmaps acquired per station. As it can be seen, the standard deviation increases with the range, which means that the measurements become more scattered around the estimated distance when the sensor moves away from the scene. Moreover, at the nearest range of 0,8 m acquired in this test, the standard deviation is higher than for all other sensor positions. As a matter of fact, measurements realized at the minimal announced distance of 0,5 m would probably be still less reliable. Secondly, an important variation of about 1,5 cm at range 4,5 m is observed. This observation confirms the upper value for distance measurement reliability given by Microsoft. The future acquisitions will be limited to this suitable range, observing that in this case the distance deviations are reduced to an interval of about 1 cm.</p>
<p>Looking at this graph, it can also be noticed that despite satisfying results all deviations are positive, meaning that the sensor would systematically measure too short distances. It seems that a systematic offset occurs because of the constant distance (approximately 2 cm) between fixing screw and lens. Indeed, the Kinect was accurately placed on the tripod with respect to its fixing screw, which definitely does not correspond to the optical center of the lens.</p>
<p>In order to eliminate this systematic effect, we suggest subtracting the values of the nearest range (0,8 m) to the values of each respective position. Thus, observing calculated intervals with respect to a first station also affected by the systematic offset provides values no more depending on the unknown misalignment. The realized graph presents the same trend curve as Figure 11, but with values varying between about 5 and -5 mm for ranges from 0,8 to 4,5 m.</p>
<p>Finally, plotting the measured distances over the real distances provides a very high correlation coefficient of r = 0,999.</p>
<p>According to the measurement task, it is possible to neglect the depth calibration step.</p>
<p>The second part of depth calibration consists on an extension of the analysis to the whole sensor array. For this purpose, for datasets of the planar wall acquired before at the ten first positions (from 0,80 m to 3 m), fitting planes were computed by means of least-squares adjustment. To fit correct planes in the Kinect point clouds, some outlying points corresponding to null values in the depthmaps and thus very far from the wall were not considered. Then, the Euclidean distances between a point cloud and its corresponding plane are calculated. It provides a residuals matrix which represents for each pixel of the considered depthmap the deviation between approximating plane and actual measured distance. This representation is shown in Figure 12a for a distance of 1,25 meters with deviations in millimeters. In addition, the histogram of the residuals obtained for this range is also presented (Figure 12b). It appears that the values are normally distributed and almost centered on the null value, with a standard deviation of about 4 mm. It can be mentioned that the standard deviations calculated for the fitted planes increase with the range from about 3 mm up to 16 mm for the latest range of 3 meters.</p>
<p>Moreover, as it can be seen in Figure 12a, the deviations with respect to the adjusted plane are more important at image boundaries and especially on the corners. As known from the literature, these radial effects increase with the distance to the wall. In our case, they can reach variations of a few centimeters up to tens of centimeters in the corners at 3 meters range. Because of this loss of precision and reliability of the measurements at the boundaries, a first approach to reduce depth errors of entire sensor consists on considering only a restricted central array. Tests were realized with a reduced depthmap size of approximately 360x300 pixels from the 512x424 initial frame. Obviously, the area was chosen very central to remove border effects. The same phenomenon can always be detected at the boundaries of the new considered area but with widely reduced residuals. They reach a few millimeters at the range of 1,25 m. In addition, the standard deviations calculated for the fitted planes on the shorter areas were reduced from about 2 to 5 mm.</p>
<p>Nevertheless, a correction approach for the entire sensor area as proposed in (Pagliari et al., 2014) can be adapted to the device investigated here. It consists on delivering signed corrections interpolated from the residuals matrices computed at different ranges. For each pixel of the array, given the distance value it contains, a correction value will be interpolated considering the nearest ranges for which residuals were computed. The obtained correction matrix then just needs to be added to the initial depthmap in order to apply the per-pixel correction.</p>
<p>A two-way conclusion arises from these observations. On the one hand, using the inspected sensor for high accuracy metrology tasks justifies the need of a per pixel correction managed over the whole sensor. Nevertheless, this correction step is time-consuming due to the local treatment of each pixel. Thus, on the other hand, people who foresee to use the Kinect for Windows v2 for lower-accuracy object reconstruction should locate their object on a central and restricted area of the sensor.</p>
<p>Thanks to the change of technology carried out by Microsoft for the release of its recent Kinect for Windows v2 sensor, the depth measurements achieved are more accurate compared to the first Kinect device. In addition, the computed point clouds present a better resolution because a distance measurement occurs for each pixel of the captured depthmaps. This allows a more accurate detection of small objects for example. The quality of color images has been also increased. These are the two main technical advantages of Kinect v2 compared to Kinect v1.</p>
<p>Tests about the influence of measurements repetition have shown that the averaging procedure has not necessarily a great influence on the final accuracy of the measurements. Indeed, the noise inherent to the sensor is already reduced in comparison to the first Kinect device, and is essentially present at objects boundaries where artifacts are hardly avoidable. Moreover, it has been shown that outdoor acquisitions can be envisaged. Kinect v2 device seems to be much less sensitive to daylight since point clouds were obtained even on a very sunny day. However, some filters need to be applied to overcome the increased noise effects and to reach exploitable results.</p>
<p>Another advantage is the simultaneous release of Kinect sensor and of its official <rs xml:id="12970100" type="software">SDK</rs>, which allows development of solutions for many tasks such as algorithms for 3D reconstruction.</p>
<p>Nonetheless, a few material drawbacks related to the use of the Kinect v2 sensor itself can be mentioned, such as the necessity of working under a Windows 8 or 8.1 station equipped with USB3 port. The required additional power input makes it also less mobile for SLAM applications for example.</p>
<p>To complete our current work, it would be interesting to check the potential of the Kinect v2 device for small object modelling purposes. In this context, point clouds of a limestone fragment (approximately 25 x 20 cm) have already been captured. The acquisitions were made under different points of view all around the stone. Visual results look very satisfying, as it can be seen on Figure 13. The multiple point clouds (eight different positions) of the object now need to be registered in order to obtain a complete 3D point cloud. A first approach will consist on a manual segmentation of each individual point cloud in order to remove the artefacts (or flying pixels) that are clearly visible at the edges and boundaries of the object. Once the final mesh of this object will be produced, an accuracy assessment will be carried out thanks to comparisons with reference meshes acquired by other imaging techniques. In a second time, tests will be performed using the Kinect Fusion tool mentioned before. It enables to directly acquire a meshed representation of the investigated object. This approach has the advantage to use much less point clouds as the manual approach and is a faster modelling method. However the model accuracy achieved through this method must be assessed. For sure, it depends on the way the Kinect is moved around the object.</p>
<p>The aim of this paper was to investigate through a set of tests the capacity of the Kinect v2 sensor, a low cost sensor, and to study if it could be an alternative to close range laser scanners for 3D measurements.</p>
<p>Several tests highlighted errors related to environment and to the properties of the captured object but also errors related to the system itself. Moreover, geometric as well as depth calibrations were carried out. Based on the obtained results, the physical limitations of the device could be assessed.</p>
<p>Considering depth measurement precision and outdoor efficiency, the achieved results look promising. Accordingly, the arrival of the second Kinect version can be seen as a real progress for computer vision tasks dealing with range imaging cameras.</p>
<p>In the future, the potential of the Kinect v2 device for small object modelling purposes will be assessed in a qualitative and a quantitative way.</p>
</body>
</text>
</tei>