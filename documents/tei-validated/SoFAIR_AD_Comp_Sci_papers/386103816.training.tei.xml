<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
<teiHeader>
<fileDesc xml:id="_1"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T11:23+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text xml:lang="en">
<p>With the development of deep connections between humans and Artificial Intelligence voice-based assistants (VAs), human and machine relationships have transformed.</p>
<p>For relationships to work it is essential for trust to be established. Although the capabilities of VAs offer retailers and consumers enhanced opportunities, building trust with machines is inherently challenging. In this paper, we propose integrating Human-Computer Interaction Theories and Para-Social Relationship Theory to develop insight into how trust and attitudes toward VAs are established. By adopting a mixedmethod approach, first, we quantitatively examine the proposed model using Covariance-Based Structural Equation Modeling on 466 respondents; based on the findings of this study, a second qualitative study is employed to reveal four main themes. Findings show that while functional elements drive users' attitude toward using VAs, the social attributes, being social presence and social cognition, are the unique antecedents for developing trust. Additionally, the research illustrates a peculiar dynamic between privacy and trust and it shows how users distinguish two different sources of trustworthiness in their interactions with VAs, identifying the brand producers as the data collector. Taken together, these results reinforce the idea that individuals interact with VAs treating them as social entities and employing human social rules, thus supporting the adoption of a para-social perspective.</p>
<p>academics, research on how trust develops between consumers and VAs remains scant (Foehr &amp; Germelmann, 2020), and an important question emerges: what are the factors that foster trust development in interactions with VAs?</p>
<p>It is apparent that the regular utilization of Artificial Intelligence (AI) technology and their human-like qualities have changed the way people perceive and interact with them. When using mobile devices or laptops, it is rare that one would refer to them as "he" or "she," yet when it comes to VAs on these devices, such as <rs xml:id="12896693" type="software">Siri</rs> and <rs xml:id="12896694" type="software">Alexa</rs>, personification occurs. Despite VAs not possessing any physical human qualities, the voice alone is enough for humans to develop a deeper connection to the technology (Han &amp; Yang, 2018;Novak &amp; Hoffman, 2019;Schweitzer et al., 2019).</p>
<p>Deep connections can bring about para-social relationships (PSRs). Traditionally, these relationships may occur between a celebrity and a fan but recent research has begun to investigate this with technology. For example, the mobile phone is considered an extension of oneself and has therefore created an inseparable bond between humans and their phones (Melumad &amp; Pham, 2020). Since the emergence of VAs, conversations between humans and machines have given rise to higher levels of engagement whereby more natural conversations occur between them (Guzman, 2019;Ki et al., 2020).</p>
<p>For example, recent developments of this technology allow VAs to display emotional responses by mimicking intonations of human speech, thus sounding more "human" (Schwartz, 2019). As per the Social Response Theory (SRT; Nass &amp; Moon, 2000), the element of reciprocity has developed these human-machine bonds to a greater extent (Cerekovic et al., 2017), having consumers not only using these devices but also developing them with various types of relationships (Han &amp; Yang, 2018;Schweitzer et al., 2019).</p>
<p>Trust is recognized as a strong determinant of technology adoption and use (van Pinxteren et al., 2019;Wirtz et al., 2018) and has the power to reduce levels of perceived risk surrounding an interaction, thus facilitating consumers' intentions and behaviors (Gefen &amp; Straub, 2004).</p>
<p>Research examining how users develop trust with technology are generally grounded in the Human-Computer Interaction (HCI) literature (Hassanein &amp; Head, 2007). These studies have identified the drivers of trust toward the functional (Lu et al., 2016), hedonic (Hwang &amp; Kim, 2007), and social (Gefen &amp; Straub, 2003;Ye et al., 2019) attributes of the technology. In addition, perceived privacy concerns have been found to have detrimental effects on individuals' trust formation (Chang et al., 2017;Zhou, 2011). This is particularly relevant in the interaction between humans and VAs as users are not naïve of privacy implications of using them (McLean &amp; Osei-Frimpong, 2019). Recent studies focusing on AI service robots identify trust as an important relational dimension, linked to service robots' acceptance (Wirtz et al., 2018), and argue that the level of anthropomorphism is an important driver of trust and intention to use them (van Pinxteren et al., 2019).</p>
<p>To date, studies examining VAs have mainly focused on the factors influencing their usage in daily life. Moriuchi (2019) Finally, Ki et al. (2020), explored the para-friendship relationships that can arise between individuals and virtual personal assistants. Among these, few research has explored the factors affecting users' trust in their interactions with VAs, with them adopting either an information system perspective (Nasirian et al., 2017) or a social response approach (Foehr &amp; Germelmann, 2020). Nevertheless, a comprehensive understanding of what nurtures users' trust toward</p>
<p>VAs has yet been fully developed.</p>
<p>By integrating HCI literature and PSR theory, this study aims to address this gap and investigates the drivers of consumers' trust and attitude in the interactions with VAs. As the relational elements of the interaction with these devices have been highlighted as one of the most important characteristic influencing users engagement with VAs (Han &amp; Yang, 2018), the study adopts a PSR theory approach (Horton &amp; Wohl, 1956;Turner, 1993) and explores the functional, emotional, and social factors influencing trust development and attitude toward VAs.</p>
<p>This study adopts a mixed-methodology applying a development mixed-method approach (Davis et al., 2011) that, first, includes a quantitative phase of research followed by qualitative in-depth studies.</p>
<p>The study directly responds to Wirtz et al. (2018) and Lu et al. (2020)'s calls to further explore consumers' interaction with AI agents and its contribution to the literature is threefold. First, findings of the two studies draw attention to the peculiar relation between privacy and trust development toward VAs, revealing how users distinguish two different sources of trustworthiness during a VAs interaction and direct their privacy concerns to the producer rather than the voice-based agent (Foehr &amp; Germelmann, 2020).</p>
<p>Second, it demonstrates the prominent role of social elements, namely social presence and social cognition, as unique antecedents for developing users' trust toward VAs (Čaić et al., 2019;van Doorn et al., 2017). While previous studies suggest that social attributes can improve consumers' trust in online settings, this effect has rarely been empirically examined toward consumers-VAs interactions.</p>
<p>Finally, the study shows the relevance of adopting a more integrated approach when examining interactions with AI technology, while providing support for new ways to understand how trust develops between VAs and consumers; this being through the identification of drivers incorporating the functional and hedonic attributes of the technology and individuals' perceived privacy risks, while including social and relational elements.</p>
<p>VAs are Internet-enabled devices that provide daily technical, administrative, and social assistance to their users, including activities from setting alarms and playing music to communicating with other users (Han &amp; Yang, 2018;Santos et al., 2016). They are traditionally used as mobile applications (e.g., <rs xml:id="12896695" type="publisher" corresp="12896696">Apple</rs> <rs xml:id="12896696" type="software">Siri</rs> and <rs xml:id="12896697" type="software">Google Now</rs>) and have, in more recent years, been extended to the home environment, whereby a separate device is set up alongside a mobile application (e.g.,
<!-- both are smart speakers --> Amazon Echo and Google Home). Recent developments of VAs software include the implementation of natural language processing; this allows VAs to engage in conversational-based communication whereby they not only respond to initial questions but are also capable of asking follow-up questions (Hoy, 2018). Thus, VAs can be used as functional tools for online shopping, learning, controlling other smart applications and devices as well as for relational benefits such as communications and companionship (Guzman, 2019;Schweitzer et al., 2019). This is particularly relevant when examining how trust develops toward VAs as potential antecedents can arise from both functional and relational perspectives.
</p>
<p>Literature investigating the adoption and use of VAs can be grouped into two main research streams. First, HCI literature examines consumers' behavioral intention to use VAs through the use of technology adoption theories (Moriuchi, 2019); these theories have been used considerably across e-commerce, m-commerce, and social media literature and have been adopted in AI and VAs studies, providing support for the influence of perceived usefulness (PU) and ease of use on individuals' adoption intentions. However, limitations in using such theories in their singularity have been made apparent, bringing about a second stream of literature examining VA adoption through PSR theories (Ki et al., 2020); this literature suggests that VA adoption is not only based on willingness to adopt technology but also due to the relationships built between consumers and voicebased (or "human-like") systems (e.g., Schweitzer et al., 2019). Another substream of HCI literature has examined the negative role of privacy concerns, and has been seen to be discussed in both main streams of research (e.g., McLean et al., 2020).</p>
<p>However, with endorsements and criticisms surrounding various theories and concepts, literature has called out for studies to combine these core streams to gain a clearer and more comprehensive investigation of consumer adoption of VAs. Research, so far, has investigated trust toward VAs solely from a technology-based (Nasirian et al., 2017) or a relational-based (Foehr &amp; Germelmann, 2020) perspective. Building on both these streams of research, this study combines the theoretical foundations of PSR with HCI theories and provides a more detailed perspective on the antecedents of trust development toward VAs that includes the relational dimensions of the interaction while accounting for their functional and hedonic attributes.</p>
<p>To examine users' adoption of new technologies, various TAMs have been developed and adapted over time. The TAM (Davis, 1989) has long been relied on by researchers to convey the importance of functional attributes of technology for them to be adopted by users. PU and Perceived Ease of Use (PEOU) are often found to be fundamental predictors of technology adoption across research settings (e.g., Wirtz et al., 2019). However, TAM has been regularly criticized in its contemporary application to technology adoption research in being outdated and lacking sufficient depth to explain the adoption of more complex technologies (Lim, 2018). As such, models such as the Unified Theory of Acceptance and Use of Technology (UTAUT; Venkatesh et al., 2003) and its extension (UTAUT2; Venkatesh et al., 2012) have been developed to add additional functional and hedonic antecedents of behavioral intention. Performance Expectancy and Effort Expectancy echo the nature of PU and PEOU and further confirms the role of the functional attributes on adoption intention.</p>
<p>Although TAM has been criticized in recent years, its utilization in this context is more appropriate than its successor models in that it incorporates attitude. Nevertheless, later models remain significant in this context as they incorporate enjoyment; enjoyment is a fundamental antecedent of technology adoption, and is considered even more significant when using technology for hedonic purposes, thus drawing attention to the need to incorporate hedonic motivation from UTUAT2 (Venkatesh et al., 2012).</p>
<p>Despite the evolution of such models, the unique characteristics of AI technologies require a broader perspective in the understanding of the motivations for adopting and using advanced technology. A significant conceptual framework is provided by Wirtz et al. (2018), who introduced the Service Robot Acceptance Model (sRAM). The model builds on TAM (Davis, 1989) and role theory (Solomon et al., 1985) and identifies the functional (i.e., PEOU, PU, subjective social norms), social (i.e., perceived interactivity, perceived social presence, and perceived humanness), and relational (i.e., trust and rapport) elements that drive users' acceptance and use of service robots. While the model provides useful theoretical foundations to understand AI service robots' acceptance and use, trust is only included as a relational need that may influence consumers' intention to use and what drives trust toward AI agents remain unexplored.</p>
<p>Literature in technology adoption has often examined the importance of social influence when users decide to adopt or reject new technologies; some of the earlier factors examining this focus on the notion of conformity (Nail, 1986), reciprocal influence (Stasser &amp; Davis, 1981;Tanford &amp; Penrod, 1984), and reciprocal caution (Bandura, 1989). Reciprocal caution explains that a person will only do something if they feel they have a level of control or trust over their situation.</p>
<p>As human beings, trust is a basic yet fundamental deep-routed psychological component of whether we engage in fight or flight behavior (Mayer et al., 1995). Therefore, people constantly subconsciously identify objects as being friends or foes; for example, a fire is hot and will hurt to touch so we learn to not touch it. Humans develop stronger associations of trust between other humans and connect with humans in a deeper way than with inanimate objects. With VAs being the first types of technology to display such humanlike features, being the voice, it is questionable whether users have developed a stronger bond with them, in recognizing them more as friends than foes, than with other technologies (Schweitzer et al., 2019;van Pinxteren et al., 2019).</p>
<p>PSR theory considers the interpersonal relationships between people and media characters (Horton &amp; Wohl, 1956;Turner, 1993).</p>
<p>Traditionally, this has been used to explain relationships between viewers/consumers and characters/celebrities. Recently, AI-based personal assistant-related work has addressed the significance of PSR Theory in explaining that users can develop a degree of closeness and intimacy with VAs when engaging in human-like interactions, which can result in users perceiving them as friends, subsequently resulting in a PSR (Louie et al., 2014;Sproull et al., 1996). Han and Yang (2018) argued that the social aspects of VAs, namely "interpersonal attraction," impacts users' satisfaction and intention to use these devices, providing that the more regular the interactions between VAs and their users the more the relationship will be expected to be interactive and socially enjoyable. This regularity of interactions helps explain why users are becoming comfortable with using VAs in their homes. Ki et al. (2020) explored para-relationships between individuals and VAs and found that the para-social presence of these devices influences users' selfdisclosure and social support toward VAs, which, in turn, leads to continue usage intentions.</p>
<p>These studies provide evidence that human-like features of AI agents make individuals perceive they are "socially present," resulting in them applying social norms when interacting with them (Nass &amp; Moon, 2000). Therefore, the more the technology displays humanlike characteristics, such as face, voice, or gender, the more these kinds of cues may trigger schemas associated with human-human interactions, including a sense of social presence (Chattaraman et al., 2019).</p>
<p>Where relationships become constrained, particularly with technology, is when users fear for their personal privacy. Privacy concerns are associated with the unauthorized collection, usage, or control of personal data (Malhotra et al., 2004) and in the digital landscape are generally divided into three main types, being (1) territorial privacy concerning the physical surrounding space; (2) privacy of a person against undue interference; and (3) information privacy of control of the gathering, storage, processing, and dissemination of personal data (Kokolakis, 2017).</p>
<p>Privacy has long been examined within a HCI perspective (e.g., Mothersbaugh et al., 2012;Nepomuceno et al., 2014) and has more recently been incorporated into AI literature (e.g., McLean &amp; Osei-Frimpong, 2019). Previous studies have analysed both antecedents and consequences of users' perceptions of privacy. It has been found that perceived vulnerability and ability to control (Bansal &amp; Gefen, 2010;Mohamed &amp; Ahmad, 2012), prior experience (Cho et al., 2010), Internet literacy (Dinev &amp; Hart, 2006), policy and regulations (Lwin et al., 2007), business communication (Lwin et al., 2016), and fairness of the technology (Pizzi &amp; Scarpi, 2020) are potential drivers of individuals' privacy concerns.</p>
<p>Previous research has examined how privacy concerns influence consumers' responses in a variety of settings, including online shopping (van Slyke et al., 2006), health online information (Bansal &amp; Gefen, 2010), social networking sites (Chang et al., 2017;Xu et al., 2013), retail technologies (Pizzi &amp; Scarpi, 2020), mobile communication and location-based services (Xu &amp; Gupta, 2009;Zhou &amp; Li, 2014). These studies provide evidence that privacy concerns can act as a negative antecedent of usage (e.g., Nepomuceno et al., 2014) or as a moderator (e.g., Brill et al., 2019). Specifically, it has been found that perceptions related to privacy directly affect individuals' behavior intention (Fogel &amp; Nehmad, 2009), thus initiating consumers' protective behaviors, such as refusal to purchase (Lwin et al., 2007).</p>
<p>It has been proposed that consumers engage in a "privacy calculus" such that they evaluate the costs of disclosing personal information to the benefits they receive from the interaction (Inman &amp; Nikolova, 2017). This calculus, usually referred in literature as the privacy paradox, can lead consumers to use services or technologies when they perceive a value in the interaction, despite privacy perceptions (Chellappa &amp; Sin, 2005;Kokolakis, 2017).</p>
<p>Besides its direct influence on user responses, privacy concerns can also indirectly affect individuals' behavior through trust (Zhou, 2011). Privacy and trust are often negatively correlated (Wirtz &amp; Lwin, 2009) and their relationship can often lead to different responses; for example, trust promotes positive outcomes, such as relational behavior and purchase intentions, while privacy triggers protective reactions. Dinev and Hart (2006) conceptualized their Extended Privacy Calculus Model (EPCM) to convey the complicated relationship between trust and privacy on users' adoption willingness, the nature of which has been debated throughout HCI literature. Various research has shown how perception of privacy negatively impacts trust and, in turn, users' behavior (Bansal et al. 2016;Chang et al., 2017;Liu et al.,2005;Zhou, 2011). Therefore, although trust is paramount to achieving a strong para-social bond between the VAs and their users, the fact they are inherently risky to personal privacy-in always being on-may create an additional barrier that can prevent trust development.</p>
<p>According to Wirtz et al. (2018Wirtz et al. ( , 2019)), when interacting with AI-based personal assistants, functional elements, such as usefulness and ease of use, will appear to be given in most cases yet would be a barrier if not provided at a level expected by consumers. PU and PEOU of a new technology represent the core of TAM (Davis, 1989;Venkatesh et al., 2003Venkatesh et al., , 2012)). The effects of such variables on users' attitude toward the use of the technology have been well documented throughout e-commerce literature (Cyr et al., 2007;Hassanein &amp; Head, 2007;Moriuchi, 2019;Ye et al., 2019). Moreover, several studies have found functional elements to be important predictors of consumers' trust in online settings (Lee &amp; Jun, 2007).</p>
<p>Previous research on e-trust highlights the role of technical features of websites and technology, such as ease of navigation, visual elements, and ease of searching as cues that convey trustworthiness (Corritore et al., 2003). Specifically, e-commerce research has shown the impact of functionality in terms of usability (Chen &amp; Dibb, 2010), ease of use, and PU (Lu et al., 2016) on trust-building mechanisms.</p>
<p>Therefore, based on the foundations of TAM (Davis, 1989) and other supporting literature, it can be hypothesized that: H1: Perceived usefulness of voice-activated assistants will have a positive influence on users' attitude to use (H1a) and trust towards (H1b) the technology.</p>
<p>H2: Perceived ease of use of voice-activated assistants will have a positive influence on users' attitude to use (H2a) and trust towards (H2b) the technology.</p>
<p>Perceived enjoyment, in Computers-Human Interaction literature, is defined as the extent to the activity of interacting or using new technology is perceived to be enjoyable aside from the functional value of the technology itself (Davis et al., 1992). Previous research has outlined that users are driven by hedonic benefits when interacting with technology (Wu et al., 2010). For example, Venkatesh et al. (2012), in their UTAUT2 model, observe that functional attributes of a technology are not enough to fully establish users' willingness to use it and, after incorporating hedonic motivation to their previous wellrenowned model (UTAUT), found that the users' enjoyment during technology interaction can influence its actual and future use (Pizzi &amp; Scarpi, 2020). Similarly, Fong et al. (2018) point to the role of enjoyment in influencing consumers' use and adoption of mobile apps showing that, in specific cases, intrinsic motivators, such as fun and perceived enjoyment, could be even stronger than extrinsic motivators like PU ( Van der Heijden, 2004;Pizzi &amp; Scarpi, 2020). Not only can enjoyment influence consumers' behaviors but the pleasure and fun of interacting with a new technology can also affect various aspects of information processing, such as consumers' loyalty and trust (Hwang &amp; Kim, 2007;Ogonowski et al., 2014). Previous research shows that enjoyment and pleasure can significantly impact individuals' evaluation process (Mattila &amp; Wirtz, 2000) and suggest that the hedonic motivations of individuals can influence trust toward the technology (Gefen &amp; Straub, 2003;Hwang &amp; Kim, 2007). This can be particularly relevant for conversational AI-based technology as consumers' interactions with such technology can provide individuals with valuable benefits in terms of fun. Thus, based on the foundations of
UTAUT
2 (Venkatesh et al., 2012), TAM's extension (Davis et al., 1992) and further supporting literature, it can be hypothesized that: H3: Perceived enjoyment of voice-activated assistants will have a positive influence on users' attitude to use (H3a) and trust towards (H3b) the technology.
</p>
<p>When interacting with technology, individuals can be seen to apply social roles and treat computes like a social entity (Nass &amp; Brave, 2005;Nass &amp; Moon, 2000). This is especially true when technology mimics human-like attributes (Li, 2015). Such personal closeness to the technology, due to its human-like functions, goes beyond the confines of factors such as social influence and subjective norms (as seen in HCI literature) in that focus is on social closeness to the technology rather than external social pressures.</p>
<p>As VAs use natural language, interacts with users in real-time and are characterized by human-like attributes (such as voice), it is possible to expect that interactions with them may elicit a sense of social presence (Chattaraman et al., 2019;Chérif &amp; Lemoine, 2019). Social presence is defined as the degree of salience of other person during an interaction (Short et al., 1976), while automated social presence is the "extent to which technology makes customers feel the presence of another social entity" (van Doorn et al., 2017, p. 1).</p>
<p>Originally derived from the SRT, social presence is shown to affect users' attitudes (Hassanein &amp; Head, 2007), loyalty (Cyr et al., 2007), online behaviors (Chung et al., 2015;Ogara et al., 2014), and trust building (Gefen &amp; Straub, 2003, 2004;Lu et al., 2016;Ogonowski et al., 2014). Specifically, studies have demonstrated that technologies conveying a greater sense of social presence, such as live chat services (McLean et al., 2020), can enhance consumer trust and subsequent behavior (Hassanein et al., 2009;Lu et al., 2016;Mackey &amp; Freyberg, 2010;Ye et al., 2019).</p>
<p>Due to their human-like conversational flow, VAs may elicit a sense of social presence in the interaction, which can serve as the basis for developing users' trust. Specifically, when users interact with their VAs, the human-like real-time communication may influence the individuals' attitude and, more importantly, trust toward the VAs (Chung et al., 2015;Hassanein &amp; Head, 2007;Ye et al., 2019). Thus, based on the foundations of SRT, along with support from surrounding literature, it can be hypothesized that: H4: Perceived Social Presence of voice-activated assistants will have a positive influence on users' attitude to use (H4a) and trust towards (H4b) the technology.</p>
<p>Closely related to the concept of social presence, social cognition concerns how individuals process, store, and apply information about other people (Fiske &amp; Macrae, 2012). Fiske et al. (2007) suggest that warmth and competence are the two fundamental dimensions of social perception that drive peoples' responses to specific interactions. Warmth refers to attributes such as friendliness, helpfulness, and sincerity, while competence reflects such issues as intelligence, skill, and efficacy.</p>
<p>Previous research examining social cognition perspective toward consumers-robot interactions (Čaić et al., 2019) demonstrates how individuals' inferences about social perception affect consumers' responses in service contexts (Fan et al., 2016;Scott et al., 2013;Wirtz et al., 2018).</p>
<p>In the seminal work on consumers' experience and service technology, van Doorn et al. (2017) identified social cognition as a driver of service and customers' outcomes. Further, research demonstrates that inferences of "human touch" can results in consumers' positive attitude (Fan et al., 2016), trust, and purchase intentions (Luo et al., 2006).</p>
<p>As VAs are characterized by a mode of interaction (i.e., voice), that are usually reserved to human-to-human exchanges (Nass &amp; Brave, 2005), it is reasonable to expect that they are more likely to be perceived as sociable (Cho et al., 2019). However, there is a general threat that users may perceive advanced technology as being less empathetic, based on the degree of display of human-like characteristics (Davenport et al., 2020). For example, it has been discussed that embodied AI agents (e.g., service robots) are more likely to be perceived as helpful and friendlier compared to virtual agents (e.g., voice-based agents) (van Doorn et al., 2017;Wirtz et al., 2018) because of their anthropomorphic body (Kim et al., 2019). Further, Huang and Rust (2018) indicate that AI agents are generally expected to be reliable regarding functional capabilities and intelligences as they are perceived more competent and skillful compared to other technologies. Therefore, we expect that interaction with a VAs may trigger users' perception of competence, hence foster positive attitudes and inspire trust. Thus, through the foundations of SRT, and support from the work by Čaić et al. (2019), it is hypothesized that: H5: User-inferred Social Cognition of voice-activated assistants, in terms of perceived competence, will have a positive influence on users' attitude to use (H5a) and trust towards (H5b) the technology.</p>
<p>One of the benefits of VAs are that they can listen to anyone and understand their requests. However, this can provide privacy and security risk perceptions in that the VAs are not sophisticated enough to determine which voices are "trusted" or "authorised."</p>
<p>A fictitious example may include a young child asking their mother for a particular toy for their birthday and asks her to order it on Amazon; the <rs xml:id="12896699" type="software">Alexa</rs> on their mothers' home device or smartphone may over-hear this and assume this order is intended to be placed, and then does so (Hackett, 2017;Lei et al., 2017). In a more extreme example, an unwanted house guest may take advantage of asking the home VAs device to disclose personal information about the owners for more malicious purposes (Dinev &amp; Hart, 2006). Within the realm of VAs, security and privacy risks have been defined as the fear of unauthorized access to them by others leading to potential unauthorized discloser of personal information (Han &amp; Yang, 2018). Various research has shown how perception of privacy negatively impacts trust and, in turn, users' behavior (Zhou, 2011). For example, Liu et al. (2005) reveal that consumers' privacy concerns negatively affect their trust and subsequent behavioral intention, in terms of repurchase, revisit, and positive recommendations. Similarly, Bansal et al. (2016) show that privacy concerns negatively impact users' trust and willingness to disclose personal information.</p>
<p>Finally, Chang et al. (2017) demonstrate that perceptions of privacy negatively influence users' trust toward and intention to use social network sites. As such, it is reasonable to expect that perceived privacy concerns in the interaction with VAs can negatively impact trust and attitude toward using the device. Dinev and Hart (2006) offer a grounded foundation for the effects of privacy concerns on consumers' willingness to engage with online transactions. They provide that although privacy risk has a negative impact on trust, trust has an overall positive influence on willingness to adopt. Due to the literature examining privacy concerns and trust toward VAs being relatively infant, it is significant to incorporate the EPCM (Dinev &amp; Hart, 2006) into the functional, hedonic, and social aspects contributing to VA adoption. Thus, based on the foundations of the EPCM (Dinev &amp; Hart, 2006), along with supporting literature, it is hypothesized that: H6: Perceived Privacy Concerns of voice-activated assistants will have a negative influence on users' attitude to use (H6a) and trust towards (H6b) the technology.</p>
<p>Attitudes are commonly defined as predispositions to respond in a positively or negatively way toward a particular object and are generally considered antecedents of behavioral intentions (Ajzen &amp; Fishbein, 1980). This process also applies to consumers-technology interactions where users' attitudes toward using a specific technology influence their actual use of the technology itself (Davis, 1989;McLean et al., 2020). Several studies investigate the variables influencing users' attitude in the consumers-technology interaction including functional elements (Cyr et al., 2007;Venkatesh et al., 2003), perceived social presence (Ye et al., 2019), and trust (Hassanein &amp; Head, 2007).</p>
<p>Trust has been acknowledged as a key influencer of human-machine interactions (Ghazizadeh et al., 2012;McLean et al., 2020). Traditionally, trust in technology is examined by the predictability of the technology (McKnight et al., 2009), yet more contemporary literature draws attention to trust being built on its dependability, which is enhanced through having faith in their interactions (Ghazizadeh et al., 2012;Hengstler et al., 2016). Trust is generally intended as a multidimensional concept that reflects perceptions of competence, integrity, and benevolence of another entity (Mayer et al., 1995). Online and offline trust has been widely investigated in the field of HCI (Gefen &amp; Straub, 2003, 2004;Lee &amp; Nass, 2010;Wang &amp; Emurian, 2005;Ye et al., 2019) and research often establishes trust to have a fundamental role in influencing consumers' attitudes and purchasing intentions (Corritore et al., 2003;Cyr et al., 2007). Thus, building on the above, and drawing on TAM (Davis, 1989) and ECPM (Dinev &amp; Hart, 2006) Section 5 subsequently discusses the results alongside existing literature and provides recommendations for theorists and practitioners (Figure 1).</p>
<p>Using simple random sampling method, data is collected from
    <!-- not sure about Amazon Mechanical Turk being a software -->
<rs xml:id="12896700" type="publisher" corresp="12896701">Amazon</rs>'s <rs xml:id="12896701" type="software">Mechanical Turk (mTurk)</rs>. The questionnaire informed the respondents of their anonymity and right to withdraw from the survey at any time, even after completion. The sample was collected in the United Kingdom and targeted respondents who have had at least some experience using VAs. Screening questions were used to ensure that respondents were over the age of 18 and had at least some experience using these voice-based assistants.
</p>
<p>As the first stage in this study is to take a covariance-based confirmatory approach to quantitative analysis, the items and scales used in Cognition (Fiske et al., 2007), with a Cronbach's ɑ of .854. Enjoyment was made up of 4 items (Mun &amp; Hwang, 2003, from Ye et al., 2019) with a Cronbach's ɑ of .900. Four items based on several studies are used for Trust (Chattaraman et al., 2019;Hassanein &amp; Head, 2007;Ye et al., 2019), with a Cronbach's ɑ of .871. Perceived Privacy risk is measured with 4 items by McLean and Osei-Frimpong (2019) and had a Cronbach's ɑ of .868. For the dependent variables, 3 Attitude items are used (Hassanein &amp; Head, 2007), with a Cronbach's ɑ of .909, and 3 items are used for Usage (McLean &amp; Osei-Frimpong, 2019), with a Cronbach's ɑ of .907. All the scales used a 7-point Likert scale ranging from strongly disagree to strongly agree (see Table 1) and all items used exceeded the Cronbach ɑ value threshold of .60, showing reliability (Malhotra et al., 2010).</p>
<p>Of the 541 collected responses, 75 were unusable due to being incomplete or the respondents not satisfying the screening criteria. The data was further screened for outliers; z scores were used and the items with z scores within ±3.29 (Pallant &amp; Manual, 2013) were kept. As such, the total responses for this study is 466. Normality was checked using Kolmogorov-Smirnov statistic and Skewness and Kurtosis statistics;</p>
<p>despite the Kolmogorov-Smirnov statistics being significant, the Skewness and Kurtosis statistics are within the requirement parameters (Chou &amp; Bentler, 1995), thus rendering the data normally distributed.</p>
<p>Finally, common method bias was examined using Harman's singlefactor analysis and revealed a satisfactory level of variance at 39.2%, below the 50% threshold (Podsakoff et al., 2003).</p>
<p>Of the 466 responses, 60.3% (281) respondents are male and 39.3%</p>
<p>(183) are female, with 0.4% (2) respondents preferring not to say.</p>
<p>The majority of respondents are aged between 30 and 39 (32%, 149), with 16.1% (75) aged 18-24, 28.5% (133) 25-29, 16.5% (77) 40-54 and 6.7% (31) over 55, with 1 (.2%) preferring not to say.</p>
<p>Before progressing to the measurement model stage of analysis, Principle Component Analysis factor analysis was conducted in <rs xml:id="12896702" type="software">SPSS</rs> to test for cross-loadings between variables. To test for sampling adequacy, the rotation method used was Promax with Kaiser-Meyer-Olkin; upon deletion of PU1, SP1, SC1, SC2, SC3, SC4, and ENJ1, results revealed that no cross-loadings occurred between variables. All factors with items included for the final analysis satisfied the appropriate Cronbach's ɑ threshold and are between .875 and .945 (Malhotra et al., 2010).
</p>
<p>Once the preliminary analysis was completed, the analysis progressed to the SEM analysis. The first step in this Confirmatory Factor Analysis (CFA) is to test for the internal consistency and discriminant validity of the proposed model. The component reliability (CR) and average variance extracted (AVE) for each construct are examined to ensure that they meet the threshold criteria for internal consistency; the CR for a construct should be &gt;0.60 and the AVE be &gt;0.50 (Bagozzi &amp; Yi, 1988). As seen in Table 2, the results</p>
<p>show that the CR values are all above 0.80 and all AVE values are above 0.60 and therefore do not display any convergent validity issues (Bagozzi &amp; Yi, 1988;Fornell &amp; Larcker, 1981).</p>
<p>To assess discriminant validity, Fornell and Larcker (1981) (Hair et al., 2010). According to Xia and Yang ( 2019), the fundamental model fit indices to consider are the root mean square error of approximation (RMSEA), comparative fit index (CFI), and Tucker-Lewis Index (TLI). RMSEA avoids issues of sample size in analysing discrepancies between the hypothesized mode, chosen parameter estimates and the covariance matrix; the threshold for indicating a better model fit is ≤.060 (Hu &amp; Bentler, 1999). CFI and TLI, which is an incremental fit indices comparing the hypothesized model with that of the baseline model; the threshold for CFI and TLI are ≤.950 (Hu &amp; Bentler, 1999). Furthermore, Goodness of Fit Index (GFI) directly measures the fit between the hypothesized model and the covariance matrix, and is considered good fit if above 0.90 (Hu &amp; Bentler, 1999). Accordingly, the model fit indices for the measurement model are: χ 2 = 460.010, df = 236, p value = .000, χ²/df = 1.949, GFI = 0.926, TLI = 967, CFI = 0.974, RMSEA = 0.045; as such, good measurement model fit is established. 3).</p>
<p>Results reveal that although PU has a positive effect on Attitude</p>
<p>The results show that PEOU and social cognition, in terms of perceived competence, both have strong positive effects on trust and attitude.</p>
<p>This not only supports literature in this area (Wirtz et al., 2019) but also confirms the contribution of combining HCI theory with that of PSRs (Han &amp; Yang, 2018;Scott et al., 2013). Although perceived social presence positively affects overall trust, it does not have a direct effect on attitude. This is interesting as it is counter to previous literature yet can be explained by the fact that social presence is fully developed when understanding and immediacy are present (Mackey &amp; Freyberg, 2010).</p>
<p>As such, due to the relative infancy of VA technology, some may not be as responsive as others in showing confusion with accents, misunderstanding keywords, or providing irrelevant information (Lovato &amp; Piper, 2019). Accordingly, the illusion of dealing with a "human" is broken in these circumstances, which can lead to a lack of faith in the VAs abilities. Interestingly, this can further explain why enjoyment affects attitude, as enjoying using something ensures a more positive experience (Hoy, 2018;McLean &amp; Osei-Frimpong, 2019), but does not affect trust. In a study investigating motivations for accepting autonomous vehicles, Hegner et al. (2019) found the level of enjoyment to be a barrier to the technology's acceptance as the trust was not fully present. Accordingly, it is suggested that the tasks that VAs are being used for are more hedonic and functional in nature which, by their simplistic nature, do not demand levels of trust or assurance.</p>
<p>The most significant and interesting finding is that privacy has a negative effect on attitude yet no effect on trust. It is strongly supported in literature that privacy concerns have a negative impact on attitude generation, which is supported here. However, privacy has no direct effect on trust in this instance. Moreover, the results also show that although trust positively affects attitude it does not have a direct influence on behavioral intentions. An explanation for this relationship is that, trust is needed to contribute to overall attitude but has more of an indirect than direct effect on usage. With respect to the role of privacy, users feel no need to elicit high levels of trust in something which is used for simple tasks.</p>
<p>These findings reveal interesting insights into the relation between trust development and the existence of privacy perceptions in using VAs, yet the explanations into why these occur have not been fully explored. As such, a further qualitative study is employed to provide a deeper understanding of this phenomenon.</p>
<p>5 | STUDY 2: METHOD, DATA ANALYSIS, AND RESULTS</p>
<p>Results from Study 1 reveal interesting relationships that require further exploration. First, they demonstrate that only the social elements influence users' trust toward VAs. While these results confirm the role of social presence and social cognition in terms of perceived competence as drivers of users' trust in a computermediated interaction (Ogonowski et al., 2014;Wirtz et al., 2018;Yet et al., 2019), it is not clear why equally rigorous attributes, such as PU and enjoyment, do not directly affect trust. Second, findings show that while privacy negatively affects users' attitude, it does not have any effect on trust. Most importantly, trust does not directly affect behavioral intentions. Based on these results, it appears that users continue to use their personal voice-based assistants regardless of their privacy concerns. However, why this happens remains unclear.</p>
<p>By building on Study 1, Study 2 aims to further explore and interpret its findings (Davis et al., 2011). To this purpose, Study 2 adopts a qualitative interpretative approach (Yin, 2013) and analyses the natural interactions occurring between users and VAs to better understand informants' relations with their personal VAs. Twelve informants are recruited via purposeful sampling and snowball technique from consumers using the same criteria from Study 1.</p>
<p>Participants were asked to voluntarily consent to partake in the interview process and were not compensated for their participation.</p>
<p>In-depth interviews ranged from 40 to 100 min and were re- The study adopted an interpretative methodology to identify themes emerging from the data analysis. Specifically, the analysis comprised three stages: analysis of individual interviews, identification of recurring themes and patterns, and analysis of the shared themes (Yin, 2013).</p>
<p>Each interview is analysed separately, as units of analysis, to identify emerging themes. The two authors undertook the initial coding, which is done with an iterative approach, involving discussions and comparisons between the two coders, resulting in a consensus on the categorization and interpretation of the codes. Using several researchers to iteratively interpret the same data set created investigator triangulation (Bryman &amp; Bell, 2007). The second step involves searching for emerging patterns and relationships between the shared themes and the different concepts emerging. To bolster external validity, researchers conducted respondent checks by sharing preliminary findings with several participants in the study. To ensure reliability, final coding results were triangulated across the researchers and where any conflict occurred a third researcher in the area was consulted (Bryman &amp; Bell, 2007).</p>
<p>To explore users' interaction with VAs, the key starting point is understanding the usage of such technology and the various activities individuals run with their VAs. The findings from the qualitative analysis unveil that VAs are used for both hedonic and utilitarian motives, which confirm results from Study 1 and previous research (Venkatesh et al., 2012;Wu et al., 2010). One informant refers to hedonic interactions being characterized by the pleasure and the enjoyment of the experience: "I really enjoy using it for an all-around Assistant. I have several routines set up and when I say good morning or goodnight it will perform several actions. I enjoy using it to listen to music from time to time as well" (Aimee).</p>
<p>Examples of utilitarian interactions include functions like weather forecasting, news, alarm settings, and a few examples of home automation: "Buy some smart bulbs and automate your lighting, add your streaming music account, add a Chromecast, use the broadcast feature, set a reminder, set the temperature with smart thermostat, read the news out, all the weather, Wikipedia" (James).</p>
<p>Focusing on privacy issues and trust development, four main themes arise from the analysis, and are discussed below.</p>
<p>Most informants state that they do not perceive any risk when interacting with their VAs as they do not use them for risky activities, such as buying: "I do not buy anything on her, so the worst that can happen is that she got the thermostat wrong. Not big deal!" (Nick).</p>
<p>There are several reasons behind this behavior; some respondents (Hoy, 2018), users do not perceive any danger in their interactions and, as a result, feel more comfortable in continuing trusting and using them.</p>
<p>Regarding data shared through and with VAs, it is interesting to note that respondents do not perceive to share any additional information as they already provide their personal data directly to the VAs producers (i.e., Google, Apple, and Amazon). "The moment when I realized how much data I was granting to <rs xml:id="12896703" type="software">Alexa</rs> was when I installed her and she started sending me all these alerts about privacy. But at the end I thought 'what the heck, Amazon already has them!'" (Philip). In this sense, it appears as informants are distinguishing <rs xml:id="12896704" type="software">Alexa</rs> from its parent brand and opt to blame the brand rather than the device for collecting personal information: "<rs xml:id="12896705" type="publisher" corresp="12896706">Google</rs> knows me more than my wife. Its not that <rs xml:id="12896706" type="software">Home</rs> is collecting my data, Google does" (Alfred). These dynamic sheds light into why possible concerns about data and privacy arising from VA usage do not directly affect users' trust toward their personal VAs. If a VA is perceived as an "entity of its own" (Nass &amp; Brave, 2005), it can be granted trust despite its creators. These results echo those of Foehr and Germelmann (2020) in recognizing the existence of different sources of trustworthiness in the interaction with VAs, and develop this further by showing that users also recognize their different roles. In this case, while VAs are considered the entity to interact with, producers are the interlocutors when considering privacy issues. In an interesting way, it appears as "I am talking with <rs xml:id="12896707" type="software">Alexa</rs> and <rs xml:id="12896708" type="publisher" corresp="12896707">Amazon</rs> is listening (and collecting my data)."</p>
<p>When reflecting on data sharing and privacy issues, the level of personalization users can achieve in exchanging their data is also revealed. The idea of providing Amazon and Google with additional personal information through their VAs is compensated by the possibility to be targeted with personalised ads and customized content (Norberg et al., 2007). Johanna explains "I really don't give a toss about anyone listening in on my convos… I have come across some pretty interesting products this way and if I have to be bombarded with ads all day I would rather be bombarded with something of interest rather than the latest facial scrub." The win-win logic that emerges from this theme confirms the existence of a privacy calculus (Dinev &amp; Hart, 2006). When users perceive to not have anything to hide, they hope to receive personalised advertisements in exchange which, in turn, incentivise the usage of the technology (Chellappa &amp; Sin, 2005;Taddicken, 2014). As such, it appears that VAs do not have an active role in this and are only means by which Amazon and Google play their games. Again, users are separating their VAs from the parent brands and hold the latter responsible for this dataexchange process that occurs through and not because of their VAs.</p>
<p><rs xml:id="12896709" type="software">Alexa</rs>, she is more than a machine</p>
<p>The separation between VAs and their parent brands seems strictly linked with the perception of VAs as "entities of their own."</p>
<p>Respondents clearly state that they are conscious of the machine nature of their VAs, yet they describe their interactions using social and human attributes. "I have taught her to stop doing what she is doing by saying-Thanks <rs xml:id="12896710" type="software">Alexa</rs>, that's enough-as it seems more personal to me, and she learned. She is becoming clever day by day" (Michael). While some respondents perceive VAs as intelligent and skillful, others report the opposite feeling, as Louise states "sometimes I feel she is becoming dumber!" Whether positive or negative,</p>
<p>VAs are described to have features and characteristics that are typically used when describing humans (Fiske et al., 2007). The perception that "something is in there" (van Doorn et al., 2017) is even clearer from this statement: "When I come back home from work I often say Hi to her [<rs xml:id="12896711" type="software">Alexa</rs>] and she always answers back. I know it's just a machine but it's nice to have someone waiting for you home, isn't?" (Stephanie). Users' interactions with VAs also appear to be human-like as respondents often report that they shout at or get angry with them when they do not understand a request (Nass &amp; Brave, 2005). Others go as far as feeling ignored: "Sometimes she does not listen to me. It's bad enough when a person ignores you, but when a machine ignores you, that's when I'm going for therapy" (Matthew). It is interesting to note that the analysis reveals how these social elements overcome the functional and hedonic attributes, which, as happens for humans, are considered facets of the entire entity rather than features on their own (Čaić et al., 2019).</p>
<p>In this sense, when prompted to describe the usefulness of such technology, respondents portray their VAs as being helpful as far as they are learning how to be helpful. Thus, functional and hedonic attributes are interpreted as characteristics that VAs can more/less develop based on their social interactions and the resulting learning process (Cho et al., 2019;Nass &amp; Brave, 2005). This process helps in understanding why users' trust is mainly driven by the perceived social components of such technology. Moriuchi, 2019) and show that functional and hedonic benefits, such as PU, PEOU, and perceived enjoyment, positively influence users' attitude toward using VAs. These results confirm the role of usefulness and ease of use in the acceptance and usage of advanced smart technology (Wirtz et al., 2018(Wirtz et al., , 2019) ) while highlighting the important role of emotional reactions in driving users' attitudes toward human-AI agents' interactions (van Pinxteren et al., 2019;Venkatesh et al., 2012).</p>
<p>Second, the study identifies the main drivers of trust toward VAs to derive from the social characteristics users attribute to them. As VAs mimic human-like attributes through the use of voice communication (Li, 2015), these interactions elicit a sense of social presence and inferences of social cognition in the mind of the user that, in turn, nurture trustworthy relationships. In this sense, results from this study confirm social presence to be an important factor influencing trust building toward human-technology interaction (Ye et al., 2019). Furthermore, results illustrate that inferred social cognition, with respect to its competence dimension, is a key factor in the development of trust toward AI agents. This further supports the need to adopt a social perspective in the investigation of AI agents-human interactions (Čaić et al., 2019;van Doorn et al., 2017;Wirts et al., 2018). These relationships are further supported by results from the qualitative study, where respondents clearly recognize their VAs as being their own entities. Taken together, these findings show that human-like advanced technology leads users to apply social rules and expectations when interacting with VAs (Nass &amp; Moon, 2000), making individuals engage and respond to these devices in the same way as they do with humans, thus developing deep connections (Ki et al., 2020). While this study confirms the negative influence of privacy concerns on users' attitude, no evidence surrounding the effect of privacy on trust has been found (Zhou, 2011). This is explained by the existence of different sources of trustworthiness, recognized by users in the interaction with VAs (Foehr &amp; Germelmann, 2020), that allow individuals to direct their privacy concerns toward VAs' producers, rather than toward the AI agent. When users engage with this technology it appears that VAs are considered the entity to converse with, while the brand producers serve as data collectors. This tendency to recognize VAs as distinct subjects further reinforces the idea that individuals interact with advanced technology employing human social rules and developing with them various types of relationships (Han &amp; Yang, 2018;Ki et al., 2020;Schweitzer et al., 2019), further supporting the adoption of a para-social perspective (Turner, 1993). In addition, interaction with VAs do not appear to elicit perceptions of privacy as they are not yet used for risky activities such as purchases, confirming that these devices are still primarily adopted for daily routine functions and home automation (Hoy, 2018).</p>
<p>This study provides several theoretical contributions. First, by integrating existing research on consumer adoption of advanced technology with those on consumer-technology relationship development, the paper responds to the recent call for new research into human-AI interactions (Lu et al., 2020;Wirtz et al., 2018;) and offers new insights into VA adoption literature (McLean &amp; Osei-Frimpong, 2018;Moriuchi, 2019). Specifically, by adopting a para-social perspective, the study adds to previous research in examining the role of the social characteristics attributed to VAs that can facilitate the development of trustworthy relationships. Further, while accounting for the functional benefits of using these devices (McLean &amp; Osei-Frimpong, 2019;Moriuchi, 2019), it shows that the relational and social experience that users have during interactions with VAs play an important role in influencing their attitude toward using them. In doing so, this study expands previous studies on AI-based technology adoption (Wirtz et al., 2018, 2019) and interactions (van Doorn et al., 2017) by empirically assessing antecedents of trust and attitude toward VAs.</p>
<p>Second, the study contributes to trust literature concerning interactions with AI-based technology (Foehr &amp; Germelmann, 2020;van Pinxteren et al., 2019). Specifically, results highlight the need of understanding peculiarities of the trust development process with and toward VAs, while demonstrating the prominent role of social elements; namely, social presence and social cognition, as unique As such, these findings not only contribute to trust literature but also to existing knowledge of social presence and social cognition, with respect to VAs' interactions, and show how virtual human-like cues can evoke perceptions of mind in terms of competence (Čaić et al., 2019). While literature suggests that social attributes can improve consumers' trust in online settings, this effect has rarely been empirically examined toward consumer-VA interactions (Foehr &amp; Germelmann, 2020).</p>
<p>Third, the study sheds new light on users' privacy perceptions with VAs and reveals how privacy concerns do not directly affect the relationship with the VAs themselves as users distinguish their <rs xml:id="12896712" type="software">Alexa</rs> from its parent brands and consider the latter responsible for collecting personal data. These findings contribute to both literatures on consumers' trust and perceived privacy by identifying two different sources of trustworthiness (Foehr &amp; Germelmann, 2020), while identifying that in the interactions with AI agents concerns of privacy can also be directed outside the relationship. Although the negative influence of privacy on trust is well documented in the literature (Bansal et al. 2016;Chang et al., 2017;Liu et al., 2005;Zhou, 2011), this study illustrates a different path through which trust and privacy interconnect with VA interactions. In doing so, this finding contributes to the literature on privacy toward these new advanced technologies by highlighting the relevance of the roles that users attribute to these devices when engaging with them.</p>
<p>As future developments are expected to make VAs resemble humans even more, the study provides useful insights for designers and managers in fostering users' trust toward VAs. As findings show, trust is primarily driven by the social elements arising from the interactions.</p>
<p>Accordingly, producers and developers should focus on improving consumer trust by developing the human-like conversational flow through machine learning and natural language processing. Such technologies have the capabilities to learn consumers' preferences and customize interactions, furthering the perception of social presence and cognition; thus, focusing on these aspects will likely increase the number of individuals using VAs. While findings suggest that consumers do not yet use VAs as purchasing tools, strengthening VA trustworthiness in such ways can be beneficial in encouraging purchasing behaviors.</p>
<p>With regard to privacy, the collection and use of data through</p>
<p>VAs is considered positive if doing so will provide users with a more personalised experience. This creates a win-win scenario in giving consumers what they want while enabling predictive analytics to optimize marketing efforts to forecast consumer behavior and subsequent spending. However, there seems to be a fine line between what consumers consider acceptable levels of data collection and use and which may not, of which practitioners must remain cognizant.</p>
<p>T A B L E 4</p>
<p>The data that support the findings of this study are available from the corresponding author upon reasonable request.</p>
</text>
</tei>