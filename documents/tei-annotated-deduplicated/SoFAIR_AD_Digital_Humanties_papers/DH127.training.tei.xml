<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
<teiHeader>
<fileDesc xml:id="_1"/>
<encodingDesc>
<appInfo>
<application version="0.8.1" ident="GROBID" when="2024-11-15T07:09+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text xml:lang="en">
<p>This article is an attempt to represent Big Data research in digital humanities as a structured research field. A division in three concentric areas of study is presented. Challenges in the first circlefocusing on the processing and interpretations of large cultural datasets -can be organized linearly following the data processing pipeline. Challenges in the second circle -concerning digital culture at large -can be structured around the different relations linking massive datasets, large communities, collective discourses, global actors, and the software medium. Challenges in the third circledealing with the experience of big data -can be described within a continuous space of possible interfaces organized around three poles: immersion, abstraction, and language. By identifying research challenges in all these domains, the article illustrates how this initial cartography could be helpful to organize the exploration of the various dimensions of Big Data Digital Humanities research.</p>
<p>Defining the nature and the boundaries of digital humanities is a long-discussed and unsolved issue (Terras et al. 2013), not only because there is no consensus on this question but also because digital humanities are currently undergoing a profound transformation that calls for a reconsideration of its fundamental concepts (Gold 2012). For years, digital humanities have been loosely regrouping computational approaches of humanities research problems and critical reflections of the effects of digital technologies on culture and knowledge (Schreibman et al. 2008). Ten years ago, they emerged as a new label, rebranding and enlarging the idea of "humanities computing" (Svensson 2009). Around this new name and under a "big tent, " a progressively larger community of practice thrived (Terras 2011). Each work at the intersection of Computer Science and the Humanities could potentially be part of this welcoming trend. Researchers gathered in national and international meetings, exchanged their views on blogs and mailing lists. If not a well-bounded field, digital humanities were surely a lively conversation.</p>
<p>The welcoming digital humanities label opened doors, connected separated academic silos, built bridges between information sciences and the various disciplines loosely forming what is called the humanities. However, openness was always associated with a need for introspection, self-reflexive writings, tentative boundaries definitions, the "What are digital humanities" articles and monographs became a genre of its own structured around several narratives of exclusion and inclusion (Rockwell 2011). Digital humanities as a research domain define themselves dynamically in the negotiation of these tensions as discussed by several digital humanities scholars (Unsworth 2002;Svensson 2009;Rockwell 2011). Table 1 gives a non-exhaustive list of these structuring tensions.</p>
<p>The starting point of this article is a relatively new particular structuring tension, opposing Big Data Digital Humanists with Small Data Digital Humanists. Research in Big Data Digital Humanities focuses on large or dense cultural datasets, which call for new processing and interpretation methods.</p>
<p>Humanists vs. digital humanists When does research in humanities become digital humanities? Can "every medievalist with a website" be part of the digital humanities (Fitzpatrick 2012a)? Does the use of a computer in humanities research make digital humanities research (Unsworth 2002)?</p>
<p>Computer scientists vs. humanists inside digital humanities Should we still distinguish computer scientists and humanists in digital humanities communities? Is the "two cultures" tension still relevant (Snow 1959)? Are digital humanities a form of "technical upgrade" of the humanities disciplines? Are digital humanities just a particular "application" of the Computer Science fields?</p>
<p>Makers vs. interpreters Are digital humanities only about "building things"? If you are not a "maker," should you not be considered as "digital humanist" (Ramsey 2011)? Is there room for purely interpretative digital humanities?</p>
<p>Distant readers vs. close readers Are digital humanities only about "distant reading" (Moretti 2005)? To study literature, should we stop reading books and only focus on quantitative algorithmic measure (Marche 2012)? Can digital humanities also enhance close reading experience? Are "distant reading" approaches a form of radical digital humanities?</p>
<p>The term Big Data itself has disputed origins (Diebold 2012;Lohr 2013). The Oxford English Dictionary defines it as "data of a very large size, typically to the extent that its manipulation and management present significant logistical challenges. " In that sense, Big Data are "big" when "manual" analysis becomes cumbersome and new study and interpretation methods must be invented. However, massiveness of Big Data is not tightly linked to a certain number of Terabytes. Boyd and Crawford (2011) note that "Big Data is not notable because of its size, but because of its relationality to other data. " Big Data is "fundamentally networked" and challenges in processing it are linked with its interconnected nature. In comparison, the Small Data Digital Humanities regroup more focused works that do not use massive data processing methods and explore other interdisciplinary dimensions linking computer science and humanities research. In comparison with Big Data, Small Data is small in the sense that it is not only smaller-scale but also well-bounded. This article intends to draw a map for Big Data digital humanities showing how it can be organized as a structured field. The ambition of this map is to show that Big Data research in digital humanities can be characterized by common methodologies and objects of studies, therefore transcending some of the tensions that have structured digital humanities so far. As it focuses only on research that deals with these "large body of information" (Katz 2005), this maps does not cover the digital humanities domain as whole. Nevertheless, given the growing importance of massive and networked cultural datasets, it is likely that Big Data digital humanities become a significant part of the whole digital humanities field. In this context, this map may help institutionalize research and education programs with clearer focuses and objectives.</p>
<p>This article presents Big Data research in digital humanities as three concentric circles (Figure 1). The first circle corresponds to research focusing on processing and interpretation big and networked cultural data sets, the first object of study of this field. Most of the methods needed to study these datasets need still to be invented, as they are currently not mastered neither by humanists or computer scientists. However, it is important to consider that data processing and interpretation occur in a larger context of the new digital culture characterized by collective discourses, large community, ubiquitous software, and global IT actors. Understanding the relation between these entities could be considered the second object of study for Big Data Digital Humanities. Eventually, the human experience of such datasets through various kinds of interfaces corresponds to a third family of challenges, differing in scope and methodology from the other two. Therefore, these three areas of studies could be represented as three concentric circles, illustrating three levels of contextualization and embodiment of cultural data. In the next sections, we will briefly discuss each of the circles in more details.</p>
<p>Massive cultural digital objects include large-scale corpus like the millions of books scanned by Google and the ones produced by numerous other digitization initiatives (Jacquesson 2010), the millions of photos and micro-message shared on social network services (Thusoo et al. 2010), giant geographical information systems like <rs xml:id="12973244" type="software">Google Earth</rs> <rs xml:id="12973245" type="bibr">(Butler 2006)</rs>, or the ever expanding networks of academic papers citing one another (Shibata et al. 2008). These interconnected objects -either digitally born or reconstructed through digitization pipelines -are too big to be read or watched. The traditional 1:1 ratio of a single scholar confronted with one document cannot cope with such abundance. Moreover, their boundaries are sometimes fuzzy, their content partially unknown and, likely to be in continuous expansion. These characteristics make them profoundly different from corpora traditionally studied by humanities researchers, despite surface resemblances.</p>
<p>The confrontation with these "massive" objects calls for fundamental questions. What can really be extracted from these huge datasets and what interpretations can be drawn based on these extractions? Will we learn more by analyzing 10 millions books that we cannot read individually or by reading five carefully (Moretti 2005)? What is the role of algorithms for mining, shaping, and representing these large digital objects? Some of these challenges can be structured following the specific parts of data processing: digitization, transcription, pattern recognition, simulation and inferences, preservation, and curation as show in Figure 2 and in the Table 2 below. Each step in the data processing pipeline can be associated with questions that are both technical and epistemological. Consider the processing pipeline of mass book digitization projects. Physical books must be transformed into images (digitization step) that are then transformed into texts (transcription step), on which various pattern can be detected (pattern recognition step like text mining or n-gram approaches) or inferred (simulation step) while being preserved and curated for future research (preservation step). This way of presenting the research challenge insists on the fact that data are never given, but taken and transformed (Gitelman 2013). The technical complexity of pipelines involved clearly demonstrates that, at each step of the data processing, choices are made and biases apply. Understanding these technical choices is crucial to develop new interpretive theories.</p>
<p>We discussed the relationship between data processing pipelines and large cultural datasets. However, data processing and interpretation happen in a larger context, which we may call Digital Culture. The study of this large context can be considered to be the second object of study for digital humanities research. One way to structure this domain is to replace the relation between software and data (the focus of the first circle) in a network of relations between new entities including large-scale communities (MOOCs classrooms, <rs xml:id="12973246" type="software">Wikipedia</rs> contributors, etc.), collective discourses (Blogs, data journalism, wiki-style collaborative writing), ubiquitous software medium (auto-completion algorithm, search engine), and global actors (Google, Facebook, GLAM, Universities).</p>
<p>Consider the millions of photos shared every hour on <rs xml:id="12973248" type="software">Facebook</rs> <rs xml:id="12973249" type="bibr">(Huang et al. 2013)</rs>. In this case, large-scale communities produce both the massive digital objects and the collective discourses Step Challenges</p>
<p>How can we develop more efficient, cheaper, faster digitization techniques allowing to perform mass-digitization programs (Coyle 2006;Lopatin 2006)? How can we develop new sensors and capture systems to obtain more information about the physical artifacts we study (Stanco et al. 2011)? How can we run crowdsourced digitization campaigns (Causer and Melissa 2014)? How can we upgrade datasets digitized with older technical methods (Paradiso and Sparacino 1997)? How can we perform efficient quality controls during digitization processes, anticipating the other steps of the technical pipelines (Liew 2004)? How can we store and compress information as it is being digitized? How can we attach metadata information documenting all these digitization processes?</p>
<p>Transcription How can we "read" ancient documents (Antonacopoulos and Downton 2007)? How can we recognize specific features in paintings (Smeulders et al. 2000;Saleh et al. 2014)? How can we segment and transcribe audio and video content (He et al. 1999)? What kind of digital preprocessing needs to be performed to facilitate these transcription processes? How can automatic and manual processes be combined? How can we monitor the level of errors and the biases of algorithms in these transcription processes?</p>
<p>Pattern recognition How can we detect common structural patterns in large collection of paintings, sculptures, and buildings models? How can we find names of people and places in texts (McCallum and Li 2003)? How can we classify the content of messages exchanged, detect events (Das Sarma et al. 2011)? How can we construct semantic graphs of data? How can we reconstruct and analyze networks from these data sets and trace the circulation of patterns?</p>
<p>Simulation and inference How can we infer new data based on the data sets we study? How can we simulate missing data sets based on patterns detected? How can any uncertainty linked with these reconstructions be assessed (Bentkowska-Kafel et al. 2012)? How can we conduct simulation simultaneously at different scales? How can the inference, extrapolation, and simulation rules be attached to the data they produce in order to document this process (Nuessli and Frédéric 2014)?</p>
<p>Preservation and curation How should data be stored to ensure both efficient short-term use and long-term preservation? What kind of storage support should be used? How can we assess their longevity? What kind of centralized or decentralized approaches are preferable? How much redundancy is needed? How should data be encoded to ensure traceability despite successive re-encoding? How can privacy, security, and authenticity of data be guaranteed? How can digitally born content be archived (Day 2006)?</p>
<p>The processing domain (1) covers the interaction between software and massive digital objects from a technical and epistemological perspective, studying in particular how to design data-processing algorithms capable of deriving new data out of massive digital objects and how data becomes knowledge through complex processes of interpretation, or hermeneutics. This is a domain we have discussed in the previous section Challenges of the processing domain have been discussed in the previous section</p>
<p>The discursive domain (2) covers the study of the shape of collective discourses in relation with massive digital cultural objects, from <rs xml:id="12973250" type="software">Facebook</rs> to scientific articles. All the natural categories of "digital linguistic studies" are relevant for this domain: lexical studies, grammatical studies, semantics, pragmatics, and semiotics</p>
<p>How do new technologies redefine scholarly discourses? How is the selective role of recognized academic journals challenged by new forms of open peer review (Shirky 2009;Fitzpatrick 2012b)? Can we imagine new publishing formats of "higher dimensions" allowing to embed videos, visualization interfaces, simulation engines, and source codes (Kaplan 2012)? What is the epistemological status of interactive visualizations? Can simulators be considered as a new kind of representation?</p>
<p>The social shaping domain (3) studies how large-scale communities shape and are shaped by the collective discourses they produce. This corresponds to typical sociolinguistic topics, adapted to the context of digital culture What happens to authorship in crowdsourced projects or wiki-style contributions (Hoffmann 2008)? What is the role of automatic reading machines for plagiarism detection (Sloterdijk 2012) or new form of writing (Goldsmith 2011)? How does mass-digitization projects entail new specific copyright issues (Borghi and Karapapa 2013)?</p>
<p>The algorithmic mediation domain (4) covers how software mediates discourses and communities. This is an area traditionally covered by software studies (Manovich 2013 about massive digital objects. They do so through the mediation of algorithms produced by one giant IT company of the web. Retroactively, collective discourses about the photos have a shaping role on the emergence and structuration of these communities.</p>
<p>In addition, as collective discourses reach rapidly a critical mass (e.g., millions of messages or status update) they tend to become themselves massive digital objects, to be archived and studied through specific text and data mining approaches. Understanding photo sharing implies understanding the complexity of this network of interactions.</p>
<p>More generally, research about digital culture can be segmented in subdomains corresponding to groups of relations between some of the entities we have been discussing. This structuration summarized in Table 3 and Figure 3, identifies five domains: the processing domain, the discursive domain, the social shaping domain, the algorithmic mediation domain, and the control domain. This grouping articulates differently the relations of Big Data Digital Humanities with traditional humanities and social sciences disciplines, not considering that digital history, digital sociology, etc., but a new segmentation of domains.</p>
<p>Big cultural data, and digital culture at large, are experienced in the real world through physical interfaces, websites and installations. They produce "experiences. " This third circle is an area of study on its own. Some interfaces are essentially immersive, in the sense that they try to project the user into full-fledged environments (e.g., 3d Virtual World). Others provide users with synthetic data representations (e.g., network visualizations). Eventually, some interfaces are essentially linguistic allowing users to browse data via linguistic inputs (e.g., search engine). We can represent the space of possible interfaces with a triangle organized around these three summits (Figure 4). Conversational agents (e.g., <rs xml:id="12973251" type="software">SIRI</rs>) are in between the immersive and linguistics summits. Word clouds are in between abstract and linguistic summits. GIS interfaces can be sorted from the most abstract (<rs xml:id="12973252" type="software">Google maps</rs>, <rs xml:id="12973253" type="software">Open Street Map</rs>) to the most immersive (<rs xml:id="12973254" type="software">Google Street view</rs>). Augmented reality interfaces combine immersive, abstract, and linguistic dimensions. Each dimension of the interface space is associated with specific challenges, some of which are summarized in Table 4.</p>
<p>Research in Big Data in digital humanities is becoming a wellstructured field with specific objects of study. In this article, we identified three concentric areas of study and discussed how challenges in each area could be mapped. We illustrated how challenges focusing on the processing and interpretations of large cultural datasets can be organized linearly following the data processing pipeline, how challenges concerning digital culture at large could be structured around a network of relations between the new entities that emerged with the digital revolution and eventually, how challenges dealing with the experience of digital data can be described using the continuous space of possible interfaces. There are surely other ways of mapping this emerging field and the suggested structuration could be certainly refined and amended. However, we hope that this initial cartography will help paving the road ahead, acting as an invitation for exploring further the idea of Big Data Digital Humanities as a structured field.</p>
<p>Frontiers in Digital Humanities | www.frontiersin.org May 2015 | Volume 2 | Article 1</p>
<p>The author declares that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
<p>reproduction in other forums is permitted, provided the original author(s) or licensor are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</p>
</text>
</tei>