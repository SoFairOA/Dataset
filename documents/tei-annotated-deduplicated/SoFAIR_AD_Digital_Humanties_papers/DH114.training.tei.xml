<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
<teiHeader>
<fileDesc xml:id="_1"/>
<encodingDesc>
<appInfo>
<application version="0.8.1" ident="GROBID" when="2024-11-15T07:09+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text xml:lang="en">
<p>The Digital Humanities was announced as a field in 2004 with the publication by Blackwell of A Companion to the Digital Humanities edited by three distinguished pioneers of digital methods in humanities scholarship, Susan Schreibman, Ray Siemens and John Unsworth. 1 Although the term digital humanities was introduced for the first time in the Companion, the work surveyed was the product of scholarly projects and experimentation stretching back more than 50 years. With the advent of the World Wide Web in the 1990s, the range of this work considerably expanded, justifying the introduction of the new term 'Digital Humanities' but, the editors of the Companion insisted, the aim remained the same: "using information technology to illuminate the human record, and bringing an understanding of the human record to bear on the development and use of information technology." 2 The editors of the Companion declared that its publication marked a turning point not only because of its use of the neologism Digital Humanities but also because for the first time a wide range of practitioners and specialists "have been brought together to consider digital humanities as a discipline in its own right, as well as to reflect on how it relates to areas of traditional humanities scholarship." 3 The Companion illustrated the wide range of disciplines exploring the possibilities offered by new digital tools and methods, including the study of literature, linguistics, history, archaeology, classics, lexicography, music, the performing arts and even philosophy and religion. Many of the methodological issues raised by the adoption of digital methods cut across disciplinary boundaries, and the Companion revealed many areas of shared concern across disciplines. In contrast to much conventional humanities work, methodological concerns loom large in the digital humanities, sometimes at the expense of the wider critical and interpretative issues. This common methodological core is one component uniting the digital humanities as a field.</p>
<p>The present volume documents an impressive range of projects undertaken under the programme 'Mixed Methods in the Humanities' funded by the Volkswagen Foundation. The aim of the Volkswagen Foundation programme harks back to themes evident in the 2004 Companion to the Digital Humanities. The projects supported by the Volkswagen Foundation programme addressed, on the one hand, major interpretative and critical themes in the humanities while, on the other hand, they sought to generate innovative computing methods and solutions. The Volkswagen Foundation programme confronted a tantalizing research question: how can the attempt to address qualitative-hermeneutical issues in the humanities drive forward innovative research in computer science? Achieving a balance between these two imperatives is a dilemma which frequently occurs in the digital humanities. The aims of the Volkswagen Foundation programme were explicitly mixed-the intention was as much to generate technological innovation as to address major issues in the humanities. Arguably a project might have been considered a success if it stimulated new discoveries in computer science but failed completely in its humanities research.</p>
<p>This 'mixed methods' vision, drawn from the ways in which Social Science research sometimes mixes quantitative and qualitative methods, is in many ways central to the digital humanities enterprise. It will be argued here that the idea of 'mixed methods' is a useful template in developing the digital humanities. This chapter will explore ways in which mixed methods fit into the wider concept of digital humanities and consider the extent to which the development of digital humanities since the middle of the 20 th century has been an exploration of mixed methods.</p>
<p>Traditionally, the humanities is seen as comprising such well-established disciplines as literature, history, classics and philology with more recent additions such as media and cultural studies. However, in the past 50 years a number of thematic approaches to the humanities have developed which use interdisciplinary and transdisciplinary perspectives to build new links across subject areas, often reaching beyond the humanities. They include medical humanities, health humanities, environmental humanities, public humanities, spatial humanities and bio-humanities. Digital humanities can be seen as one of these thematic flavours of the humanities. Like digital humanities, many of these thematic humanities only crystallized quite recently although work had been going on in the area for some decades. For example, the roots of the environmental humanities can be traced back more than a century, but as an increasing quantity of research at the intersection of history, indigenous studies, anthropology, philosophy, political theory and nonfiction writing emerged, a group of Australian researchers gave it the name 'ecological humanities'. By 2010, the wider term 'environmental humanities' had been widely adopted. It is perhaps natural to stress the humanities component in these designations but it is not necessarily so. In the case of medical humanities, for example, the emphasis is on the training of medical practitioners and enhancement of medical practice. It was for this reason that health humanities, which stressed the contribution humanities could make to well-being, emerged.</p>
<p>An interdisciplinary approach and enthusiasm for the possibilities of mixing methodologies are characteristic of all these thematic approaches to the humanities. These thematic humanities often describe themselves as working 'at the intersection' of a variety of disciplines. They frequently advocate a methodological bricolage. In a keynote address to a recent symposium on mixed methods in the environmental humanities, Sharlene Hesse-Biber has described the contribution that she feels mixed models can make in this field:</p>
<p>"I like to think of us taking a kind of mixed model approach where we have a set of questions that are talking to one another. Really, we're weaving a tale of how to figure out what's happening out in the environment, how to get all those stakeholders together. It's not kind of melding things together, but weaving and talking. This idea of weaving voices together [is] not so much in competition with one another but with the goal of complex understanding of meaning. I also like the idea of crossing boundaries; weaving and crossing borders is a way to get at the range of issues out there and to try to understand complex ways of dealing wpolitiqueith these issues."foot_0</p>
<p>This emphasis on crossing borders and weaving voices together describes very well some of the approaches taken by projects described in this volume, and illustrates how there is a great deal in common between these new thematic humanities such as digital humanities, environmental humanities, and health humanities.</p>
<p>Following the publication of the Companion in 2004, the term digital humanities quickly displaced its precursors such as 'Humanities Computing'. Many labs and professional organizations recast themselves as digital humanities organizations. An Alliance of Digital Humanities Organizations was established. 5 The major international conference in the field was renamed Digital Humanities, and now it attracts hundreds of delegates. As digital humanities rapidly became the accepted designation of the field, however, there was an explosion of literature debating its scope, range and nature. Indeed, in the 10 years after the publication of the Companion it seemed as if the field would be overwhelmed by discussion about the nature and scope of Digital Humanities. The Digital Humanities Manifesto 2.0, emulating such models as the Vorticist Blast! manifestos, saw digital humanities as a disruptive force that would transform the whole concept of the humanities: "Digital Humanities is not a unified field but an array of convergent practices that explore a universe in which: a) print is no longer the exclusive or the normative medium in which knowledge is produced and/or disseminated; instead, print finds itself absorbed into new, multimedia configurations; and b) digital tools, techniques, and media have altered the production and dissemination of knowledge in the arts, human and social sciences." 6 Such claims prompted further debates as to the level of formal technical knowledge required to practice digital humanities, whether the emphasis should be on building digital tools and resources or on critical frameworks, and how valid criticism driven by quantification or algorithms might be. 7 For some commentators, the adoption of digital methods was a youthful insurgency against a fuddy-duddy and hidebound humanities. Mark Sample notoriously commented that "The Digital Humanities should not be about the digital at all. It's all about innovation and disruption. The digital humanities is really an insurgent humanities." 8 The attacks on digital humanities by such eminent critics as Stanley Fish in 2012 were as much as anything a reaction to the exaggerated claims for the field made by Sample and others. 9 As Ted Underwood has remarked, these criticisms seem now to have subsided. 10 One of the concerns of Fish was that more traditional qualitative forms of humanities method might be sidelined by the rise of quantitative methods. In fact, to some degree Fish's criticisms can be seen as a plea for more mixed methods.</p>
<p>It is surprising that these debates about nature and scope of digital humanities should have become so heated, since earlier work on Humanities Computing had already established clear theoretical frameworks which with hindsight appeared surprisingly far-sighted and robust. One area in which it was clear from an early stage that humanities computing would be different to traditional work in the humanities was the need for the research to be undertaken by teams blending computing, humanities and other professional skills. In this respect, humanities computing was similar to other areas adopting new technologies. The importance of assembling teams with a wide range of specialist and professional skills has for example long been recognized in medical imaging. 11 How universities, which privilege specialist academic knowledge, accommodate the different blends of skills and knowledge necessary to use new digital methods, is a difficult issue, and one that has still not been satisfactorily resolved in either the humanities or the sciences.</p>
<p>These issues as to what humanities computing might be and what structures should support it were already the subject of lively debate in the academy in the 1990s. In Britain, a key moment in the development of digital humanities took place on the banks of Loch Lomond in Scotland in September 1996, when a meeting was held at the Buchanan Hotel entitled 'Defining Humanities Computing'. Attending the meeting were representatives of three leading universities which had been involved in the Computers in Teaching Initiative established in Britain in the early 1990s. 12 Many of the names are familiar still: Harold Short, Willard McCarty and Marilyn Deegan from King's College London; Christian Kay, Jean Anderson and Ann Gow from the University of Glasgow; and Stuart Lee, Mike Popham and Mike Fraser from the University of Oxford. The Buchanan Hotel meeting was perhaps the nearest thing to a digital humanities summit meeting that has ever taken place in Britain.</p>
<p>Many of the questions debated on the banks of Loch Lomond in 1996 remain current and relevant. How should we define humanities computing theoretically or pragmatically in terms of current practice? Where does humanities computing fit within institutions of higher education? How will computing integrate into standard humanities courses? What should research in humanities computing be about? For some attendees at the 1996 meeting, computing facilitated and supported academic research and the role of humanities computing specialists was analogous to that of lab technicians. For others, particularly Willard McCarty, who has been the most persistent and forceful advocate of this view in Britain, humanities computing-and thus by extension digital humanities-is a field of intellectual endeavour and investigation on a par with more traditional academic disciplines such as history, classics or media studies.</p>
<p>In the course of the discussions in Scotland, Willard drafted the following definition of the field as it appeared to him in 1996:</p>
<p>"HUMANITIES COMPUTING is an academic field concerned with the application of computing tools to humanities and arts data or their use in the creation of these data. It is methodological in nature and interdisciplinary in scope. It works at the intersection of computing with the other disciplines and focuses both on the pragmatic issues of how computing assists scholarship and teaching in these disciplines, and on the theoretical problems of shift in perspective brought about by computing. It seeks to define the common ground of techniques and approaches to data, and how scholarly processes may be understood and mechanized. It studies the sociology of knowledge as this is affected by computing as well as the fundamental cognitive problem of how we know what we know. Within the institution, humanities computing is manifested in teaching, research, and service. The subject itself is taught, as well as its particular application to another discipline at the invitation of the home department. Practitioners of humanities computing conduct their own research as well as participate by invitation in the projects of others. They take as a basic responsibility collegial service, assisting colleagues in their work and collaborating with them in the training of students."</p>
<p>This is a beautifully crafted working definition which applies as much to the digital humanities today as to the humanities computing of 1996. The emphasis on the sociology of knowledge is just as important in the age of social media and Cambridge Analytica as it was in the early days of the Web. But there is an air of passivity and reticence about the definition. There is a focus on reproducing existing scholarly technique rather than on investigating how digital methods can transform scholarship.</p>
<p>It is assumed that computers assist scholarship. The way in which digital humanities has moved on towards more experimental projects that might seek to transcend the limitations of existing scholarly method is amply illustrated by the portfolio of projects financed by the Volkswagen Foundation discussed in this volume. But nevertheless these projects all fall within Willard McCarty's broad 1996 definition of humanities computing as a field concerned with the application of computing tools to humanities and arts data. Like the projects discussed in 1996, they are methodological in nature and interdisciplinary in scope.</p>
<p>Another issue that McCarty's definition skirts around is the methodological emphasis of the digital humanities. While emphasising the importance of methodological discussion in the digital humanities, the Loch Lomond formulation stops short of defining how these methodological concerns shape and characterize the digital humanities. One common feature of digital humanities methodologies is the importance of mixed methods, the connecting thread of the Volkswagen Foundation projects discussed in this volume. The heterogeneous nature of humanities data, with their frequent silences, disjunctures and deceptions, means that they cannot always be convincingly and credibly used in conjunction with quantitative techniques. Exploring the potential and pitfalls of such mixed methods is at the heart of the digital humanities.</p>
<p>Indeed, much of the history of the digital humanities can be seen as a dialectic around such mixed methods-a debate as to the extent to which quantitative meth-ods can be used in conjunction with more hermeneutic forms of analysis, a wariness as to what are the appropriate boundaries of automated analysis and an anxiety to ensure that the critic's voice ultimately remains a human one. In many ways, viewing the history of humanities computing and then digital humanities as a series of debates and exercises exploring these boundaries between the computational and the manual is a useful perspective from which to view the development of digital humanities.</p>
<p>These anxieties about the most appropriate balance between an automated and manual method were expressed from the time that computers began to appear in the 1960s, as is apparent from three articles in a 1965 issue of the Journal of Higher Education. Ephim Fogel, while insisting that he saw computers as potentially valuable to humanities scholars, expressed concern as to how the lone humanities scholar would have the time and resources to deploy computing in research or be able to amass sufficient data for quantifiable analysis. 13 Allan Markman, while envisaging that one day all the literature of the world would be available for searching on magnetic tape, nevertheless emphasized that the machine was only a tool: "Man made the machine, and men will use it as a tool. By itself it is nothing." 14 All these were concerns about how far mixed methods could be implemented in literary studies. The historian Franklin Pegues was cautiously optimistic about the assistance computers could provide but insisted that machines must be kept in their place:</p>
<p>"The machine must serve to free the humanist from time-consuming labor and enlarge his horizons for greater and more important accomplishments. In short, the machine can help the scholar be a better humanist. It will be a sad day for the humanities if scholars seek to undertake only that work which can be computeroriented." 15 This assumption that the machine must only ever be a tool and subservient to the higher aims of humanities scholarship remains common. Again, it can be interpreted as nervousness about the nature and extent to which the use of digital technology in humanities scholarship promotes and requires the use of mixed methods.</p>
<p>The same tension between quantification and hermeneutics as humanities scholars began to explore the potential of computers can be seen in a landmark article by the archaeologist David Clarke published in 1973, ' Archaeology: the Loss of Innocence'. 16 Clarke described how changes in the technological environment as a result of the Second World War had fostered the professionalization of archaeology. Among the new possibilities which had precipitated that process was the computer which, in Clarke's words, "provides an expanding armoury of analogue and digital techniques for computation, experimentation, simulation, visual display and graphic analysis … They also provide powerful hammer-and-anvil procedures to beat out archaeological theory from intransigent data". 17 Clarke went on to observe that "A major embarrassment of the computer has been that it enabled us to do explicitly things which we had always claimed to do intuitively. In many cases it is now painfully clear that we were not only deceived by the intuitions of innocence but that many of the things we wished to do could not be done or were not worth doing and many unimagined things can be done". 18 The Processual Archaeology or New Archaeology advocated by Clarke placed computer analysis at the heart of a rigorous scientific approach to archaeological data. But the overemphasis of processual archaeology on quantification brought charges of ignoring human agency. Computers became associated in Archaeology with number-crunching, and it is only more recently with the rise of new non-quantitative techniques such as 3D imaging and GIS that computers have again emerged as key archaeological tools. 19 Similar anxieties as a result of the false assumption that computers were chiefly about counting were also evident at the same period in discussions in the historical discipline, particularly in the debates around cliometrics. 20 These examples illustrate the varied roots of the digital humanities. It is unfortunate that the growth of the digital humanities is often presented in an over-simplified form. The origins of the digital humanities are usually linked to the pioneering work of Father Roberto Busa who in the 1940s collaborated with IBM to create a massive computerized index to the works of St Thomas Aquinas. There is no doubting the scale and influence of Busa's achievement, using first punch card and then magnetic tape to process 22 million words, finally producing the 20 million lines and 65,000 pages of the Index Thomisticus in 1980. Busa's claim to be the founding father of the digital humanities was reinforced by the fact that he provided a foreword to the 2004 Companion. Question marks have been raised against Busa's legacy recently-it has been pointed out that many of those who entered Busa's data were anonymous forgotten women, and the fact that IBM had in the 1940s collaborated with fascist regimes in Europe has recently been pointed out. 21 The main problem with the Busa digital humanities foundation myth, however, is the way in which it restricts perspectives on the development of the digital humanities to a single line of development which privileges the encoding and manipulation of text. There are many other routes of development of digital humanities which have been given less prominence. If more attention were paid to these other paths of development of digital humanities, a richer view of the range and potential of digital humanities would emerge, and some of the circular debates about the nature of the digital humanities and its place in scholarship might be avoided.</p>
<p>For example, the roots of many of the large digital packages most widely used by humanities researchers in the English-speaking world lay in the library and commercial world and followed a different path of development. Libraries quickly realized the potential of computing to streamline cataloguing work, particularly by sharing records compiled to common bibliographic standards. A pioneer of this work was Henrietta Avram (1919-2006), a computer specialist at the Library of Congress, who in the 1960s developed the standard for automated library catalogue records, the Machine Readable Catalogue (MARC). 22 MARC was adopted in the 1970s for large-scale bibliographical enterprises such as the Eighteenth Century Short Title Catalogue which afterwards became the English Short Title Catalogue (ESTC), listing publications in Britain and North America from 1473 to 1800. The availability of the ESTC MARC records both online and in CD ROM from the 1980s enabled many new lines of research into the printed record of the early modern period to be more readily pursued, such as commercial partnerships in printing or the geographical distribution of printers.</p>
<p>While libraries were increasingly able to share and distribute catalogue information in printed form, great strides were also being made in distributing images of 21 M. the books and manuscripts held by libraries and archives. As early as 1938, H.G. Wells envisaged a world brain consisting of a universal library on microfilm. The British Museum became interested in the extent to which microfilming could reduce wear and tear on its collections and in 1935 Eugene Power, who afterwards established University Microfilms International which pioneered the large-scale microfilm publication of primary materials, helped set up a programme at the British Museum for the microfilming of rare books. By the 1970s, microform publication was a major commercial activity and firms such as Universal Microfilms, Primary Sources Microfilm (an imprint of Gale) and Chadwyck-Healey vied to maximize their coverage of such major bibliographic resources as the ESTC.</p>
<p>When digital imaging became more widespread in the 1980s, it was an obvious step to amalgamate scans of microfilm with bibliographic records. Thus, Early English Books Online (EEBO) combined ESTC records for 1473-1700 with digitized images from microfilm to provide users with access to the corpus of English language printing for this period. Subsequent projects went further, with Eighteenth Century Collections Online (ECCO) and the Burney Newspapers applying optical character recognition to the microfilm scans to try and produce fully searchable packages. The poor quality of the microfilm scans and the difficulties of automatically converting archaic letter forms mean that the accuracy of the searches performed in these packages is very low, but nevertheless these large-scale commercial packages, covering great swathes of the primary sources used by many humanities disciplines, have established themselves as indispensable tools.</p>
<p>Packages such as EEBO and ECCO represent a separate path of development in the digital humanities to the Busa foundation myth, one rooted in the adoption of computing in libraries and the introduction of imaging technologies such as microform. There is also a much stronger commercial element in this line of development, notwithstanding Busa's collaboration with IBM. The key players in bringing EEBO and ECCO to fruition were the microform publishers ProQuest (the successor to University Microfilms which also bought Chadwyck-Healey) and Gale. Moreover, the influence of these pioneering projects can also be seen in subsequent developments such as Google Books and the tools derived from it. Although digital imaging is a vast improvement on microfilm, not least because of its ability to readily provide colour images, much of our current use of digital imaging is dependent on methods and procedures established in large-scale microfilming projects in the 1930s.</p>
<p>In a similar fashion, the use of specialist imaging techniques such as multi-spectral imaging to examine historic records and recover faded text owes much to the pioneering work undertaken using ultraviolet and infrared photography in the first half of the twentieth century. The Benedictine monk Rudolph Kögel demonstrated in 1914 how ultraviolet fluorescence could be used to recover damaged text. 23 The Chaucer scholar John Manly presented an ultraviolet lamp to the British Museum in 1929, 24 and the roots of the forensic imaging, which has characterized many digital humanities projects ranging from the Electronic Beowulf to the recovery of the field diaries of the Victorian missionary David Livingstone, can be found in these early experiments. 25 Another major source which has appeared in the digital humanities but has also been overshadowed by the Busa legend is the use of databases in historical computing. Manfred Thaller, himself one of the great pioneers of the use of computing in history, has lamented how the way in which the early use of computing by historians relied on relational databases has been forgotten. 26 The punched cards which Busa used had been developed to enable automated processing of one of the great historical information sources, the census. Herman Hollerith pioneered the use of automated sorting of punch cards to expedite data handling in the 1890 US Census. 27 Historians quickly recognized the potential of methods used to deal with large amounts of data such as that generated by the census. From the mid-1950s, economic historians in the United States such as Robert Fogel, Douglass North and Lance Davies began to use computers to analyse more precisely topics such as the economics of slavery. 28 This encouraged British scholars such as Roderick Floud to use computing in investigating issues such as historic standards of living. 29 In France, Lucien Febvre, one of the founders of the Annales school of history, dreamt of historical laboratories similar to scientific ones, and this vision came closer to reality when historians such as Michel Vovelle used computers in the 1960s and 1970s to analyse thousands of wills. 30 The German quantitative historian and computer scientist, Manfred Thaller, in an interview with Julianne Nyhan gives a vivid account of the difficulties of developing databases for historical research in these early days. 31 There was no consensus about what tools and languages were most appropriate and generally the only way forward was to learn the programming language for oneself. Thaller began to imagine a package comparable to those used in Social Sciences which would be specifically geared to the difficulties of processing historical sources. It would enable the user to capture all the different features of a particular historical source and would also cope with the inconsistencies, gaps and uncertainties of historical sources. This historical software system, <rs xml:id="12973738" type="software">CLIO</rs>, was developed with support from the Volkswagen Foundation and became enormously influential in the 1980s and early 1990s. Although it has fallen from favour with the growth of the PC and generalized packages such as
<rs xml:id="12973739" type="publisher" corresp="12973740">Microsoft</rs>
<rs xml:id="12973740" type="software">Office</rs>, nevertheless there is a great deal of value in the way in which <rs xml:id="12973741" type="software">CLIO</rs> approaches historical sources. Thaller has continued to produce many other pioneering projects such as the digitization of the Duderstadt Municipal Archive, also funded by the Volkswagen Foundation, and in his retirement Professor Thaller is exploring how computers can be redesigned so as to better represent historical sources-a truly visionary project. 32 The roots of the Digital Humanities thus spread far and wide, considerably beyond the narrow scope implied by the suggestion that the field emerged from beneath the cloak of Father Busa. The work of film historians and pioneers of oral history also fed into the development of digital techniques. Research into the digital processing of sound and music has been enormously influential in the development of the digital humanities. Creative practice has been another major source of inspiration. Already by the 1950s, artists were experimenting with the creative possibilities of devices such as oscilloscope, plotter and early animation. In 1966, a group of contemporary artists joined forces with computer scientists and engineers from Bell Labs in the United States to host a series of performances using new technologies. A pioneering exhibition of algorithmic art called 'Cybernetic Serendipity' was held at the Institute of Contemporary Arts in 1968. 33 create the BBC's Radiophonics Workshop in the 1950s, as they do in the heritage of Roberto Busa.
</p>
<p>Once we understand the diverse roots of the digital arts and humanities, their multi-faceted and varied character makes more sense. There is no single discipline or clear-cut set of methods in the digital humanities; it is a creative and intellectual ecosystem. It is not surprising that this is a world of mixed methods.</p>
<p>There is no space here nor would it be practical to discuss how mixed methods might be deployed in future in the digital humanities and what this indicates for the development of the field. The various project reports assembled here illustrate some of the possibilities. Given the scope of the Digital Humanities, it is not surprising that these are wide-ranging. The ways in which digitization gives us new ways of viewing text inevitably loom large. The ways in which distant readings of text might combine both quantitative and qualitative insights is a rich area of investigation. It is also intriguing to consider how theoretical concepts from the humanities such as the idea of intertextuality can be measured and documented. Some of the most exciting challenges in using mixed methods in the digital humanities lie in multimedia-in developing ways of quantifying features of the vast image banks that have now been created or in exploring digital music. Reading over methodological insights from one area to the other remains one of the most exciting areas of development for Digital Humanities.</p>
<p>But perhaps the most tantalizing area for further research is that summarized by Manfred Thaller as follows:</p>
<p>"Historians face a specific challenge: they need to derive conclusions from evidence which is always incomplete, contradictory and anything but precise." 35 While Thaller insists on the importance of disciplinary focus, and is uneasy about the way in which the idea of digital humanities weakens these digital boundaries, it can be argued that this problem is one that occurs in many humanities disciplines-the primary source materials on which we rely are often vague, contradictory and full of gaps. It challenges many of the ideas of information which have underpinned ideas of computing since the time of Claude Shannon. In approaching these inconsistent and frequently misleading historical data, we need mixed methods, and it is for this reason that their potential, so well illustrated in this volume, is an exciting avenue for the digital humanities.</p>
<p>Johnson: The Digitization of the Burney Newspaper Collection", in: S. G. Brandtzaeg, P. Goring, and C. Watson (eds.), Travelling Chronicles: News and Newspapers from the Early Modern Period to the Eighteenth Century (Leiden: Brill, 2018), 49-71; and A. Prescott and L. Hughes, "Why Do We Digitise: the Case for Slow Digitisation", in: Archive Journal, September 2018, available at: htt ps://www.archivejournal.net/essays/why-do-we-digitize-the-case-for-slow-digitization/.</p>
<p>Sharlene Hesse-Biber, "Transcript of Key Note Address on Mixed Methods and Transformative Approaches to Social and Environmental Justice" in: Janet McIntyre-Mills and Norma R. A. Romm (eds.): Mixed Methods and Cross Disciplinary Research: Towards Cultivating Eco-Systemic Living (Cham: Springer, 2019), ix.</p>
<p>https://adho.org/about [accessed: 08.08.2021].</p>
<p>Thaller, "Vagueness and Uncertainty".</p>
</text>
</tei>