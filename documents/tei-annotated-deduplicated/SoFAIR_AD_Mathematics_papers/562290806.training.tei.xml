<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
<teiHeader>
<fileDesc xml:id="_1"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:45+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text xml:lang="en">
<p>Regression analysis makes up a large part of supervised machine learning, and consists of the prediction of a continuous independent target from a set of other predictor variables. The difference between binary classification and regression is in the target range: in binary classification, the target can have only two values (usually encoded as 0 and 1), while in regression the target can have multiple values. Even if regression analysis has been employed in a huge number of machine learning studies, no consensus has been reached on a single, unified, standard metric to assess the results of the regression itself. Many studies employ the mean square error (MSE) and its rooted variant (RMSE), or the mean absolute error (MAE) and its percentage variant (MAPE). Although useful, these rates share a common drawback: since their values can range between zero and +infinity, a single value of them does not say much about the performance of the regression with respect to the distribution of the ground truth elements. In this study, we focus on two rates that actually generate a high score only if the majority of the elements of a ground truth group has been correctly predicted: the coefficient of determination (also known as R-squared or R 2 ) and the symmetric mean absolute percentage error (SMAPE). After showing their mathematical properties, we report a comparison between R 2 and SMAPE in several use cases and in two real medical scenarios. Our results demonstrate that the coefficient of determination (R-squared) is more informative and truthful than SMAPE, and does not have the interpretability limitations of MSE, RMSE, MAE and MAPE. We therefore suggest the usage of R-squared as standard metric to evaluate regression analyses in any scientific domain.</p>
<p>The role played by regression analysis in data science cannot be overemphasised: predicting a continuous target is a pervasive task not only in practical terms, but also at a conceptual level. Regression is deeply investigated even nowadays, to the point of still being worth of considerations in top journals (Jaqaman &amp; Danuser, 2006;Altman &amp; Krzywinski, 2015;Krzywinski &amp; Altman, 2015), and widespread used also in the current Schielzeth, 2017;Menard, 2000) and to the proposal of ad-hoc variations (Young, 2000;Renaud &amp; Victoria-Feser, 2010;Lee et al., 2012).</p>
<p>Parallel to the model explanation expressed as the variance, another widely adopted family of measures evaluate the quality of fit in terms of distance of the regressor to the actual training points. The two basic members of such family are the mean average error (MAE) (Sammut &amp; Webb, 2010a) and the mean squared error (MSE) (Sammut &amp; Webb, 2010b), whose difference lies in the evaluating metric, respectively linear L 1 or quadratic L 2 . Once more, the available references are numerous, related to both theoretical (David &amp; Sukhatme, 1974;Rao, 1980;So et al., 2013) and applicative aspects (Allen, 1971;Farebrother, 1976;Gilroy, Hirsch &amp; Cohn, 1990;Imbens, Newey &amp; Ridder, 2005;Köksoy, 2006;Sarbishei &amp; Radecka, 2011). As a natural derivation, the square root of mean square error (RMSE) has been widely adopted (Nevitt &amp; Hancock, 2000;Hancock &amp; Freeman, 2001;Applegate et al., 2003;Kelley &amp; Lai, 2011) to standardize the units of measures of MSE . The different type of regularization imposed by the intrinsic metrics reflects on the relative effectiveness of the measure according to the data structure. In particular, as a rule of thumb, MSE is more sensitive to outliers than MAE; in addition to this general note, several further considerations helping researchers in choosing the more suitable metric for evaluating a regression model given the available data and the target task can be drawn (Chai &amp; Draxler, 2014;Willmott &amp; Matsuura, 2005;Wang &amp; Lu, 2018). Within the same family of measures, the mean absolute percentage error (MAPE) (De Myttenaere et al., 2016) focuses on the percentage error, being thus the elective metric when relative variations have a higher impact on the regression task rather than the absolute values. However, MAPE is heavily biased towards low forecasts, making it unsuitable for evaluating tasks where large errors are expected (Armstrong &amp; Collopy, 1992;Ren &amp; Glasure, 2009;De Myttenaere et al., 2015). Last but not least, the symmetric mean absolute percentage error (SMAPE) (Armstrong, 1985;Flores, 1986;Makridakis, 1993) is a recent metric originally proposed to solve some of the issues related to MAPE. Despite the yet not reached agreement on its optimal mathematical expression (Makridakis &amp; Hibon, 2000;Hyndman &amp; Koehler, 2006;Hyndman, 2014;Chen, Twycross &amp; Garibaldi, 2017), SMAPE is progressively gaining momentum in the machine learning community due to its interesting properties (Maiseli, 2019;Kreinovich, Nguyen &amp; Ouncharoen, 2014;Goodwin &amp; Lawton, 1999),</p>
<p>An interesting discrimination among the aforementioned metrics can be formulated in terms of their output range. The coefficient of determination is upper bounded by the value 1, attained for perfect fit; while R 2 is not lower bounded, the value 0 corresponds to (small perturbations of) the trivial fit provided by the horizontal line y = K for K the mean of the target value of all the training points. Since all negative values for R 2 indicate a worse fit than the average line, nothing is lost by considering the unit interval as the meaningful range for R 2 . As a consequence, the coefficient of determination is invariant for linear transformations of the independent variables' distribution, and an output value close to one yields a good prediction regardless of the scale on which such variables are measured (Reeves, 2021). Similarly, also SMAPE values are bounded, with the lower bound 0% implying a perfect fit, and the upper bound 200% reached when all the predictions and the actual target values are of opposite sign. Conversely, MAE, MSE, RMSE and MAPE output spans the whole positive branch of the real line, with lower limit zero implying a perfect fit, and values progressively and infinitely growing for worse performing models. By definition, these values are heavily dependent on the describing variables' ranges, making them incomparable both mutually and within the same metric: a given output value for a metric has no interpretable relation with a similar value for a different measure, and even the same value for the same metric can reflect deeply different model performance for two distinct tasks (Reeves, 2021). Such property cannot be changed even if projecting the output into a bounded range through a suitable transformation (for example, arctangent or rational function). Given these interpretability issues, here we concentrate our comparative analysis on R 2 and SMAPE, both providing a high score only if the majority of the ground truth training points has been correctly predicted by the regressor. Showing the behaviour of these two metrics in several use cases and in two biomedical scenarios on two datasets made of electronic health records, the coefficient of determination is demonstrated to be superior to SMAPE in terms of effectiveness and informativeness, thus being the recommended general performance measure to be used in evaluating regression analyses.</p>
<p>The manuscript organization proceeds as follows. After this Introduction, in the Methods section we introduce the cited metrics, with their mathematical definition and their main properties, and we provide a more detailed description of R 2 and SMAPE and their extreme values ("Methods"). In the following section Results and Discussion, we present the experimental part ("Results and Discussion"). First, we describe five synthetic use cases, then we introduce and detail the Lichtinghagen dataset and the Palechor dataset of electronic health records, together with the different applied regression models and the corresponding results. We complete that section with a discussion of the implication of all the obtained outcomes. In the Conclusions section, we draw some final considerations and future developments ("Conclusions").</p>
<p>In this section, we first introduce the mathematical background of the analyzed rates ("Mathematical Background"), then report some relevant information about the coefficient of determination and SMAPE ("R-squared and SMAPE").</p>
<p>In the following formulas, X i is the predicted i th value, and the Y i element is the actual i th value. The regression method predicts the X i element for the corresponding Y i element of the ground truth dataset. Define two constants: the mean of the true values</p>
<p>and the mean total sum of squares</p>
<p>The coefficient of determination (Wright, 1921) can be interpreted as the proportion of the variance in the dependent variable that is predictable from the independent variables.</p>
<p>Mean square error (MSE)</p>
<p>(best value = 0; worst value = +∞) MSE can be used if there are outliers that need to be detected. In fact, MSE is great for attributing larger weights to such points, thanks to the L 2 norm: clearly, if the model eventually outputs a single very bad prediction, the squaring part of the function magnifies the error.</p>
<p>Since R 2 ¼ 1 À MSE MST and since MST is fixed for the data at hand, R 2 is monotonically related to MSE (a negative monotonic relationship), which implies that an ordering of regression models based on R 2 will be identical (although in reverse order) to an ordering of models based on MSE or RMSE.</p>
<p>Root mean square error (RMSE)</p>
<p>(best value = 0; worst value = +∞)</p>
<p>The two quantities MSE and RMSE are monotonically related (through the square root). An ordering of regression models based on MSE will be identical to an ordering of models based on RMSE.</p>
<p>Mean absolute error (MAE)</p>
<p>(best value = 0; worst value = +∞) MAE can be used if outliers represent corrupted parts of the data. In fact, MAE is not penalizing too much the training outliers (the L 1 norm somehow smooths out all the errors of possible outliers), thus providing a generic and bounded performance measure for the model. On the other hand, if the test set also has many outliers, the model performance will be mediocre.</p>
<p>Mean absolute percentage error (MAPE)</p>
<p>(best value = 0; worst value = +∞) MAPE is another performance metric for regression models, having a very intuitive interpretation in terms of relative error: due to its definition, its use is recommended in tasks where it is more important being sensitive to relative variations than to absolute variations (De Myttenaere et al., 2016). However, its has a number of drawbacks, too, the most critical ones being the restriction of its use to strictly positive data by definition and being biased towards low forecasts, which makes it unsuitable for predictive models where large errors are expected (Armstrong &amp; Collopy, 1992).</p>
<p>Symmetric mean absolute percentage error (SMAPE)</p>
<p>(best value = 0; worst value = 2) Initially defined by Armstrong (1985), and then refined in its current version by Flores (1986) and Makridakis (1993), SMAPE was proposed to amend the drawbacks of the MAPE metric. However, there is little consensus on a definitive formula for SMAPE, and different authors keep using slightly different versions (Hyndman, 2014). The original SMAPE formula defines the maximum value as 200%, which is computationally equivalent to 2. In this manuscript, we are going to use the first value for formal passages, and the second value for numeric calculations.</p>
<p>The rates RMSE, MAE, MSE and SMAPE have value 0 if the linear regression model fits the data perfectly, and positive value if the fit is less than perfect. Furthermore, the coefficient of determination has value 1 if the linear regression model fits the data perfectly (that means if MSE = 0), value 0 if MSE = MST, and negative value if the mean squared error, MSE, is greater than mean total sum of squares, MST.</p>
<p>Even without digging into the mathematical properties of the aforementioned statistical rates, it is clear that it is difficult to interpret sole values of MSE, RMSE, MAE and MAPE, since they have +∞ as upper bound. An MSE = 0.7, for example, does not say much about the overall quality of a regression model : the value could mean both an excellent regression model and a poor regression model . We cannot know it unless the maximum MSE value for the regression task is provided or unless the distribution of all the ground truth values is known. The same concept is valid for the other rates having +∞ as upper bound, such as RMSE, MAE and MAPE.</p>
<p>The only two regression scores that have strict real values are the non-negative R-squared and SMAPE. R-squared can have negative values, which mean that the regression performed poorly. R-squared can have value 0 when the regression model explains none of the variability of the response data around its mean (Minitab Blog Editor, 2013).</p>
<p>The positive values of the coefficient of determination range in the [0, 1] interval, with 1 meaning perfect prediction. On the other side, the values of SMAPE range in the [0, 2], with 0 meaning perfect prediction and 2 meaning worst prediction possible. This is the main advantage of the coefficient of determination and SMAPE over RMSE, MSE, MAE, and MAPE: values like R 2 = 0.8 and SMAPE = 0.1, for example, clearly indicate a very good regression model performance, regardless of the ranges of the ground truth values and their distributions. A value of RMSE, MSE, MAE, or MAPE equal to 0.7, instead, fails to inform us about the quality of the regression performed.</p>
<p>This property of R-squared and SMAPE can be useful in particular when one needs to compare the predictive performance of a regression on two different datasets having different value scales. For example, suppose we have a mental health study describing a predictive model where the outcome is a depression scale ranging from 0 to 100, and another study using a different depression scale, ranging from 0 to 10 (Reeves, 2021). Using R-squared or SMAPE we could compare the predictive performance of the two studies without making additional transformations. The same comparison would be impossible with RMSE, MSE, MAE, or MAPE.</p>
<p>Given the better robustness of R-squared and SMAPE over the other four rates, we focus the rest of this article on the comparison between these two statistics.</p>
<p>The coefficient of determination can take values in the range (-∞, 1] according to the mutual relation between the ground truth and the prediction model. Hereafter we report a brief overview of the principal cases.</p>
<p>R 2 ≥ 0: With linear regression with no constraints, R 2 is non-negative and corresponds to the square of the multiple correlation coefficient.</p>
<p>R 2 = 0: The fitted line (or hyperplane) is horizontal. With two numerical variables this is the case if the variables are independent, that is, are uncorrelated. Since R 2 ¼ 1 À MSE MST , the relation R 2 = 0 is equivalent to MSE = MST, or, equivalently, to:</p>
<p>Now, Eq. 9 has the obvious solution X i = Y for 1 ≤ i ≤ m, but, being just one quadratic equation with m unknowns X i , it has infinite solutions, where X i = Y ± ɛ i for a small ɛ i , as shown in the following example: .317571, 40.336481, 5.619065,44.529437, 71.192687, 32.036909, 6.977097, 66.425010, 95.971166, 5 02545, 43.75556, 41.18064, 42.09511, 44.85773, 44.09390, 41.58419, 43.25487, 44.27568, 49.75250} MSE = MST = 1051.511</p>
<p>R 2 &lt; 0: This case is only possible with linear regression when either the intercept or the slope are constrained so that the "best-fit" line (given the constraint) fits worse than a horizontal line, for instance if the regression line (hyperplane) does not follow the data (CrossValidated, 2011b). With nonlinear regression, the R-squared can be negative whenever the best-fit model (given the chosen equation, and its constraints, if any) fits the data worse than a horizontal line. Finally, negative R 2 might also occur when omitting a constant from the equation, that is, forcing the regression line to go through the point (0,0).</p>
<p>A final note. The behavior of the coefficient of determination is rather independent from the linearity of the regression fitting model: R 2 can be very low even for completely linear model, and vice versa, a high R 2 can occur even when the model is noticeably nonlinear. In particular, a good global R 2 can be split in several local models with low R 2 (CrossValidated, 2011a). SMAPE By definition, SMAPE values range between 0% and 200%, where the following holds in the two extreme cases: SMAPE = 0: The best case occurs when SMAPE vanishes, that is when</p>
<p>and, since the m components are all positive, equivalent to</p>
<p>By the triangle inequality |a + c| ≤ |a| + | c| computed for b = -c, we have that |a -b| ≤ |a| + |b| and thus fjaÀbj jajþjbj 1 . This yields that SMAPE = 2 if jX i ÀY i j jX i jþjY i j ¼ 1 for all i = 1,…,m. Thus we reduced to compute when nða; bÞ ¼ jaÀbj jajþjbj ¼ 1: we analyse now all possible cases, also considering the symmetry of the relation with respect to a and b, ξ(a,b) = ξ(b,a).</p>
<p>If a = 0, nð0; bÞ</p>
<p>Then nða; bÞ ¼ nðb þ e; eÞ ¼ e 2bþe &lt; 1. Same happens when a,b &lt; 0: thus, if ground truth points and the prediction points have the same sign, SMAPE will never reach its maximum value.</p>
<p>Finally, suppose that a and b have opposite sign, for instance a &gt; 0 and b &lt; 0. Then b = -c, for c &gt; 0 and thus nða; bÞ ¼ nða; ÀcÞ ¼ jaþcj jajþjcj ¼ aþc aþc ¼ 1. Summarising, SMAPE reaches its worst value 200% if</p>
<p>that is, ground truth and prediction always have opposite sign, regardless of their values.</p>
<p>For instance, if the ground truth points are (1, -2, 3, -4, 5, -6, 7, -8, 9, -10) , any prediction vector with all opposite signs (for example, (-307.18, 636.16, -469.99, 671.53, -180.55, 838.23, -979.18 , 455.16, -8.32, 366.80) ) will result in a SMAPE metric reaching 200%.</p>
<p>Explained the extreme cases of R-squared and SMAPE, in the next section we illustrate some significant, informative use cases where these two rates generate discordant outcomes.</p>
<p>In this section, we first report some particular use cases where we compare the results of R-squared and SMAPE ("Use Cases"), and then we describe a real biomedical scenario where the analyzed regression rates generate different rankings for the methods involved ("Medical Scenarios").</p>
<p>As mentioned earlier, we exclude MAE, MSE, RMSE and MAPE from the selection of the best performing regression rate. These statistics range in the [0, +∞) interval, with 0 meaning perfect regression, and their values alone therefore fail to communicate the quality of the regression performance, both on good cases and in bad cases. We know for example that a negative coefficient of determination and a SMAPE equal to 1.9 clearly correspond to a regression which performed poorly, but we do not have a specific value for MAE, MSE, RMSE and MAPE that indicates this outcome. Moreover, as mentioned earlier, each value of MAE, MSE, RMSE and MAPE communicates the quality of the regression only relatively to other regression performances, and not in an absolute manner, like R-squared and SMAPE do. For these reasons, we focus on the coefficient of determination and SMAPE for the rest of our study.</p>
<p>We list hereafter a number of example use cases where the coefficient of determination and SMAPE produce divergent outcomes, showing that R 2 is more robust and reliable than SMAPE, especially on bad poor quality regressions. To simplify comparison between the two measures, define the complementary normalized SMAPE as:</p>
<p>(worst value = 0; best value = 1) UC1 use case Consider the ground truth set REAL ¼ fr i ¼ ði; iÞ 2 R 2 ; i 2 N; 1 i 100g collecting 100 points with positive integer coordinates on the straight line y = x. Define then the set PRED j = {p i } as</p>
<p>so that REAL and PRED j coincides apart from the first j points 1, 6, 11,… congruent to 1 modulo 5 that are set to 0. Then, for each 5 ≤ j ≤ 20, compute R 2 and cnSMAPE (Table 1).</p>
<p>Both measures decrease with the increasing number of non-matching points p 5k + 1 = 0, but cnSMAPE decreases linearly, while R 2 goes down much faster, better showing the growing unreliability of the predicted regression. At the end of the process, j = 20 points out of 100 are wrong, but still cnSMAPE is as high as 0.80, while R 2 is 0.236, correctly declaring PRED 20 a very weak prediction set.</p>
<p>UC2 use case In a second example, consider again the same REAL dataset and define the three predicting sets</p>
<p>In all the three cases start, middle, end the predicting set coincides with REAL up to 10 points that are set to zero, at the beginning, in the middle and at the end of the prediction, respectively. Interestingly, cnSMAPE is 0.9 in all the three cases, showing that SMAPE is sensible only to the number of non-matching points, and not to the magnitude of the predicting error. R 2 instead correctly decreases when the zeroed sequence of points is further away in the prediction and thus farthest away from the actual values: R 2 is 0.995 for PRED start , 0.6293 for PRED middle and -0.0955 for PRED end .</p>
<p>UC3 use case Consider now the as the ground truth the line y = x, and sample the set T including twenty positive integer points</p>
<p>as the same points of T with a small amount of noise N (i) on the y axes, so that r i are close but not lying on the y = x straight line. Consider now two predicting regression models:</p>
<p>The set PRED c = T representing the correct model;</p>
<p>The set PRED w representing the (wrong) model with points defined as p w i = f(x i ), for f the 10-th degree polynomial exactly passing through the points r i for 1 ≤ i ≤ 10.</p>
<p>Clearly, p w i coincides with r i for 1 ≤ i ≤ 10, but ||p w ir i || becomes very large for i ≥ 11. On the other hand t i ≠ r i for all i's, but ||t ir i || is always very small. Compute now the two measures R 2 and cnSMAPE on the first N points i = 1, …, N for 2 ≤ N ≤ 20 of the two different regression models c and w with respect to the ground truth set REAL (Table 2).</p>
<p>For the correct regression model, both measures are correctly showing good results. For the wrong model, both measures are optimal for the first 10 points, where the prediction exactly matches the actual values; after that, R 2 rapidly decreases supporting the inconsistency of the model, while cnSMAPE is not affected that much, arriving for N = 20 to a value 1/2 as a minimum, even if the model is clearly very bad in prediction.</p>
<p>Values generated through Eq. ( 11). R 2 , coefficient of determination (Eq. ( 3)). cnSMAPE, complementary normalized SMAPE (Eq. ( 10)). UC4 use case Consider the following example: the seven actual values are (1, 1, 1, 1, 1, 2, 3) , and the predicted values are (1, 1, 1, 1, 1, 1, 1) . From the predicted values, it is clear that the regression method worked very poorly: it predicted 1 for all the seven values.</p>
<p>If we compute the coefficient of determination and SMAPE here, we obtain R-squared = -0.346 and SMAPE = 0.238. The coefficient of determination illustrates that something is completely off, by having a negative value. On the contrary, SMAPE has a very good score, that corresponds to 88.1% correctness in the cnSMAPE scale.</p>
<p>In this use case, if a inexperienced practitioner decided to check only the value of SMAPE to evaluate her/his regression, she/he would be misled and would wrongly believe that the regression went 88.1% correct. If, instead, the practitioner decided to verify the value of R-squared, she/he would be alerted about the poor quality of the regression. As we saw earlier, the regression method predicted 1 for all the seven ground truth elements, so it clearly performed poorly.</p>
<p>Let us consider now a vector of 5 integer elements having values (1, 2, 3, 4, 5) , and a regression prediction made by the variables (a, b, c, d, e) . Each of these variables can assume all the integer values between 1 and 5, included. We compute the coefficient of determination and cnSMAPE for each of the predictions with respect to the actual values. To compare the values of the coefficient of determination and cnSMAPE in the same range, we consider only the cases when R-squared is greater or equal to zero, and we call it non-negative R-squared. We reported the results in Fig. 1. As clearly observable in the plot Fig. 1, there are a number of points where cnSMAPE has a high value (between 0.6 and 1) but R-squared had value 0: in these cases, the coefficient of determination and cnSMAPE give discordant outcomes. One of these cases, for example, is the regression where the predicted values have values (1, 2, 3, 5, 2) , R 2 = 0, and cnSMAPE = 0.89.</p>
<p>In this example, cnSMAPE has a very high value, meaning that the prediction is 89% correct, while R 2 is equal to zero. The regression correctly predicts the first three points (1, 2, 3) , but fails to classify the forth element (4 is wrongly predicted as 5), and the fifth element (5 is mistakenly labeled as 2). The coefficient of determination assigns a bad outcome to this regression because it fails to correctly classify the only members of the 4 and 5 classes. Diversely, SMAPE assigns a good outcome to this prediction because the variance between the actual values and the predicted values is low, in proportion to the overall mean of the values.</p>
<p>Faced with this situation, we consider the outcome of the coefficient of determination more reliable and trustworthy: similarly to the Matthews correlation coefficient (MCC) (Matthews, 1975) in binary classification (Chicco &amp; Jurman, 2020;Chicco, Tötsch &amp; Jurman, 2021;Tötsch &amp; Hoffmann, 2021;Chicco, Starovoitov &amp; Jurman, 2021;Chicco, Warrens &amp; Jurman, 2021), R-squared generates a high score only if the regression is able to correctly classify most of the elements of each class. In this example, the regression fails to classify all the elements of the 4 class and of the 5 class, so we believe a good metric would communicate this key-message.</p>
<p>To further investigate the behavior of R-squared, MAE, MAPE, MSE, RMSE and SMAPE, we employed these rates to a regression analysis applied to two real biomedical applications.</p>
<p>We trained and applied several machine learning regression methods on the Lichtinghagen dataset (Lichtinghagen et al., 2013;Hoffmann et al., 2018), which consists of electronic health records of 615 individuals including healthy controls and patients diagnosed with cirrhosis, fibrosis, and hepatitis. This dataset has 13 features, including a numerical variable stating the diagnosis of the patient, and is publicly available in the University of California Irvine Machine Learning Repository (2020). There are 540 healthy controls (87.8%) and 75 patients diagnosed with hepatitis C (12.2%). Among the 75 patients diagnosed with hepatitis C, there are: 24 with only hepatitis C (3.9%); 21 with hepatitis C and liver fibrosis (3.41%); and 30 with hepatitis C, liver fibrosis, and cirrhosis (4.88%).</p>
<p>To further verify the effect of the regression rates, we applied the data mining methods to another medical dataset made of electronic health records of young patients with obesity (Palechor &amp; De-La-Hoz-Manotas, 2019, De-La-Hoz-Correa et al., 2019). This dataset is publicly available in the University of California Irvine Machine Learning Repository (2019) too, and contains data of 2,111 individuals, with 17 variables for each of them. A variable called
NObeyesdad indicates the obesity level of each subject, and can be employed as a regression target. In this dataset, there are 272 children with insufficient weight (12.88%), 287 children with normal weight (13.6%), 351 children with obesity type I (16.63%), 297 children with obesity type II (14.07%), 324 children with obesity type III (15.35%), 290 children with overweight level I (13.74%), and 290 children with overweight level II (13.74%). The original curators synthetically generated part of this dataset (Palechor &amp; De-La-Hoz-Manotas, 2019, De-La-Hoz-Correa et al., 2019).
</p>
<p>For the regression analysis, we employed the same machine learning methods two of us authors used in a previous analysis (Chicco &amp; Jurman, 2021): Linear Regression (Montgomery, Peck &amp; Vining, 2021), Decision Trees (Rokach &amp; Maimon, 2005), and Random Forests (Breiman, 2001), all implemented and executed in the
R programming language (Ihaka &amp; Gentleman, 1996). For each method execution, we first shuffled the patients data, and then we randomly selected 80% of the data elements for the training set and used the remaining 20% for the test set. We trained each method model on the training set, applied the trained model to the test set, and saved the regression results measured through R-squared, MAE, MAPE, MSE, RMSE, and SMAPE. For the hepatitis dataset , we imputed the missing data with the Predictive Mean Matching (PMM) approach through the Multiple Imputation by Chained Equations (MICE) method (Buuren &amp; Groothuis-Oudshoorn, 2010). We ran 100 executions and reported the results means and the rankings based on the different rates in Table 3 (hepatitis dataset) and in Table 4 (obesity dataset).
</p>
<p>Hepatitis dataset results: different rate, different ranking We measured the results obtained by these regression models on the Lichtinghagen hepatitis dataset with all the rates analyzed in our study: R 2 , MAE, MAPE, RMSE, MSE and SMAPE (lower part of Table 3).</p>
<p>These rates generate 3 different rankings. R 2 , MSE and RMSE share the same ranking (Random Forests, Linear Regression and Decision Tree). SMAPE and MAPE share the same ranking (Decision Tree, Random Forests and Linear Regression). MAE has its own ranking (Random Forests, Decision Tree and Linear Regression).</p>
<p>It is also interesting to notice that these six rates select different methods as top performing method. R 2 , MAE, MSE and RMSE indicate Random Forests as top performing regression model, while SMAPE and MAPE select Decision Tree for the first position in their rankings. The position of Linear Regression changes, too: on the second rank for R 2 , MSE and RMSE, while on the last rank for MAE, SMAPE and MAPE.</p>
<p>By comparing all these different standings, a machine learning practitioner could wonder what is the most suitable rate to choose, to understand how the regression experiments actually went and which method outperformed the others. As explained earlier, we suggest the readers to focus on the ranking generated by the coefficient of determination, because it is the only metric that considers the distribution of all the ground truth values, and generates a high score only if the regression correctly predict most of the values of each ground truth category. Additionally, the fact that the ranking indicated We performed the analysis on the Lichtinghagen dataset (Lichtinghagen et al., 2013;Hoffmann et al., 2018) with the methods employed by Chicco &amp; Jurman (2021). We report here the average values achieved by each method in 100 executions with 80% randomly chosen data elements used for the training set and the remaining 20% used for the test set. R 2 : worst value -∞ and best value +1. SMAPE: worst value 2 and best value 0. MAE, MAPE, MSE and RMSE: worst value +∞ and best value 0. We reported the complete regression results including the standard deviations in Table S1. R 2 formula: Eq. ( 3). MAE formula: Eq. ( 6). MAPE formula: Eq. ( 7). MSE formula: Eq. ( 4). RMSE formula: Eq. ( 5). SMAPE formula: Eq. ( 8). We highlighted the values of the coefficient of determination in bold.</p>
<p>by R-squared (Random Forests, Linear Regression and Decision Tree) was the same standing generated by 3 rates out of 6 suggests that it is the most informative one (Table 3).</p>
<p>Hepatitis dataset results: R 2 provides the most informative outcome Another interesting aspect of these results on the hepatitis dataset regards the comparison between coefficient of determination and SMAPE (Table 3). We do not compare the standing of R-squared with MAE, MSE, RMSE, and MAPE because these four rates can have infinite positive values and, as mentioned earlier, this aspect makes it impossible to detect the quality of a regression from a single score of these rates.</p>
<p>R-squared indicates a very good result for Random Forests (R 2 = 0.756), and good results for Linear Regression (R 2 = 0.535) and Decision Tree (R 2 = 0.423). On the contrary, SMAPE generates an excellent result for Decision Tree (SMAPE = 0.073), meaning almost perfect prediction, and poor results for Random Forests (SMAPE = 1.808) and Linear Regression (SMAPE = 1.840), very close to the upper bound (SMAPE = 2) representing the worst possible regression.</p>
<p>These values mean that the coefficient of determination and SMAPE generate discordant outcomes for these two methods: for R-squared, Random Forests made a very good regression and Decision Tree made a good one; for SMAPE, instead, Random Forests made a catastrophic regression and Decision Tree made an almost perfect one. At this point, a practitioner could wonder which algorithm between Random Forests and Decision Trees made the better regression. Checking the standings of the other rates, we clearly see that Random Forests resulted being the top model for 4 rates out of 6, while Decision Tree resulted being the worst model for 3 rates out of 6. This information confirms that the ranking of R-squared is more reliable than the one of SMAPE (Table 3).</p>
<p>Obesity dataset results: agreement between rankings, except for SMAPE Differently from the rankings generated on the hepatitis dataset, the rankings produced on the obesity dataset are more concordant (Table 4). Actually, the ranking of the S2. R 2 formula: Eq. ( 3). MAE formula: Eq. ( 6). MAPE formula: Eq. ( 7). MSE formula: Eq. ( 4). RMSE formula: Eq. ( 5). SMAPE formula: Eq. ( 8). We highlighted the values of the coefficient of determination in bold. The only significant difference can be found in the SMAPE standing: differently from the other rankings that all put Decision Tree as second best regressor and Linear Regression as worst regressor, the SMAPE standing indicates Linear Regression as runner-up and Decision Tree on the last position. SMAPE, in fact, swaps the positions of these two methods, compared to R-squared and the other rates: SMAPE says Linear Regression outperformed Decision Tree, while the other rates say that Decision Tree outperformed Linear Regression.</p>
<p>Since five out of six rankings confirm that Decision Tree generated better results than Linear Regression, and only one of six say vice versa, we believe that is clear that the ranking indicated by the coefficient of determination is more informative and trustworthy than the ranking generated by SMAPE.</p>
<p>Even if regression analysis makes a big chunk of the whole machine learning and computational statistics domains, no consensus has been reached on a unified prefered rate to evaluate regression analyses yet. In this study, we compared several statistical rates commonly employed in the scientific literature for regression task evaluation, and described the advantages of R-squared over SMAPE, MAPE, MAE, MSE and RMSE.</p>
<p>Despite the fact that MAPE, MAE, MSE and RMSE are commonly used in machine learning studies , we showed that it is impossible to detect the quality of the performance of a regression method by just looking at their singular values. An MAPE of 0.7 alone, for example, fails to communicate if the regression algorithm performed mainly correctly or poorly. This flaw left room only for R 2 and SMAPE. The first one has negative values if the regression performed poorly, and values between 0 and 1 (included) if the regression was good. A positive value of R-squared can be considered similar to percentage of correctness obtained by the regression. SMAPE, instead, has the value 0 as best value for perfect regressions and has the value 2 as worst value for disastrous ones.</p>
<p>In our study, we showed with several use cases and examples that R 2 is more truthful and informative than SMAPE: R-squared, in fact, generates a high score only if the regression correctly predicted most of the ground truth elements for each ground truth group, considering their distribution. SMAPE, instead, focuses on the relative distance between each predicted value and its corresponding ground truth element, without considering their distribution. In the present study SMAPE turned out to perform bad in identifying bad regression models.</p>
<p>A limitation of R 2 arises in the negative space. When R-squared has negative values, it indicates that the model performed poorly but it is impossible to know how bad a model performed. For example, an R-squared equal to -0.5 alone does not say much about the quality of the model, because the lower bound is -∞. Differently from SMAPE that has values between 0 and 2, the minus sign of the coefficient of determination would however clearly inform the practitioner about the poor performance of the regression.</p>
<p>Although regression analysis can be applied to an infinite number of different datasets, with infinite values, we had to limit the present to a selection of cases, for feasibility purposes. The selection of use cases presented here are to some extent limited, since one could consider infinite many other use cases that we could not analyze here. Nevertheless, we did not find any use cases in which SMAPE turned out to be more informative than R-squared. Based on the results of this study and our own experience, R-squared seems to be the most informative rate in many cases, if compared to SMAPE, MAPE, MAE, MSE and RMSE. We therefore suggest the employment of R-squared as the standard statistical measure to evaluate regression analyses, in any scientific area.</p>
<p>In the future, we plan to compare R 2 with other regression rates such as Huber metric H δ (Huber, 1992), LogCosh loss (Wang et al., 2020) and Quantile Q γ (Yue &amp; Rue, 2011). We will also study some variants of the coefficient of determination, such as the adjusted R-squared (Miles, 2014) and the coefficient of partial determination (Zhang, 2017). Moreover, we will consider the possibility to design a brand new metric for regression analysis evaluation, that could be even more informative than R-squared.</p>
<p>Note:</p>
<p>We define N, correct model, and wrong model in the UC3 Use case paragraph. R 2 , coefficient of determination (Eq. (</p>
<p>3</p>
<p>)). cnSMAPE, complementary normalized SMAPE (Eq. (</p>
<p>10</p>
<p>)). Chicco et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.623 12/24</p>
<p>Chicco et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.623 16/24 coefficient</p>
<p>Chicco et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.623</p>
<p>The authors thank David Reeves (the University of Manchester) for his useful advice.</p>
<p>The authors received no funding for this work.</p>
<p>The following information was supplied regarding data availability:</p>
<p>The
<rs xml:id="12894730" type="software" subtype="implicit">software
code</rs> is available at
GitHub:
<rs xml:id="12894732" type="url" corresp="12894730">https://github.com/davidechicco/R-squared_ versus_other_regression_rates</rs>.
</p>
<p>Davide Chicco is an Academic Editor for PeerJ Computer Science.</p>
<p>Davide Chicco conceived and designed the experiments, performed the experiments, analyzed the data, performed the computation work, prepared figures and/or tables, authored or reviewed drafts of the paper, and approved the final draft. Matthijs J. Warrens analyzed the data, authored or reviewed drafts of the paper, contributed to the analysis of the mathematical properties, and approved the final draft. Giuseppe Jurman conceived and designed the experiments, performed the experiments, analyzed the data, performed the computation work, prepared figures and/or tables, authored or reviewed drafts of the paper, and approved the final draft.</p>
<p>Supplemental information for this article can be found online at http://dx.doi.org/10.7717/ peerj-cs.623#supplemental-information.</p>
</text>
</tei>