<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
<teiHeader>
<fileDesc xml:id="_1"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T12:27+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text xml:lang="en">
<p>The safe and stable operation of wind power systems requires the support of wind speed prediction. To ensure the controllability and stability of smart grid dispatching, a novel hybrid model consisting of data-adaptive decomposition, reinforcement learning ensemble, and improved error correction is established for short-term wind speed forecasting. In decomposition module, empirical wavelet transform algorithm is used to adaptively disassemble and reconstruct the wind speed series. In ensemble module, Q-learning is utilized to integrate gated recurrent unit, bidirectional long short-term memory, and deep belief network. In error correction module, wavelet packet decomposition and outlier-robust extreme learning machine are combined to developing predictable components. An appropriate correction shrinkage rate is used to obtain the best correction effect. Ljung-Box Q-Test is utilized to judge the termination of the error correction iteration. Four real data are utilized to validate model performance in the case study. Experimental results show that: (a) The proposed hybrid model can accurately capture the changes of wind data. Taking 1-step prediction results as an example, the mean absolute errors for site #1, #2, #3, and #4 are 0.0829m/s, 0.0661m/s, 0.0906m/s, and 0.0803m/s, respectively; (b) Compared with several state-of-the-art models, the proposed model has the best prediction performance.</p>
<p>The deterioration of the global environment and the exhaustion of traditional energy sources have strengthened people's desire to explore renewable energy [1]. As a green and low-carbon energy, wind energy has opened a new door for utilizing new energy [2]. Focusing on the sustainable supply characteristics of wind energy, the development, and utilization of wind energy has become an important research direction for universities and enterprises around the world [3]. The latest report of the Global Wind Energy Council (GWEC) in 2019 shows that the global wind power industry market will add 355GW of capacity between 2020 and 2024. It is estimated that by 2024, the global installed capacity of wind power will be about 71GW per year, which shows that the wind power industry has huge industry potential and room for development [4].</p>
<p>Nevertheless, the chaos, instability, randomness, and intermittency of wind energy bring great challenges to wind energy utilization [5]. The complexity of the temporal and spatial distribution of wind speeds in different regions exacerbates this difficulty. The efficient use of wind energy relies heavily on the accurate prediction of wind speed [6]. Inaccurate wind speed prediction will reduce the safety factor and power generation efficiency of the wind power system, resulting in huge economic losses and energy waste. To break through this dilemma, many researchers have invested a lot of time in the research of wind speed prediction models [7].</p>
<p>Wind speed prediction models can be broadly classified into four categories, namely physical model, statistical model, intelligent model, and hybrid model. The most common physical model is Numerical Weather Prediction (NWP) [8], which analyzes meteorological and geographic information such as temperature, density, speed, and spatial distribution to realize wind speed prediction. However, the NWP prediction method has very high requirements on data sources and processors. The slow updating speed makes it show shortcomings in short-term wind speed forecasting. In contrast, the simpler structure of the statistical model gives it a great advantage in time consumption. Common statistical models include the Auto-Regression Moving Averaging (ARMA) [9], Auto-Regressive Integrated Moving Average (ARIMA) [10], persistence model [11], classical Box-Jenkins methodology model [12], and so on. Statistical models can directly use historical wind speed data for rapid short-term prediction [13]. Unfortunately, statistical models such as ARMA are difficult to capture the strong nonlinear characteristics of real wind speed time series. This shortcoming makes its prediction accuracy difficult to meet strict engineering requirements. The development of Artificial Intelligence (AI) technology has promoted the birth of machine learning models that can handle nonlinear characteristic time series well [14]. The widely used machine learning models include Artificial Neural Network (ANN) [15], Back Propagation Neural Network (BPNN) [16], Extreme Learning Machine (ELM) [17], Long Short-Term Memory (LSTM) [18], Deep Boltzmann Machine (DBM) [19], Convolutional Neural Network (CNN) [20], etc. These models have excellent computing power. Further, to pursue models with more universality and higher prediction accuracy, the era of hybrid modeling has come [21]. A large number of surveys show that many professionals have devoted their efforts to the research of data decomposition methods, ensemble methods, and error correction methods. The literature review of these three aspects can be summarized as follows:</p>
<p>According to research experience, it can be known that the original wind speed time series has many noises and unstable characteristics [22]. This makes it hard for predictive models to capture the changing law of wind. Directly using the original wind speed series to carry out data-driven modeling may lead to larger forecasting errors. The data preprocessing method based on decomposition solves this problem to a certain extent. Sun et al. use Fast Ensemble Empirical Mode Decomposition (FEEMD) to decompose the wind speed series into stationary subseries for data-driven modeling [23]. Experimental results show that the decomposition method effectively improves prediction accuracy. Moreno et al. utilize the Variational Mode Decomposition -Singular Spectrum Analysis (VMD-SSA) method to decompose and preprocess the wind speed time series in northeastern Brazil [24]. The combination of rough decomposition and fine decomposition makes non-stationary time series easier to be learned by ARIMA. Yan et al. improve Singular Spectrum Decomposition (SSD) and combine it with Grasshopper Optimization Algorithm (GOA), LSTM, and Deep Belief Network (DBN) to build a multi-step wind speed prediction model [25]. Improved Singular Spectrum Decomposition (ISSD) has stronger data preprocessing capabilities than algorithms such as VMD and Complete Ensemble Empirical Mode Decomposition (CEEMD). The decomposition method breaks the original wind speed series with high volatility into multiple stable subseries [26]. This idea has been proved to be one of the most effective data preprocessing methods [27]. However, for the decomposition algorithm, the choice of the decomposition layer has a great influence on decomposition efficiency. The previous method is to select the appropriate number of decomposition layers by combining the user's prior knowledge, but this will inevitably introduce human errors. Therefore, a data preprocessing method for adaptively determining the number of decomposition levels needs to be studied.</p>
<p>There are two main factors that affect the effectiveness of the ensemble model, one is the benchmark predictor, and the other is the integrated optimization algorithm. The focus of different prediction models is different, which also causes the prediction performance of a single benchmark model to vary greatly on different data sets. To improve the robustness and universality of the model, ensemble methods based on meta-heuristic optimization algorithms are explored [28]. Song et al. use Gray Wolf Optimization (GWO) algorithm to do a weighted integration of traditional prediction models such as BPNN, Elman Neural Network (ENN), Wavelet Neural Network (WNN), and Generalized Regression Neural Network (GRNN) [29]. The combined model shows better prediction accuracy in multiple datasets. Deep learning models have stronger learning capabilities than traditional machine learning models. On this basis, Liu et al. use Multiple Mutations Adaptive Genetic Algorithm (MMAdapGA) to integrate ELM, <rs xml:id="12899962" type="software">Outlier-Robust Extreme Learning Machine (ORELM)</rs>, and DBN [5]. The addition of DBN provides a breakthrough to improve the accuracy limit. More hidden layers enable the deep network to learn more wind speed information. It verifies the hypothesis that the benchmark predictor has a great influence on the prediction effect of the integrated model. In addition, the advantages of multi-objective optimization algorithms are reflected in the universality of prediction results. Niu et al. used Multi-Objective Grasshopper Optimization Algorithm (MOGOA) to ensemble five benchmark models such as BPNN, GRNN, ARIMA, ENN, and ELM [30]. Compared with the single objective optimization algorithm, the Pareto surface formed by multiple non-dominated solutions makes the model perform well on multiple error indicators. The ensemble strategy integrates the advantages of multiple benchmark predictors and complements each other to maximize the performance [31]. Compared with common meta-heuristic optimization algorithms, reinforcement learning with strong learning ability has demonstrated strong strength in many fields [32]. If reinforcement learning can be used to integrate AI models, an excellent result may occur. Therefore, an optimized ensemble method that combines reinforcement learning and better benchmark predictor is worth investigating.</p>
<p>The randomness and intermittency of wind speed often make the machine learning model unable to fully fit its changes. At this time, data post-processing methods come in handy. The error correction is performed by calculating the error between the forecasting data and the real data to improve the numerical output accuracy of the model [33]. Ding et al. developed an NWP model for wind speed forecasting based on Bidirectional Gated Recurrent Unit Neural Networks (BiGRUNNs) error correction [34]. As a variant of Recurrent Neural Network (RNN), BiGRUNN's model structure effectively solves the problem of gradient disappearance. The introduction of the error correction system raises the model prediction accuracy to a higher level. Duan et al. proposed a decomposition-prediction correction method based on Improved Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (ICEEMDAN) -ARIMA [35]. The wind speed error series is decomposed by ICEEMDAN into multiple Intrinsic Mode Function (IMF) error subseries, and these error series are predicted by ARIMA. Finally, these series are added to the original prediction results to obtain a final result with higher accuracy. Liu et al. constructed an error correction model combining Empirical Wavelet Transform (EWT) and <rs xml:id="12899963" type="software">ORELM</rs> <rs xml:id="12953746" type="bibr">[36]</rs>. This correction method can adaptively determine the number of decomposition levels. The addition of the decomposition method reduces the instability of the error series and makes it easier to train the correction model. The stronger robustness of <rs xml:id="12899965" type="software">ORELM</rs> can make the model converge quickly. The prediction result after correcting the error is closer to the fluctuation characteristics of wind speed [37]. Unfortunately, one-time error correction may lead to the risk of over-fitting. Too many corrections may introduce new errors, and too few corrections may not fully utilize the predictable components. This process can be controlled by obtaining an appropriate correction shrinkage rate. The introduction of correction shrinkage rate can slow down the entire fitting process and prevent over-fitting. At the same time, training multiple predictors to perform multiple iteration corrections can prevent the validation data from being input to the predictor at one time, and avoid the situation where the data used to detect predictability is consistent with the data used to train the model. In other words, if the data used to train the model is consistent with the data used to test the series predictability, the potentially predictable components in the test data may be detected as unpredictable. Then the prediction accuracy of the error correction model in the validation set will be abnormally higher than its accuracy in the test set. Using different parts of the data to train the correction predictor can balance the performance of the model on the training data and the testing data, and the predictability of the testing data can be developed to the greatest extent. Therefore, an improved adaptive multiple error correction method based on decomposition and prediction needs to be explored.</p>
<p>Reviewing the above decomposition methods, ensemble methods, and error correction methods, scholars have made outstanding contributions in the field of wind speed time series forecasting. Table 1 summarizes the structure of the models, contributions, and the year of publication. 1 displays the main design method steps of the proposed model.</p>
<p>The proposed reinforcement learning ensemble model.</p>
<p>According to the spectrum signal, EWT can realize the adaptive division of frequency bands. The EWT is composed of two parts of functions, including empirical scale function and empirical wavelet function. Generally, the empirical scale function is defined as a filter that filters low-pass signals. Specifically, its mathematical expression can be provided as follows [38]:</p>
<p>The empirical wavelet function is regarded as a band-pass filter applied to each frequency band, and its mathematical expression is provided as follows:</p>
<p>The internal units of GRU and LSTM are very similar, but the difference is that GRU merges the input gate and the forget gate into a single update gate. Finally, there are only two gate structures in the GRU, namely the update gate and the reset gate. The update gate is used to determine whether to retain the previous state information and the degree of retention. The reset gate is used to determine whether to combine the information of the current state and the previous state. The mathematical expression of the GRU neural network is provided as follows [39]: represents the multiplication of corresponding elements between the matrices.</p>
<p>BiLSTM enables recursive feedback of past and future hidden layer states through a two-way network. This can make the inner connection between current data and past-future time data be further explored. The hidden layer state t  of BiLSTM is a combination of three parts, including the previous hidden layer output state</p>
<p>propagates forward along the time axis, and the previous hidden layer output state 1 i  - that propagates backward along the time axis, and the input amount t x at the current moment. Specifically, the combined expression of each level of hidden layer state is provided as follows [40]:</p>
<p>Where LSTM represents the operation process of the traditional LSTM network [41], t  is the forward hidden layer state, i  is the backward hidden layer state,</p>
<p> is the output weight of the hidden layer in the forward propagation unit, b t  is the output weight of the hidden layer in the backward propagation unit, c t  is the hidden layer bias optimization parameter at the current moment.</p>
<p>Multiple Restricted Boltzmann Machines (RBMs) and a BP layer constitute the DBN.</p>
<p>The training process of DBN consists of two parts. One is to train RBMs layer by layer in a greedy and unsupervised manner, and the other is to fine-tune the model structure parameters by training BPNN in a supervised manner. Specifically, the energy function expression of RBMs is given as follows [5]:</p>
<p>Where , m represents the subseries decomposed in m-th layer, N represents the number of subseries layer.</p>
<p>After the decomposition is completed, the benchmark models are used for predictive modeling. The training data is input to the model and output s-step prediction results , , ,</p>
<p>on the validation set, the S is the prediction step of the benchmark predictor. Taking GRU as an example, the predicted results of m layers of subseries are superimposed to obtain the predicted results ˆs GRU X . Finally, the trained benchmark predictors GRU [43], BiLSTM [44], and DBN [5] are obtained. Figure 2 shows the mechanism of decomposition-prediction modeling.</p>
<p>The mechanism of decomposition-prediction modeling.</p>
<p>Reinforcement learning is a method that belongs to the field of machine learning. It emphasizes that actions change with the environment, and the purpose is to maximize the expected benefits. The algorithm is inspired by behaviorist theories in psychology [45]. Stimulated by the rewards and punishments of the external environment, the agent will gradually form the expected results of these stimuli in the iterative process, and eventually produce habitual behaviors in the direction that can obtain the most benefits. Compared with supervised learning and unsupervised learning, reinforcement learning directly abandons the need for Markov Decision Processes (MDP) [46]. It can find a balance between the unknown world and real-world knowledge from the perspective of online planning to solve the optimal decision-making problem.</p>
<p>Q-learning algorithm belongs to time series differential learning, which can achieve model-free learning more efficiently than the Monte Carlo reinforcement learning algorithm [47]. This feature makes Q-learning more suitable for solving the optimization problem of wind speed time series [32]. Constructing a reasonable external environment, state space, action space, and reward function can ingeniously transform the above optimization problem into a reinforcement learning problem for solving. In this study, the Q-learning algorithm is utilized for models ensemble of GRU, BiLSTM, and DBN. The disadvantage of a single deep learning model is that its prediction performance is easily affected by data and the external environment, which leads to unstable prediction results. The ensemble of multiple deep learning models can effectively mitigate this negative impact. In the proposed model, the best weight combination is obtained by setting a reasonable reward and punishment strategy.</p>
<p>Step a: Initialize various parameters and states, including reward discounts  , 01   , learning rates  , 0 1   , greedy parameters  , Q tables, states 0 SS  , and strategies  .</p>
<p>Among them, the Q agent is a value-based reinforcement learning agent. It maintains a Q table as critic to estimate the value function. State S and action A are used as the input of the critic, and a corresponding long-term return expectation is output after training. In addition, the state</p>
<p>,, S w w w  explains the composition of each specific state in the learning process, and 1 2 3 ,, w w w are the weight coefficient.</p>
<p>Step b: Perform actions</p>
<p>according to the  -greedy strategy   . The mathematical calculation method of the strategy   is provided as follows:</p>
<p>    largest value action , probability 1randomly action , probability</p>
<p>Where the  is exploration probability, (</p>
<p>  .</p>
<p>Step c: Obtain instant rewards t r based on the calculation method of rewards R .</p>
<p>According to the weight coefficients</p>
<p>More specifically, the prediction error is represented by the Mean Square Error (MSE), and its expression is given as follows:</p>
<p>Where Ŷ is the wind speed prediction results, Y is the real wind speed data, Y N is the number of the real data.</p>
<p>Step d: Utilize the newly calculated evaluation function Q to update the Q table, and set the current state to</p>
<p>. The mathematical formula of the update method is provided as follows:</p>
<p>Where the learning rate is presented as  , 0 1   , the discount factor is presented as  , 01   .</p>
<p>Step e: Repeat the above Steps b~d until the iteration termination condition is met. Finally, the EWT-Q-GRU-BiLSTM-DBN reinforcement learning ensemble model is obtained.</p>
<p>Figure 3 shows the mechanism of the deep reinforcement learning ensemble strategy.</p>
<p>The mechanism of deep reinforcement learning ensemble strategy.</p>
<p>WPD is developed on the basis of wavelet decomposition and has the ability to process high and low frequency signals at the same time [48]. It is worth mentioning that WPD can adaptively match the corresponding frequency band suitable for the signal spectrum according to the characteristics and analysis requirements of the signal. Specifically, the mathematical expression of WPD is provided as follows:</p>
<p>is the scale parameter, l and k is the translation parameter, n is the frequency parameter, 2 kl - and 2 kl g - are the low-pass filter bank and the high-pass filter bank in the wavelet packet decomposition, respectively.</p>
<p>To better deal with outliers and non-Gaussian noise points in the dataset,
<rs xml:id="12899966" type="software">ORELM</rs> is proposed to enhance the robustness of outliers [49]. The mathematical expression for minimizing the loss function and solving 1 l -norm is provided as follows:
</p>
<p>Where  e is the training error, C is the regularization parameter. T is the output of training data and H is the output results, the output weight is represented as  .</p>
<p>L is the augmented Lagrangian function.  is the Lagrange multiplier,  is the penalty coefficient, and</p>
<p>As a subcategory of the Portmanteau statistical test, Ljung-Box Q-Test is often used to detect the autocorrelation of time series data [50]. Generally, if a series passes the LBQ-test, the autocorrelation of the residual sample will be reduced below the critical value determined by the significance level, and the autocorrelation will be eliminated basically. Specifically, the calculation method of LBQ-test is provided as follows [51]:</p>
<p>Where k a is the autocorrelation estimate of the residual at lag k; n is the number of samples;</p>
<p>is the mathematical modeling residual; m is the appropriate number of lags for the autocorrelation of the series samples.</p>
<p>In this study, an improved error correction technique is adopted to correct residual errors. If the residual errors in the prediction results of the first two stages can be exploited, the prediction system can better deal with the intermittent and randomness of the wind data [52]. The Multi-Predictor Deep Q Decomposition Ensemble Model (MPDQDEM) emphasizes the linear and non-linear components of the wind speed series, due to the built-in RL, GRU, BiLSTM, and DBN. The
<rs xml:id="12899967" type="software">ORELM</rs> is selected as the correction model to grasp the remaining non-linear components, outliers, and non-Gaussian noise points. For the remaining predictable components in the residual error, its low-frequency interference signal and some unsubdivided high-frequency signals are difficult to be fully developed by
<rs xml:id="12899968" type="software">ORELM</rs>. WPD is a time-frequency localization analysis method that can perform a multi-level division of frequency bands, and its time window and frequency window can be changed. WPD has the characteristic of adaptively selecting the frequency band to match with the signal spectrum, which makes the combination of WPD and
<rs xml:id="12899969" type="software">ORELM</rs> can achieve better residual prediction results.
</p>
<p>Step a: Pre-processing of residual series data.</p>
<p>Taking into account the sample balance of the divided dataset and the number of cross-validation, the validation data is divided into 6 parts to train different <rs xml:id="12899970" type="software">ORELM</rs> models, and the length of each part is 100. Different from directly training <rs xml:id="12899971" type="software">ORELM</rs> with all the validation data, using short data to train multiple <rs xml:id="12899972" type="software">ORELM</rs> models can avoid the data used for LBQ-test [50], and the data used for training the predictor are the same group. The forecasting accuracy of the <rs xml:id="12899973" type="software">ORELM</rs> in detected data will not be abnormally higher than the accuracy in testing data. The <rs xml:id="12899974" type="software">ORELM</rs> model keeps the same accuracy level both on the validation data and test data effectively ensuring that the method is effective. Therefore, the predictable state of the detected data can represent the state of the testing data. In addition, the WPD data processing method is incorporated to cooperate with the predictor to achieve a better correction effect. Considering the time-consuming problem of decomposition methods in multiple sub-<rs xml:id="12899975" type="software">ORELM</rs> modules, the number of decomposition cannot be set too large. Too few decomposition layers may lead to poor decomposition results. To achieve a balance between the two, we set the number of the decomposition to 3. In this part, subseries #1 of WPD is used as an example to show the method theory more conveniently.</p>
<p>Step b: Pre-defined parameter variables. Step c: Test the predictability of residual series data.</p>
<p>Theoretically, the automatic correction function determines whether the residual series data can be predicted by <rs xml:id="12899976" type="software">ORELM</rs>. In Step c, the LBQ-test method evaluates the predictability of the data according to the automatic correction function. If the prediction residual errors c v e are predictable after testing, then the series is used as the input of the <rs xml:id="12899977" type="software">ORELM</rs> model for training.</p>
<p>Step d: Correct the residual error. ,,,,, eeeeee   are obtained from the EWT-Q-GRU-BiLSTM-DBN model in the previous stage. During iteration #1, the sliding window starts to slide from the first part of the validation data. In the first sliding window of iteration #1, the 0 1 e is detected by the LBQ-test. If the detection result shows an unpredictable state, the error correction process is directly ended. If the detection result shows a predictable state, the data 0 1 e is input into the <rs xml:id="12899978" type="software">ORELM</rs> for training to get ORELM #1-1 in the first sliding window of iteration #1. As the sliding window moves, six sub-ORELMs #1-1~6 are obtained to form ORELM #1. And then, the prediction results</p>
<p>, , , , , , T p p p p p p p     can be obtained through the modeling prediction of ORELM #1. The 1 T p is the prediction results of ORELM #1 in testing data. It is worth mentioning that slowing down the model fitting process can prevent overfitting to a certain extent. Here, we introduce an error correction shrinkage rate r to control this process, which is inspired by the learning rate in Gradient Boosting (GBoost) [53] machine. At the end of Iteration #1, a prediction result   When the residual error in the prediction result is detected as an unpredictable state or when the number of iterations reaches the maximum, the process of multiple error corrections ends.</p>
<p>Superimposing the residual prediction results  </p>
<p>We traverse different correction shrinkage rates by cross validation to find the most suitable value. If the correction shrinkage rate is too small, the correction component will be too small and the correction will not be in place. If the correction shrinkage rate is too large, it will cause <rs xml:id="12899979" type="software">ORELM</rs> to correct errors excessively. The excessive correction has instead introduced new error components.</p>
<p>Figure 4 shows the mechanism of the improved decomposition-prediction multiple error correction method.</p>
<p>The mechanism of improved decomposition-prediction multiple error correction method.</p>
<p>3 Results and discussion</p>
<p>To evaluate the prediction performance of the proposed model, four groups of original real wind speed series collected from Xinjiang, China are provided for the case studies. The data involved in the experiment is collected by high-sensitivity stationary wind measurement base station equipment. To realize the real-time collection of wind speed data, the front-end measuring sensor of this equipment adopts dual-channel redundancy mode. The sampling interval of wind data is 3 seconds to 10 minutes. The measurement range of wind data is 0m/s~70m/s, and the maximum resolution is 0.1m/s. Most of these wind speed measurement base stations are located in basins and mountains, and the covered altitude ranges from 0m to 5000m. The ambient temperature of the equipment for wind measurement is -40℃~+55℃. The datasets used in this paper were collected in the summer of 2018. High-resolution data from nearly 30 wind measurement stations have been collected through actual measurements, with more than 1,500 wind data collected in each group. The time interval of the dataset utilized in this study is 1 min (the frequency of wind data recording).</p>
<p>It is worth mentioning that in order to unify the length of the data, the length of each data series is 1500. There is no 'thumb of rule' for dataset partitioning. In general, it is better to divide the training set and test set into a ratio of 0.7:0.3 or 0.8:0.2 for predictive modeling [54]. In the wind speed series, 1 -st ~600 -th is training data, 601 -st ~1200 -th is validation data, and 1201 -st ~1500 -th is testing data.</p>
<p>Figure 5 shows the exploratory data analysis of wind speed data in four wind farms.</p>
<p>The exploratory data analysis includes longitude, latitude, minimum, maximum, mean, standard deviation, median, and quantile.</p>
<p>Figure 5 The specific statistical information of the studied original datasets.</p>
<p>According to Figure 5, observations can be summarized as follows. The four wind speed data sets used in the experiment all have large wind speed amplitudes. The average wind speeds reached 19.57m/s, 14.02m/s, 18.01m/s, 7.01m/s, and the maximum wind speed is 26.67m/s, approximately equal to 96km/h, reaching the level of wind scale 10 (whole gale). In addition, these four sets of wind speed data show greater volatility and instability. The fluctuation range of the difference between the maximum and minimum wind speed data is 8.09m/s~15.49m/s. Combining the longitude, latitude, and geographic location of the wind measurement station, it can be found that these data sets belong to the high wind environment data in high-altitude inland areas. Our research is conducted on the strong non-stationary wind in this environment, which contains abundant wind energy resources.</p>
<p>Four</p>
<p>Where again Ŷ is the prediction results, the Y is the real data, Y N is the number of the real data, the Ŷ and Y is the average of the prediction results and real data, respectively.</p>
<p>Case I compares the performance of 8 benchmark models to screen out 3 deep learning models with the best performance in different datasets. Case II compares the impact of different ensemble strategies, including Q-learning, State Action Reward State Action (SARSA) [55], Non-dominated Sorting Genetic Algorithm II (NSGA-II), GWO, Particle Swarm Optimization and Gravitational Search Algorithm (PSOGSA), Whale Optimization Algorithm (WOA). Case III compares the impact of diverse decomposition algorithms, including EWT, Maximal Overlap Discrete Wavelet Packet Transform (MODWPT), ICEEMDAN, and Ensemble Empirical Mode Decomposition (EEMD). Case IV compares the performance of each stage of the proposed model, it is worth mentioning that the improved error correction technique is utilized in this subsection. Case V compares the performance of the proposed model and several state-of-the-art models. The abbreviations of the proposed model are provided as shown in Table 2.</p>
<p>Both traditional machine learning models and improved deep learning models can show good prediction accuracy in time series forecasting. To find the most suitable model from the alternative predictors, we modeled and compared Broyden Fletcher Goldfarb Shanno (BFGS), BPNN, Support Vector Machine (SVM), ENN, Nonlinear Auto Regressive (NAR), GRU, BiLSTM, and DBN to evaluate their performance.</p>
<p>Figure 6 shows the 1-step forecasting errors, the fitted curve, the frequency distribution graph, and the scatter graph of these benchmark models in site #1. Table 3 presents the deterministic prediction errors of these benchmark models, where the model with the smallest error in each site is marked in bold. According to Figure 6 and Table 3, observations can be summarized as follows:</p>
<p>To find the best benchmark model, we hope that the same alternative models perform best in multiple data sets, so that we can choose them without hesitation. However, due to the different characteristics of different models, the data characteristics that these predictors are good at processing are also different. So when there are many experimental comparison models, it may appear that the best performance in each group of experimental results is not always those few models [56]. Effective integration can often make the performance of the hybrid model higher than all benchmark models. The higher the upper limit of the benchmark model, the better the effect of the ensemble model [5]. Based on this theoretical basis, we adjust the criteria for selecting benchmark models to find the best performing model in each set of data sets, and then take the intersection of the models selected by multiple experiments to obtain candidate benchmark predictors.</p>
<p>In the above four experiments, the best performance in each set of experiments is one of the three deep learning models: GRU, BiLSTM, and DBN. It can be found that if the deep learning model with the best performance in each set of experiments is used as the main predictor in the integrated model, and the remaining two deep learning models are used as auxiliary predictors, satisfactory phenomena may appear. The ensemble optimization algorithm obtains the ensemble ratio of the three benchmark predictors in the hybrid model through multiple iterations. Since each model is good at processing different data characteristics, it is possible that the auxiliary predictor will make up for some of the defects of the main predictor. This makes the integrated hybrid model have the characteristics of each sub-predictor and can adapt to more complex situations. It is worth mentioning that if all the benchmark predictors are integrated, the structure of the hybrid model will be very complicated. It seriously affects the computational efficiency of the model, and at the same time may produce the risk of overfitting.</p>
<p>a) The best performer on each dataset is one of the three deep learning models: GRU, BiLSTM, and DBN. Taking 1-step prediction results of site #1 as an example, the MAEs of BFGS, BPNN, SVM, ENN, NAR, GRU, BiLSTM, and DBN are 0.7220m/s, 0.7183m/s, 0.7208m/s, 0.7161m/s, 0.7477m/s, 0.7134m/s, 0.7306m/s, and 0.7172m/s, respectively. It can be seen that the model with the minimum error in site #1 is GRU. In the same comparison way, BiLSTM predictor performs better in site #2 and site #4, and DBN predictor performs better in site #3. Since the deep learning model has more hidden layers, it can more fully analyze the nonlinear characteristics of the wind speed time series. The better learning ability of deep networks makes them stand out, so the three deep learning models are selected as benchmark predictors for the next module.</p>
<p>Generally speaking, the stronger learning ability and adaptability of deep learning models enable them to obtain satisfactory prediction results. As for why GRU, BiLSTM, and DBN do not always perform best in these four sets of wind speed prediction experiments, some analyses are given as follows:</p>
<p>b) Compared with traditional prediction methods, deep learning models do not always perform better on all datasets. Taking 1-3 step prediction results of site #1 as an example, the MAEs of BPNN and BiLSTM are 0.7183m/s, 1.2300m/s, 1.3464m/s, 0.7306m/s, 1.2793m/s, and 1.4475m/s, respectively. It can be seen from the prediction error-index that the BPNN performs better than the BiLSTM at this time. From the fitting curve graph, error distribution graph, and scatter plot, the traditional machine learning model does not lag far behind the deep learning model. The possible reason is that the deep learning model puts higher requirements on the user's prior knowledge in the hidden layer structure and parameter setting. Unreasonable initialization may lead to the risk of overfitting. Therefore, a more stable approach needs to be adopted.</p>
<p>c) The DBN deep learning model did not show the expected excellent performance in site #4. Taking the 1-3 step prediction results of site #4 as an example, the MAEs of DBN are 0.8009m/s, 1.5899m/s, and 1.9418m/s. The reason for the poor performance of deep learning models may be that there are too little learnable data, resulting in inadequate model training. The deep learning model does not exert all its energy, which may make its prediction performance inferior to traditional models. Therefore, a more robust method needs to be utilized in subsequent sections.</p>
<p>In this subsection, different ensemble strategies are employed to compare the optimization performance. The optimization algorithms involved in this sub-section include Q-learning, SARSA, NSGA-II, GWO, PSOGSA, and WOA. The single objective, multi-objective optimization algorithms, and reinforcement learning algorithms all iteratively obtain the best weight combination results by setting optimization goals. Appropriate ensemble weights can make benchmark predictors complement each other and play a stronger role. Figure 7 shows the 1-step forecasting errors, the fitted curve, the frequency distribution graph, and the scatter graph of these ensemble models in site #1. Table 4 presents the deterministic prediction errors of these ensemble models, where the model with the smallest error in each site is marked in bold. Table 5 lists the performance improvement percentages of benchmark models by the Q learning ensemble strategy. According to Figure 7 and Tables 4~5, observations can be summarized as follows:</p>
<p>a) The ensemble strategy can make the benchmark model complement each other. Taking 1-step prediction results of site #1 as an example, the P MAEs (%) improvement of Q-GRU-BiLSTM-DBN vs. GRU, vs. BiLSTM, and vs. DBN are 4.51%, 6.75%, and 5.01%, respectively. In addition, when a single predictor is unexpected, the ensemble method can compensate for the shortcomings and improve the robustness. Taking 1-3 step prediction results of site #4 as an example, the P MAE (%) of Q-GRU-BiLSTM-DBN vs. DBN are 19.18%, 38.44%, and 44.05%. The ensemble strategy gives each benchmark predictor an optimal combining weight, so that these models can be integrated organically.</p>
<p>b) Among the weighted ensemble methods compared, Q-learning has the best ensemble effect. Taking 1-step prediction results of site #1 as an example, the MAEs of Q/SARSA/NSGA-II/GWO/PSOGSA/WOA-GRU-BiLSTM-DBN are 0.6812m/s, 0.6871m/s, 0.7026m/s, 0.7119m/s, 0.7123m/s, and 0.7121m/s, respectively. Reinforcement learning shows excellent talents in weight optimization due to the reward and punishment mechanism. Different from the meta-heuristic optimization algorithm, the Q-learning algorithm belongs to time difference learning, which has better optimization performance. Comparing with the Monte Carlo reinforcement learning algorithm, Q-learning can achieve more efficient model-free learning. Comparing the multi-objective optimization algorithm and the single objective optimization algorithm, it can be found that the Pareto solution set of the multi-objective optimization problem contains more effective information. However, the single objective weighted summation can only approximate the convex Pareto surface, so NSGA-II is superior in performance to single objective algorithms such as GWO, PSOGSA, and WOA. The single objective optimization algorithms compared are all based on similar bionic theories, so the small differences in their performance are difficult to explain theoretically.</p>
<p>The decomposition method can significantly improve the prediction accuracy of the model by reducing the instability of the wind speed series. To select the most suitable decomposition method, we compare EWT, MODWPT, ICEEMDAN, and EEMD based on Case II. Figure 8 shows the 1-step forecasting errors, the fitted curve, the frequency distribution graph, and the scatter graph of these decomposition-ensemble models in site #1. Table 6 presents the deterministic prediction errors of these decomposition-ensemble models, where the model with the smallest error in each site is marked in bold. Table 7 lists the performance improvement percentages of the ensemble models by the different decomposition methods. According to Figure 8 and Tables 6~7, observations can be summarized as follows:</p>
<p>a) The EWT, MODWPT, ICEEMDAN, and EEMD decomposition methods can effectively improve the prediction accuracy of the model. Taking 1-step prediction results of site #1 as an example, the P MAEs (%) improvement of the hybrid model EWT/MODWPT/ICEEMDAN/EEMD-Q-GRU-BiLSTM-DBN vs. Q-GRU-BiLSTM-DBN are 84.27%, 78.06%, 62.26%, and 60.65%, respectively. The original wind speed series has obvious chaos and volatility, which poses a great challenge to modeling. Fortunately, the decomposition method overcomes these disadvantages by decomposing the original series into several more stable subseries. In short, the decomposition method can make the wind speed series easier to predict by reducing the instability.</p>
<p>b) Among the above-mentioned comparative decomposition methods, the model pre-processed by EWT decomposition has the best performance. Taking 1-step prediction results of site #1 as an example, the MAEs of the hybrid model EWT/MODWPT/ICEEMDAN/EEMD-Q-GRU-BiLSTM-DBN are 0.1072m/s, 0.1494m/s, 0.2571m/s, and 0.2681m/s, respectively. The possible reason is that EWT can adaptively determine the number of decomposition layers, which to a certain extent avoids human error. In addition, because MODWPT overcomes the problem of phase distortion, it also has a good decomposition effect. Comparing with the EEMD, the ICEEMDAN reduces modal effects by adding adaptive noise to the time series. Better convergence makes the decomposition of ICEEMDAN more efficient. The frequent modal aliasing in EMD-like algorithms exposes the shortcomings of the above two EMD-based variant algorithms, so their decomposition effect is not as good as EWT and MODWPT.</p>
<p>The prediction results without post-processing often still have some potential predictable components. If these predictable components are developed, the prediction performance of the model can be further improved. In this subsection, an improved error correction technique is adopted to correct residual errors. Figure 9 shows the 1-step forecasting errors, the fitted curve, the frequency distribution graph, and the scatter graph of GRU, BiLSTM, DBN, MPDQEM, MPDQDEM, and ICMPDQDEM in site #1. Table 9 presents the deterministic prediction errors of these models, where the model with the smallest error in each site is marked in bold.</p>
<p>To embody the best error correction shrinkage rates in the module of error correction, the last block cross-validation is adopted [57]. After normalization, the range of error correction shrinkage rates is set to 0~1. Too small step length of candidate correction shrinkage rates will greatly increase the time-consuming process of error correction. An excessively large candidate correction shrinkage rates step length may result in missed the optimal error correction rate. To balance the two, the correction step length is set to 0.1. The main purpose of introducing error correction shrinkage rate is to slow down the correction process and reduce the risk of overfitting. Too small error correction shrinkage rate may lead to incomplete correction, and there are still predictable components remaining. A too large error correction shrinkage rate may lead to over-correction, which may introduce new errors and reduce the accuracy of the model. Table 8 presents the best correction shrinkage rate when the MAEs are the smallest at site #1~#4.</p>
<p>The optimal correction shrinkage rate under different sites. According to Figure 9 and Table 9, it can be summarized as follows:</p>
<p>a) The improved error correction technique can effectively correct the residual errors. Taking 1-3 step prediction results of site #1 as an example, the MAEs of ICMPDQDEM are 0.0829m/s, 0.1028m/s, and 0.1425m/s, respectively. The possible reason is that block cross-validation selects the best correction shrinkage rate. An appropriate correction rate can ensure that the correction model can mine predictable components as much as possible while avoiding the mixing of redundant errors. In addition, the WPD can finely decompose high-frequency and low-frequency data. The <rs xml:id="12899980" type="software">ORELM</rs> has strong robustness. These two advantages make its prediction accuracy satisfactory.</p>
<p>b) The proposed error correction method has a more significant improvement effect when the prediction steps are higher. Taking 1-3 step prediction results of site #1 as an example, the ICMPDQDEM reduces the P MAEs (%) of the MPDQDEM with 22.67%, 44.37%, and 50.07%, respectively. It can be seen that the model performance is improved to a greater extent in the 2-step and 3-step predictions. Generally, as the forecasting steps increase, the error will become larger. These residuals contain more predictable components. These predictable components are detected by the LBQ-test, and multiple iterations are performed to correct the errors. Then, the autocorrelation component of the correction results almost disappears. This shows that the predictable components in the prediction residuals are basically eliminated. c) Each module of the proposed model has a positive effect on the improvement of prediction accuracy. The ICMPDQDEM performed best in the comparison experiment. Taking 1-step prediction results of site #1 as an example, the MAEs of GRU, BiLSTM, DBN, MPDQEM, MPDQDEM, and ICMPDQDEM are 0.7134m/s, 0.7306m/s, 0.7172m/s, 0.6812m/s, 0.1072m/s, and 0.0829m/s, respectively. The deep reinforcement learning ensemble in the MPDQEM makes the benchmark predictors complementary to each other through the optimal weight combination. It avoids the performance limitations of a single predictor and the prediction bias caused by accidental errors. By decomposing the original wind speed series into several more stable subseries, the decomposition algorithm in the MPDQDEM makes the chaotic series orderly. The ICMPDQDEM corrects the residual errors based on the previous two-stage model, so that the performance of the hybrid model reaches the best. The complementarity of each module makes the proposed model satisfactory.</p>
<p>To objectively verify the superiority of the proposed model, we selected four state-of-the-art models published in 2018~2021 for experimental comparison. Appendix. Table B shows the structure of the four state-of-the-art models.</p>
<p>Liu et al. [5] constructed a deep learning framework including data prediction, multi-learner ensemble, and adaptive multiple error correction processing. Compared with Liu's method, we have made further improvements to each module. In terms of predictors, more deep learning models are chosen. As for the decomposition method, the EWT which adaptively determines the number of decomposition layers is selected.</p>
<p>For ensemble optimization, a novel reinforcement learning algorithm is used. In the correction strategy, a decomposition idea is added. Song et al. [29] employed the GWO to complete the optimization fusion of the benchmark models. However, the defect of the ICEEMDAN in determining the number of decomposition layers limits its performance improvement. We use an adaptive decomposition tool EWT to reduce the generation of human error, and the optimal number of decomposition layers helps the model achieve satisfactory results.</p>
<p>In the process of model reproduction, we continued the parameters used in the original author's article as much as possible. Appendix. Tables C~F list the parameter settings of the state-of-the-art models. Figure 10 shows the 1-step forecasting errors, the fitted curve, the frequency distribution graph, and the scatter graph of these comparison models in site #1. Figure 11 lists the error indicators of each prediction site. Table 10 presents the deterministic prediction errors of the proposed model and above state-of-the-art models, where the model with the smallest error in each site is marked in bold. Compared with the 300 sample points (that is, 300 minutes) output from modeling, this time consumption is feasible. The update of the model will not affect forecasting continuity. In addition, the time taken by the trained model to predict a wind speed series is in the range of [1.76s, 2.11s]. It can be seen that the longest forecasting time is 2.11s, which is much smaller than the time interval of the model (1minute). This shows that the update of the model will not cause conflicts between the prediction process and the prediction results, and then lead to the loss of prediction sample points. Due to the complex structure of the proposed model, the computation time is longer than the single methods, but in general, the proposed deep reinforcement learning model can meet the feasibility of time consumption.
<rs xml:id="12899981" type="software">Apache Spark</rs> <rs xml:id="12953747" type="bibr">[60]</rs>,
<rs xml:id="12899983" type="software">Apache Hadoop</rs> <rs xml:id="12953748" type="bibr">[61]</rs>, and other big data platforms, its computing speed can be greatly improved. Faster modeling can better adapt to engineering needs.
</p>
<p>The proposed model uses wind speed data with a time interval of 1 min for modeling.</p>
<p>If the research results are applied to engineering practice, the requirement for wind speed time history is that the data points need to be sampled continuously at equal intervals. Different from the 10-minute average wind speed standard in China, the 1-minute wind data used in the proposed model belongs to high-resolution data. For high-resolution data, although the acquisition process may be troublesome, the application is very flexible. Commonly, high-resolution data can be converted into 10-min, 30-min, or even 60-min low-resolution data through the classic averaging method [62]. In addition, some feature extraction methods can also convert high-resolution data into low-resolution data while preserving the key information of wind speed data to the greatest extent [44]. As more and more power electronic equipment and facilities are applied to traditional wind power systems, the construction of high-resolution wind forecasting systems has become particularly important. More specifically, the application potentials of high-resolution data are summarized as follows:</p>
<p> High-resolution wind speed prediction is helpful for the timely scheduling and dispatching of the wind power system. The minute-level short-term wind speed prediction provides a guarantee for the daily safe operation of large-scale wind farms, including avoiding tripping and power failure caused by load information errors in the microgrid [63], and reducing the impact of instantaneous voltage fluctuations on system equipment [64].</p>
<p> High-resolution wind speed prediction provides more detailed results, which can save equipment costs for enterprises Precise wind speed prediction provides more guidance for the safety margin setting of the wind power system network. A reasonable load safety margin reservation adjustment can avoid waste of electricity, thereby reducing enterprise operating expenses.</p>
<p>Wind conditions at different sites are rather complex and random. Section 3.1 discusses the characteristics of the studied wind field, and the proposed model is suitable for wind speed prediction in high-altitude windy environments of inland areas. Different from the wind in inland areas, the sea-land breeze in coastal areas is also a typical common wind. The application of inland breeze and sea-land breeze is very important, and their respective characteristics are summarized as follows:</p>
<p>Xinjiang areas (inland breeze): Xinjiang is located in the northern temperate zone, deep inland and far from the sea, with scarce precipitation and arid climate. Xinjiang's topography is dominated by plains, basins, mountains, and Gobi deserts. The minimal ground shelter is very conducive to the generation of high-speed winds. Affected by mountains, high-altitude convective wind disturbances, and wind vortices can also cause near-surface winds. Different from coastal areas, the pressure fluctuations caused by the significant daily temperature difference in Xinjiang will also promote the formation of wind. Combining these geographical conditions, the wind speed in Xinjiang can usually reach the level of wind scale 10 (whole gale).</p>
<p>Coastal areas (sea-land breeze): Sea-land breeze occurs in offshore and coastal areas. The sea-land breeze is usually caused by the temperature difference between the sea and the land during the day-night cycle. The temperature difference causes the density and pressure difference of the atmosphere near the surface in coastal areas. The pressure gradient forces the airflow to move from high-pressure areas to low-pressure areas. Among them, the airflow blowing from the ocean to the land during the day is called the sea breeze, and the airflow blowing from the land to the ocean at night is called the land breeze. Taking the sea-land breeze in the tropics as an example, the wind speed of the sea breeze is about 7m/s, and the wind speed of the land breeze is about 1m/s~2m/s.</p>
<p>The differences in generation mechanism, location, and wind speed amplitude make the wind conditions of the above two types of winds vary greatly. For different wind conditions, it usually requires in-depth and specific experimental research to obtain satisfactory modeling results. A model compatible with wind speed prediction tasks in inland and coastal areas is expected to be studied. It is worth mentioning that due to data-driven characteristics, machine learning models are sensitive to the size of training samples [44]. Under normal circumstances, the prediction performance of a machine learning model will increase as the number of training samples increases.</p>
<p>When the training sample is too small, it is difficult for the model to be trained, and the performance cannot be exerted fully.</p>
<p>In this research, a deep reinforcement learning hybrid ensemble model is proposed for wind speed forecasting. After repeated experimental verification and case studies, the main findings are listed as follows: (a) The adopted adaptive data decomposition method can avoid the human error caused by a lack of prior knowledge. In addition, it is worth discussing more modeling application scenarios in future works. Finding a compatible model that is suitable for both inland large-scale wind fields and coastal sea wind fields will provide more help for wind energy development.</p>
<p> A</p>
<p>The study is fully supported by the National Natural Science Foundation of China (Grant No. 61873283), the Changsha Science &amp; Technology Project (Grant No. KQ1707017), the innovation driven project of the Central South University (2019CX005), the Postgraduate Scientific Research Innovation Project of Hunan Province [Grant No. CX20200106], and the Fundamental Research Funds for the Central Universities of Central South University [Grant No. 2020zzts528].</p>
<p>The authors declare that there is no conflict of interest regarding the publication of this paper.</p>
<p>The number of grasshoppers 100</p>
<p>The number of dims 5</p>
<p>The distance between the two individuals [1,4] f and l 0.</p>
<p>We wish to confirm that there are no known conflicts of interest associated with this publication and there has been no significant financial support for this work that could have influenced its outcome.</p>
<p>We confirm that the manuscript has been read and approved by all named authors and that there are no other persons who satisfied the criteria for authorship but are not listed. We further confirm that the order of authors listed in the manuscript has been approved by all of us.</p>
<p>We confirm that we have given due consideration to the protection of intellectual property associated with this work and that there are no impediments to publication, including the timing of publication, with respect to intellectual property. In so doing we confirm that we have followed the regulations of our institutions concerning intellectual property.</p>
<p>We understand that the Corresponding Author is the sole contact for the Editorial process (including Editorial Manager and direct communications with the office). He/she is responsible for communicating with the other authors about progress, submissions of revisions and final approval of proofs. We confirm that we have provided a current, correct email address which is accessible by the Corresponding Author and which has been configured to accept email from csuliuhui@csu.edu.</p>
</text>
</tei>