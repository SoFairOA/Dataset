<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
<teiHeader>
<fileDesc xml:id="_1"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T10:13+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text xml:lang="en">
<p>Numerous statistical machine learning methods suitable for application to highly correlated features, as those that exist for spectral data, could potentially improve prediction performance over the commonly used partial least squares approach. Milk samples from 622 individual cows with known detailed protein composition and technological trait data accompanied by mid-infrared spectra were available to assess the predictive ability of different regression and classification algorithms. The regression-based approaches were partial least squares regression (PLSR), ridge regression (RR), least absolute shrinkage and selection operator (LASSO), elastic net, principal component regression, projection pursuit regression, spike and slab regression, random forests, boosting decision trees, neural networks (NN), and a post-hoc approach of model averaging (MA). Several classification methods (i.e., partial least squares discriminant analysis (PLSDA), random forests, boosting decision trees, and support vector machines (SVM)) were also used after stratifying the traits of interest into categories. In the regression analyses, MA was the best prediction method for 6 of the 14 traits investigated [curd firmness at 60 min, α S1 -casein (CN), α S2 -CN, κ-CN, α-lactalbumin, and β-lactoglobulin B], whereas NN and RR were the best algorithms for 3 traits each (rennet coagulation time, curd-firming time, and heat stability, and curd firmness at 30 min, β-CN, and β-lactoglobulin A, respectively), PLSR was best for pH, and LASSO was best for CN micelle size. When traits were divided into 2 classes, SVM had the greatest accuracy for the majority of the traits investigated. Although the well-established PLSR-based method performed competitively, the application of statistical machine learning methods for regression analyses reduced the root mean square error compared with PLSR from between 0.18% (κ-CN) to 3.67% (heat stability). The use of modern statistical machine learning methods for trait prediction from mid-infrared spectroscopy may improve the prediction accuracy for some traits.</p>
<p>Fourier-transform mid-infrared spectroscopy (MIRS) is a methodology that exploits mid-infrared region light to indirectly predict the concentration of constituents in a sample. When a sample is analyzed using MIRS, light is passed through the sample at a sequence of wavelengths in the mid-infrared region (5,000 to 900 cm -1 ), activating the chemical bonds of the sample matter with a consequential effect on the absorption of energy from the light (Skolik et al., 2018). The extent of the energy absorbed creates the spectrum for that sample, which should therefore be useful to predict the quantity of individual components within the sample. Infrared spectroscopy is used in different fields, from medicine (Petrich, 2001) to astronomy (Keller et al., 2006), as well as in animal science (De Marchi et al., 2014).</p>
<p>Mid-infrared spectroscopy is a low-cost, rapid and nondisruptive technique, routinely used in the analysis of cow milk samples for the determination of fat, protein, lactose, and CN concentrations in both bulk and individual animal samples (De Marchi et al., 2014). For this reason, MIRS is a potentially useful vehicle for collecting vast quantities of data at a population level. The literature documents the ability of MIRS to predict novel milk-related traits such as the coagulation properties of milk (Cecchinato et al., 2009;Visentin et al., 2016;El Jabri et al., 2019) and individual milk fatty acids (Soyeurt et al., 2006;Bonfatti et al., 2017), as well as animal-related traits, such as energy efficiency (McParland et al., 2014), energy intake (McParland and Berry, 2016), and methane emissions (Dehareng et al., 2012).</p>
<p>Partial least squares regression (PLSR) has traditionally been the method of choice in relating MIRS data of cow milk to novel milk and animal characteristics, owing to its capability to consider collinear, highdimensional data sets. Nonetheless, the investigation of the application of other statistical machine learning (ML) methods in predicting an outcome variable has been demonstrated in animal science research. Both Li et al. (2018) and Xu et al. (2019) used novel ML methods to respectively predict phenotypic performance using SNP data (Li et al., 2018) or cow metabolic status from animal and herd-level features. Nevertheless, the application of statistical ML methods in MIRS analyses is still rare. The potential usefulness of statistical ML methods to predict milk traits from spectra is due to their ability to perform well in multidimensional correlated data but also importantly to identify nonlinear associations between the wavelengths and the observed value of the trait. Recently, the division of continuous traits into categories before MIRS prediction analyses has also been considered (Manuelian et al., 2017;Grelet et al., 2019;Duplessis et al., 2020).</p>
<p>The novelty of the present study is the evaluation of modern statistical ML methods in predicting a series of cow milk quality traits, including milk technological traits (i.e., rennet coagulation time, curd-firming time, curd firmness at 30 and 60 min, CN micelle size, pH, and heat stability) and individual milk proteins (i.e., α S1 -CN, α S2 -CN, β-CN, κ-CN, α-LA, β-LG A, and β-LG B) from milk MIRS. These outcome traits were also divided into categories and the performance of modern classification methods assessed with the purpose of determining which performs best. The use of modern statistical ML methods for trait prediction from MIRS may improve the prediction accuracy for some traits.</p>
<p>The data set used in the present study is described in detail by both Visentin et al. (2015) and McDermott et al. (2016). In brief, 730 milk samples from 622 cows were collected between August 2013 and August 2014 from 7 different Irish research herds. The samples originated from Holstein-Friesian, Jersey, and Norwegian Red cows, as well as their crosses; all cows were fed a predominantly grass-based diet with occasional concentrate and grass silage supplementation. The samples were collected during morning and evening milking and represented different stages of lactation and different parities. All samples were analyzed by the same MilkoScan FT6000 (Foss Electronic A/S), and the resulting spectrum, comprising 1,060 transmittance data points in the mid-infrared light region, was stored. The traits investigated in the present study included the milk technological traits of rennet coagulation time (RCT), curd-firming time (k20), curd firmness at 30 and 60 min (a30, a60), casein micelle size (CMS), pH, and heat stability, as well as detailed milk protein traits, including α S1 -CN, α S2 -CN, β-CN, κ-CN, α-LA, β-LG A, and β-LG B.</p>
<p>The milk coagulation properties were quantified using a Formagraph (Foss Electronic A/S). Milk pH of all samples was assessed with a SevenCompact pH meter S220 (Mettler Toledo AG). The CN micelle hydrodynamic diameter was determined using a Zetasizer Nano system (Malvern Instruments Inc.). Heat stability was tested using the method outlined by Davies and White (1966). Milk proteins were determined using reversephase HPLC using an adaptation of the method of Visser et al. (1991) and are expressed as grams per liter of milk.</p>
<p>To satisfy the assumption that all observations used were independent (a requirement of some methods tested in this study), only 1 observation for each cow was retained for analysis. Where multiple records existed for an animal, the Mahalanobis distances between the average principal component (PC) scores from the entire data set and the multiple observations from the animal were computed. The observation with the greatest distance was retained, with the aim of maximizing the variability in the data set.</p>
<p>High-noise-level regions (Hewavitharana and van Brakel, 1997) were removed from each spectrum; the spectral regions between 1,710 and 1,600 cm -1 , between 3,690 and 2,990 cm -1 , and &gt;3,822 cm -1 were discarded. Consequently, a total of 531 wavelengths were used for the analyses. The transmittance values of the wavelengths were transformed to absorbance values by taking the log 10 of the reciprocal of the transmittance value. Outliers for the traits of interest were defined as those &gt;3 standard deviations (SD) from the mean of the respective trait and were subsequently removed from the analysis of that trait. The 16 noncoagulating milk samples were removed from the analyses of RCT, k20, a30, and a60. The numbers of samples as well as the mean, SD, median, minimum and maximum, coefficient of variation, and skewness of all traits investigated are provided in Table 1.</p>
<p>To assess the utility of classification-based statistical methods, the milk technological traits were divided into 2 classes based on their respective median value; the median was chosen as a threshold to split these traits into high and low, representing a proxy for the suitability of milk for cheese production. The content of each protein was divided into 4 classes based on quartiles, with the aim of reducing the range in values within each class. In a separate series of analyses, noncoagulating samples (n = 16) were rejoined to the data set to test the ability of classification models to discriminate between coagulating and noncoagulating samples.</p>
<p>To compare the performance of the different statistical ML approaches, the data were divided into 4 subdata sets with approximately the same numbers of observations in each, and 4-fold cross-validation was performed. The data division and cross-validation were performed separately for each trait, using the fold function in the
<rs xml:id="12965622" type="software" subtype="component" corresp="12965624">groupdata2</rs> package <rs xml:id="12965663" type="bibr">(Olsen, 2020)</rs> in
<rs xml:id="12965624" type="software" subtype="environment">R</rs> (
<rs xml:id="12965625" type="publisher" corresp="12965624">R Core Team</rs>, 2020) to balance data across folds. All the analyses were conducted using the statistical software
<rs xml:id="12965626" type="software">R</rs>
<rs xml:id="12965627" type="version" corresp="12965626">3.6.1</rs>.
</p>
<p>Eleven different regression-based statistical ML methods were explored; James et al. (2017) provide an excellent review of such methods. For some of the approaches, the tuning parameters were user defined or selected via cross-validation, and for others the default settings were used.</p>
<p>Partial least squares regression is a supervised dimension reduction method (Geladi and Kowalski, 1986). Partial least squares regression seeks out a small number of new variables (i.e., factors) that are linear combinations of the wavelengths, exploiting information on the response variable in doing so. Thus, PLSR uses both the trait data and the spectra to detect directions in the data space that best explain both. Partial least squares regression then fits a linear regression model via least squares to the trait data and the generated factors. Because a large portion of the information in the original data is captured by the generated factors, and because they are fewer in number, overfitting is mitigated. The number of PLSR factors to generate is data-dependent and user defined, typically by examining the change in root mean square error (RMSE) with each additional factor. In the present study, leaveone-out cross-validation was used to choose the number of factors to use in the model. A different number of factors were used in each of the 4 folds. The
<rs xml:id="12965628" type="software" subtype="environment">R</rs> package
<rs xml:id="12965629" type="software" subtype="component" corresp="12965628">pls</rs> <rs xml:id="12965664" type="bibr">(Mevik et al., 2019)</rs> was used here to implement PLSR.
</p>
<p>Principal component regression is similar in nature to PLSR but instead uses a linear regression model (estimated via least squares) of the trait on a small number of PC derived from the spectra alone. Similar to PLSR, a small number (compared with the number of wavelengths) of PC generally suffice to explain most of the variability in the data. The number of PC to retain is user defined, here by examining the change in RMSE with each additional PC. The
<rs xml:id="12965631" type="software" subtype="environment">R</rs> package
<rs xml:id="12965632" type="software" subtype="component" corresp="12965631">pls</rs> <rs xml:id="12965665" type="bibr">(Mevik et al., 2019)</rs> was again used to implement PC regression.
</p>
<p>Projection pursuit regression (PPR; Friedman and Stuetzle, 1981) is similar to both PLSR and PC regression in that it extracts linear combinations of the wavelengths as new derived features. Projection pursuit regression then models the trait as a nonlinear function of the newly derived features, where the prediction process uses flexible smoothing methods. The
<rs xml:id="12965634" type="software" subtype="environment">R</rs> package <rs xml:id="12965635" type="software" subtype="component" corresp="12965634">stats</rs> (
<rs xml:id="12965636" type="publisher" corresp="12965634">R Core Team</rs>,
2020) was used to apply PPR.
</p>
<p>Ridge regression (RR; Hoerl and Kennard, 1970) fits a linear regression model that includes all wavelengths but shrinks each regression coefficient estimated separately toward zero during model fitting. This approach, known as regularization, reduces the variance of predictions, at the expense of an increase in their bias.</p>
<p>Although RR is not a dimension reduction method and includes all wavelengths, it is computationally efficient as it fits only a single model. User specification of a tuning parameter is required, which here was selected by cross-validation. The package
<rs xml:id="12965637" type="software">glmnet</rs> <rs xml:id="12965666" type="bibr">(Friedman et al., 2010)</rs> was used for the RR analysis in this study.
</p>
<p>Although RR shrinks the regression coefficients toward zero, it does not shrink any to exactly zero (except when the tuning parameter is infinite) and so all variables are always included in the prediction model. This can result in good prediction accuracy but poor model interpretability. Least absolute shrinkage and selection operator (LASSO; Tibshirani, 1996) is similar in nature to RR but allows coefficient estimates to be exactly zero and, hence, is also a variable selection method that results in more interpretable models. The tuning parameter was selected based on the lowest mean cross-validated error. The
<rs xml:id="12965639" type="software" subtype="environment">R</rs> package
<rs xml:id="12965640" type="software" subtype="component" corresp="12965639">glmnet</rs> was again used for the LASSO analysis.
</p>
<p>Elastic net (EN; Zou and Hastie, 2005) offers a compromise between RR and the LASSO, in that it selects wavelengths similar to the LASSO but shrinks the coefficients of correlated wavelengths together like RR. Thus, EN can be considered a dimension reduction method, although it will select more wavelengths than the LASSO. The
<rs xml:id="12965641" type="software" subtype="environment">R</rs> package used for the EN analyses was
<rs xml:id="12965642" type="software" subtype="component" corresp="12965641">glmnet</rs>.
</p>
<p>Model averaging (MA) is a novel approach that consists of averaging the predictions from several of the previously considered approaches, which, in the present study, were PLSR, RR, LASSO, and EN. These models were selected due to their similarity in approach.</p>
<p>Spike and slab regression (SSR; Mitchell and Beauchamp, 1988) takes a Bayesian approach by assuming a bimodal prior distribution for the regression coefficients, with one mode at zero and one nonzero mode, followed by the use of a generalized EN to fit the model. The
<rs xml:id="12965643" type="software" subtype="environment">R</rs> package used for the analyses was
<rs xml:id="12965644" type="software" subtype="component" corresp="12965643">spikeslab</rs> <rs xml:id="12965667" type="bibr">(Ishwaran et al., 2010)</rs>.
</p>
<p>Random forests (RF) produce multiple decision trees (DT), the predictions from which are combined to give a consensus prediction. Decision tree-based methods (Breiman et al., 1984) are so called because they can be summarized visually by a tree-like structure. Decision trees work by segmenting the predictor space into several simple regions. Prediction for a test spectrum is simply the mean of the training observations in the region to which the test spectrum belongs. The predictor space is segmented recursively, beginning with a root node and subsequently creating branches determined by splitting rules based on the predictor values. The terminal nodes or leaves of the resulting tree define the simple regions used for prediction. However, DT suffer from high variance in its response, which RF overcomes by averaging predictions from many DT, but where at each split only a random sample of wavelengths are considered. The number of DT and the number of wavelengths randomly sampled as candidates at each split is user defined. Here 500 DT were used, and the number of wavelengths considered at each split was the number of wavelengths divided by 3. The
<rs xml:id="12965646" type="software" subtype="environment">R</rs> package used for the analyses was
<rs xml:id="12965647" type="software" subtype="component" corresp="12965646">randomForest</rs> <rs xml:id="12965668" type="bibr">(Liaw and Wiener, 2002)</rs>.
</p>
<p>Boosting is a general concept that can be used with many statistical ML methods to improve predictions. Unlike the RF setting, each DT is fitted to a modified version of the original data set. The trees are grown sequentially, where, at each stage, a tree is fitted to the residuals from the previous model fit, thus improving model fit in areas of the predictor space where performance in a single DT was poor. Boosting requires the specification of several settings: here the number of trees considered was 500, and the shrinkage parameter was set to 0.01. The approach was implemented using the
<rs xml:id="12965649" type="software" subtype="component" corresp="12965650">gbm</rs>
<rs xml:id="12965650" type="software" subtype="environment">R</rs> package <rs xml:id="12965669" type="bibr">(Greenwell et al., 2019)</rs>.
</p>
<p>Neural networks (NN) are nonlinear generalizations of a linear model. In a regression setting, NN first construct derived features that are linear combinations of the wavelengths. The outcome variable is then modeled as a function of linear combinations of the derived features. An NN is typically represented in a network diagram with several hidden layers, each representing different functions of the derived features. Here, a 2-layer NN was fitted, with Bayesian regularization employed to improve generalizability, using the
<rs xml:id="12965652" type="software" subtype="environment">R</rs> package
<rs xml:id="12965653" type="software" subtype="component" corresp="12965652">brnn</rs> <rs xml:id="12965670" type="bibr">(Perez Rodriguez and Gianola, 2020)</rs>.
</p>
<p>The outcome traits were divided into classes and the performance of 4 classification approaches assessed.</p>
<p>Partial least square discriminant analysis (PLSDA) is a dimension reduction model used for classification purposes. Partial least square discriminant analysis works similarly to PLSR, but for the former the response variable is dichotomized. The model proceeds similarly to PLSR, with prediction to the classes determined by whether or not the output is greater than a specified threshold. The
<rs xml:id="12965655" type="software" subtype="environment">R</rs> package used for the analysis was
<rs xml:id="12965656" type="software" subtype="component" corresp="12965655">caret</rs> <rs xml:id="12965671" type="bibr">(Kuhn, 2020)</rs>.
</p>
<p>Random forests applied for classification purposes follows the procedure previously described for RF for regression purposes. The implementation of RF for classification purposes used the same number of trees as in the regression setting, but the number of wavelengths considered at each split was set to the square root of the number of wavelengths.</p>
<p>Boosting DT were also used for classification purposes, with the number of trees considered remaining at 500, as in the regression setting.</p>
<p>Support vector machine (SVM) is a classification method that allows for nonlinear decision boundaries between classes by enlarging the feature space using kernels. In the enlarged space, the boundary is linear, but in the wavelength space, the boundary is nonlinear and more flexible. The
<rs xml:id="12965658" type="software" subtype="environment">R</rs> package used for the SVM analyses was
<rs xml:id="12965659" type="software" subtype="component" corresp="12965658">e1071</rs> <rs xml:id="12965672" type="bibr">(Meyer et al., 2019)</rs>, in which a linear kernel was employed.
</p>
<p>The performance of each regression method was evaluated by examining the RMSE from the calibration data (3 folds of the data), the root mean square error from the cross-validation data (the remaining fold; RMSEV), and the coefficient of determination (R 2 , from both the calibration and the cross-validation data). Furthermore, the slope coefficient of a simple linear regression of the observed on the predicted value of each trait, as well as the bias corresponding to the mean of the observed minus the mean of the predicted values of the trait were obtained from the cross-validation data. The ratio of performance to interquartile distance (RPIQ) was used to assess the model consistency (Bellon-Maurel et al., 2010). The RPIQ is calculated as the ratio between the interquartile range of the observed trait values and the RMSE. The RPIQ was used in the present study instead of ratio performance deviation because it is better suited to non-normally distributed traits. Given a lack of evidence to support the use of threshold values for interpretation (Bellon-Maurel et al., 2010), the RPIQ was used in this study to compare performance of alternative models rather than to quantify prediction accuracy of specific traits per se.</p>
<p>The performance of each classification method for the milk technological traits was evaluated by examining the area under the receiver operating characteristic curve, the sensitivity (i.e., proportion of the high class correctly classified), the specificity (i.e., proportion of the low class correctly classified), and the accuracy (i.e., the ratio of the number of correctly classified observations to the total number of observations). For the milk proteins, which were divided into 4 classes, the classification methods' performance was assessed by the accuracy (i.e., the ratio of the number of correctly classified observations to the total number of observations).</p>
<p>In the regression analyses, the RMSE, R 2 , RMSEV, bias, and RPIQ were calculated as the average of the 4 folds of calibration or cross-validation data. The standard deviation (SD) of RMSE, R 2 , RMSEV, bias, and RPIQ across folds were also calculated, thus reflecting the variability or robustness across folds. The slope and its standard error in the regression analyses were estimated once across the entire data set of predicted values (i.e., across all 4 folds). The prediction performance for classification was calculated as the average of the 4 folds of calibration or cross-validation data; the SD reflects the variability across folds. In the present study, when a continuous trait was investigated, the RMSEV was used to identify the "best" model. When a trait in question was a categorical trait, the accuracy was used to identify the "best" model. S1 to S18 and Supplemental Figures S1 to S3 are available on the Teagasc Open Access Repository, T-Stór (https: / / hdl .handle .net/ 11019/ 2390; Frizzarin et al., 2021).</p>
<p>Table 2 details the regression model with the lowest RMSEV for each trait, the RMSEV obtained, and the coefficient of determination in the cross-validation data set. The difference between the RMSEV of the "best" prediction model and the corresponding RMSEV obtained from PLSR on the same trait is also detailed. The MA approach most frequently performed "best" across all traits, having the lowest RMSEV for 6 of the 14 traits investigated (i.e., a60, α S1 -CN, α S2 -CN, κ-CN, α-LA, and β-LG B). The LASSO and NN methods performed similarly to MA for κ-CN prediction. The NN had the lowest RMSEV for all of RCT, k20, and heat stability prediction, whereas LASSO had the lowest RMSEV for CMS prediction. Ridge regression had the lowest RMSEV for a30, β-CN, and β-LG A, but PLSR had the lowest RMSEV for pH. The average difference in RMSEV between PLSR and the "best" model varied from 0.18% (κ-CN) to 3.67% (heat stability). The prediction performance for each of the milk technological traits is presented in Supplemental Tables S1 to S7 (https: / / hdl .handle .net/ 11019/ 2390). Supplemental Tables S8 to S14 (https: / / hdl .handle .net/ 11019/ 2390) summarize the prediction performance of the different models for all the milk proteins.</p>
<p>The number of factors selected by PLSR across folds was consistent for some traits (±2 factors), although for others the number of factors selected varied across folds (±8 factors). Notably, the number of wavelengths selected for use in the model varied according to trait and model; SSR selected on average between 0.25 (κ-CN) and 93.5 (RCT) fewer wavelengths than LASSO, whereas EN always selected more wavelengths than either LASSO or SSR. The numbers of wavelengths selected by LASSO, EN, and SSR for each trait are presented in Supplemental Table S15 (https: / / hdl .handle .net/ 11019/ 2390), and the subsets of wavelengths selected are presented graphically in Supplemental Figures S1 to S3 (https: / / hdl .handle .net/ 11019/ 2390). The different models tended to select similar subsets of wavelengths. Also, PLSR, RR, and RF attributed greatest coefficients to these regions. In particular, the regions between 1,100 and 1,000 cm -foot_1 , between 1,530 and 1,462 cm -1 , between 1,790 and 1,735 cm -1 , and between 3,730 and 3,710 cm -1 were important for several traits. The region between 1,100 and 1,000 cm -1 was recurrently present for all the investigated traits, with the exception of β-CN. The region between 1,530 and 1,430 cm -1 was present for all the protein traits, as as for RCT, a60, CMS, and pH. The region between 1,790 and 1,735 cm -1 was present for all the milk technological traits with the exception of a30 and was present for α-LA and β-LG A. The region between 3,730 and 3,710 cm -1 was present for a30, a60, pH, α S1 -CN, β-CN, κ-CN, and β-LG B. In this specific region, for a60, pH, α S2 -CN, and β-LG B, the wavelength 3,726 cm -1 was always selected; for α S1 -CN, β-CN, and κ-CN, the wavelength 3,714 cm -1 was always selected.</p>
<p>Table 3 summarizes the "best" prediction model and its prediction accuracy across all traits. Support vector machine was the method with the greatest accuracy for 6 of the 7 binary technological traits investigated (i.e., RCT, k20, a30, CMS, pH, and heat stability); PLSDA had the same accuracy as SVM for RCT, pH, and heat stability prediction. Partial least squares discriminant analysis was the model with the greatest accuracy also for a60 prediction. For the binary technological traits, the greatest average accuracy was for pH prediction (0.80, SD = 0.03, 0.02), and the lowest average accuracy was for CMS (0.62, SD = 0.03). Sensitivity of discrimination of coagulating samples ranged from 0.98 (SD = 0.02; boosting DT) to 1.00 (SD = 0.00; PLSDA), but specificity was poor and ranged from 0.44 (PLSDA, RF, SVM) to 0.50 (boosting DT).</p>
<p>When the protein traits were split into quartiles for prediction, PLSDA had the greatest accuracy for 3 of the 6 traits (i.e., α S2 -CN, β-LG A, and β-LG B). Support vector machine produced the greatest accuracy for 2 traits (i.e., β-CN, and α-LA), and RF had the greatest accuracy for the remaining 2 traits (i.e., α S1 -CN and κ-CN). When the protein traits were divided into quartiles, accuracy ranged from 0.40 (SD = 0.04; α S2 -CN) to 0.48 (SD = 0.02; α S1 -CN). The prediction performance for the milk technological traits when classified are shown in Supplemental Table S16 (https: / / hdl .handle .net/ 11019/ 2390). Supplemental Tables S17 andS18 (https: / / hdl .handle .net/ 11019/ 2390) summa- rize the prediction performances for classified CN and whey proteins, respectively.</p>
<p>Although traditional statistical methods have served the prediction of phenotypes from milk spectral data well for several cattle (McParland and Berry, 2016) and milk traits (De Marchi et al., 2014), the objective of the present study was to evaluate alternative statistical approaches with a focus on ML techniques.</p>
<p>Partial least squares regression is considered the benchmark method, given its consistently strong prediction performance in chemometric analyses (Wold et al., 2001). However, PLSR did not consistently perform the best for the traits considered in the present study. With the exception of pH, the average difference in RMSEV between PLSR and the best prediction method ranged from 0.18% (κ-CN) to 3.67% (heat stability). Nonetheless, although variable prediction accuracy was observed across cross-validation folds, the "best" overall method was generally consistently the best in each fold. For example, when heat stability was predicted, NN always outperformed PLSR, with the RMSEV ranging from 0.76% to 6.03% lower with the NN method. Other methods investigated here, such as PPR, performed poorly, possibly due to the difficulty in choosing the correct tuning parameters, which requires careful specification of many settings by the user. Thus, the examined methods demonstrated better or comparable performance to the traditionally used PLSR, in line with Wolpert and Macready's (1997) assertion that for any algorithm, any superior performance in one class of problems is offset by its performance in another class. The "best" model varied depending on the data distribution and the range and variability present in the trait under investigation. Therefore, the same trait in a different data set could potentially be "best" predicted using a different method, and practitioners should consider these methods when predicting milk traits from mid-infrared data. Other examples of methods compared in the literature for predictive performance also gave inconsistent results across studies (e.g., Ferrand-Calmels et al., 2014;Bonfatti et al., 2017;El Jabri et al., 2019). The variability in performance is likely related to differences in the traits predicted and data sets used. Notwithstanding this, the MA approach draws strength from averaging across several methods, resulting in more accurate predictions than those achieved by any of the individual methods alone.</p>
<p>Shrinkage methods are used in genomic prediction (Li and Sillanpää, 2012;Ogutu et al., 2012;Azevedo et al., 2015) for dimension reduction. Indeed, shrinkage methods should identify the variables most strongly related to the trait being predicted. Hence, in chemometric analyses, it is expected that shrinkage methods could also be used to identify the most informative wavelengths where wavelengths may be considered to be analogous to SNPs in genomic predictions. However, the literature presents contrasting results about the potential of shrinkage methods (e.g., Bonfatti et al., 2017;El Jabri et al., 2019). Other methodologies not based on shrinkage models have been developed for wavelength selection in spectroscopy (Gottardo et al., 2015;Vohland et al., 2014). In the present study, 3 variable selection approaches were investigated, namely, LASSO, EN, and SSR. These 3 methods shrink to zero the coefficients of the wavelengths not deemed to be related to the trait under investigation. All remaining methods considered all the wavelengths available in the data set for the prediction, giving different weights (coefficients) to each wavelength but never attributing zero as a coefficient. In the present study, LASSO was the "best" model for 2 of the 14 traits investigated. Combining information from (1) a cross-investigation of the wavelengths selected by wavelength selection models (LASSO, EN, and SSR), (2) the coefficients calculated by PLSR and RR, and (3) the variable importance in RF, can inform which wavelength regions are related to specific traits. In fact, LASSO, EN, and SSR partly selected the same wavelengths, and these wavelengths were also associated with the greatest coefficients in both PLSR and RR as well as the greatest importance in RF; thus, specific wavelengths were identified as important for trait prediction across models. Requiring only selected wavelengths in the prediction model of milk constituents could justify the development of an instrument focused solely on these specific wavelength regions to predict prespecified groups of traits, for example, milk coagulation traits or milk proteins. Such an instrument should have reduced construction (and thus purchase) costs, making it more amenable for more widespread in-line use.</p>
<p>The data set used in the current study was a subset of that previously used to quantify the potential of MIRS as a predictor of individual milk proteins (McDermott et al., 2016) and technological traits (Visentin et al., 2015) using PLSR. Different editing of the original data set was required in the current study to satisfy the assumptions of some of the ML methods employed here. Further, the handling of the data was different in the current study, where the data set was divided into 4 subdata sets or folds (25% in each fold) to perform 4-fold cross-validation. Visentin et al. (2015) randomly divided the data set once into calibration and validation data sets, with 80% of the data included in the calibration data set. Thus, the PLSR results reported in the current study are not identical to those in previous studies.</p>
<p>Some traits, including RCT, k20, a30, a60, and pH, can be used together to define milk suitability for cheese making. Manuelian et al. (2017) investigated the ability of PLSR applied to MIRS to predict milk coagulation traits in Mediterranean buffalo; Manuelian et al. (2017) reported a coefficient of determination in crossvalidation varying from 0.27 for k20 prediction to 0.76 for pH prediction. After this, Manuelian et al. (2017) categorized the samples into noncoagulating milk and coagulating milk with the purpose of discriminating the samples based on their milk coagulating ability; the model correctly classified 91.57% and 67.86% of noncoagulating milk samples in the calibration and validation sets, respectively. Results from the present study reveal a poor discrimination between coagulating and noncoagulating milk, likely due to the unbalanced data available; only 3.2% of samples were considered noncoagulating in the data set used here.</p>
<p>Although different studies reported the potential of predicting classes, either by clustering similar traits or by dividing a specific trait in classes (Manuelian et al., 2017;Grelet et al., 2019;Duplessis et al., 2020), accurate methods that permit the comparison of regression results and classification results are needed to enable appropriate conclusions about the optimal approach; unfortunately, no such statistical method currently exists. Which approach should be used depends on the context and on the type of variable to be predicted. These studies used canonical discriminant analysis (Manuelian et al., 2017) or PLSDA (Grelet et al., 2019;Duplessis et al., 2020) to perform class prediction, but the SVM has been shown in the present study to be a possible alternative due most likely to its ability to exploit nonlinear associations between the wavelengths and the trait.</p>
<p>Milk coagulation properties such as greater curdfirming capacity and shorter milk coagulation time are correlated with improved sensory properties of cheese as well as with greater cheese yield (Martin et al., 1997;Pretto et al., 2013). Heat stability, CMS, and pH are fundamental traits for cheese production and production of other milk-related products such as milk powder (Singh, 2004). Similarly, α S1 -CN, β-CN, κ-CN, and β-LG B in milk have positive effects on cheese yield (Wedholm et al., 2006). Although the ML methods investigated here only slightly improved predictions over PLSR, and only for some traits, and despite the prediction accuracy remaining low, the modern ML methods investigated in the present study clearly demonstrate promise. Improved prediction of traits, however small, is useful for the milk processing industry to discriminate milk at the preprocessing stage, enabling milk to be used for the process for which it is most suited.</p>
<p>Although some of the improvements in accuracy may be small, they can be generated at no additional physical or computational expense; the run times for the methods considered are of a similar order of magnitude to run times for PLS approaches. Further, all the methods considered can be easily implemented on a standard personal computer. As such, the modern statistical ML methods have similar practical utility to currently used methods.</p>
<p>In chemometric analyses, more interpretable methodologies (e.g., PLSR or LASSO) should be preferred to more complicated methods (e.g., NN) if the results do not differ, due to their interpretability and their reliance on fewer tuning parameters. In the current study, the "best" method varied depending on the data distribution and the range and variability present in the trait under investigation. Several methods demonstrated better or comparable performance to the traditionally used PLS-based methods, and practitioners should consider such alternative methods when predicting milk traits from mid-infrared data. The MA approach was the method that most often had the lowest RMSEV, and its use and implementation should be considered for regression analyses. The division of continuous traits into classes can be a useful solution for traits poorly predicted with regression methods. However, prediction of traits that were divided into more than 2 classes performed poorly here. Although accuracy of prediction of traits in this study was moderate, the application of novel statistical ML methods may improve the prediction of milk traits; however, the well-established PLSR-based method still performed competitively. (<rs xml:id="12965661" type="software" subtype="implicit">Code</rs> generated for use in this study is available at <rs xml:id="12965662" type="url" corresp="12965661">https: / / github .com/ maria -ire/ Code -used -for -milk -quality -traits -predicted -from -routinely -available -milk -spectra -Paper -analyses</rs>.)</p>
<p>1</p>
<p>RCT = rennet coagulation time; k20 = curd-firming time; a30, a60 = curd firmness at 30 and 60 min, respectively.</p>
<p>2</p>
<p>Proportion of correctly classified observations. 3 RCT = rennet coagulation time; k20 = curd-firming time; a30, a60 = curd firmness at 30 and 60 min, respectively.</p>
<p>Frizzarin et al.: MACHINE LEARNING APPROACHES TO SPECTRAL PREDICTION Journal of Dairy Science Vol. 104 No. 7, 2021</p>
<p>PLSR = partial least square regression; RR = ridge regression; EN = elastic net; Average = model averaging approach; NN = neural network; LASSO = least absolute shrinkage and selection operator.</p>
<p>RCT = rennet coagulation time; k20 = curd-firming time; a30, a60 = curd firmness at</p>
<p>and 60 min, respectively.</p>
<p>Journal of Dairy Science Vol. 104 No. 7, 2021</p>
<p>This research was funded by a Science Foundation Ireland, Starting Investigator Research Grant, "Infrared spectroscopy analysis of milk as a low-cost solution to identify efficient and profitable dairy cows," 18/SIRG/5562, and has been supported in part by a research grant from Science Foundation Ireland and the Department of Agriculture, Food and Marine on behalf of the Government of Ireland under the grant 16/RC/3835 (VistaMilk). The authors thank Mark Fenelon and Andre Brodkorb of the Teagasc Food Research Centre for their time in assisting with our milk protein queries and also thank Giulio Visentin and Audrey McDermott, formerly of Teagasc Moorepark, for collating the data used in this study. The authors have not stated any conflicts of interest.</p>
</text>
</tei>