<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
<teiHeader>
<fileDesc xml:id="_1"/>
<encodingDesc>
<appInfo>
<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T16:32+0000">
<ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
</application>
</appInfo>
</encodingDesc>
</teiHeader>
<text xml:lang="en">
<p>The concept of innovation ecosystems has become prominent due to its explanatory power. It offers a convincing account of innovation, explaining how and why innovation pathways change and evolve. It has been adopted to explain, predict, and steer innovation. The increasing importance of innovation for most aspects of human life calls for the inclusion of ethical and social rights aspects into the innovation ecosystems discourse. The current innovation ecosystems literature does not provide guidance on how the integration of ethical and social concerns into innovation ecosystems can be realised. One way to achieve this is to draw on the discussion of responsible research and innovation (RRI). This paper applies RRI to the innovation ecosystems discourse and proposes the concept of responsible innovation systems. It draws on the discussion of the ethics of artificial intelligence (AI) to explore how responsible AI innovation ecosystems can be shaped and realised.</p>
<p>The concept of innovation ecosystem has been widely adopted in parts of the academic discourse on technology management and innovation studies (Arenas, Goh, &amp; Urueña, 2019;Bacon, Williams, &amp; Davies, 2019;Chae, 2019;Hou &amp; Shi, 2021;Hu, Huang, Zeng, &amp; Zhang, 2016;Javed et al., 2020;Kim, Kim, &amp; Lee, 2016;Leong, Tan, Xiao, Tan, &amp; Sun, 2017;Park, Lee, Yoo, &amp; Nam, 2016;Senyo, Liu, &amp; Effah, 2019). It has also inspired numerous other discourses and the ecosystems concept is liberally applied to emerging technical systems in media and policy discourses. This wide-spread use of the term is testament to the intuitive power of the term to describe the way in which novel and emerging technologies develop and flourish. So far, the innovation ecosystems terminology has been used mostly in descriptive terms, for example to describe how certain socio-technical configurations come into being or why they develop in particular ways. Discourses using the term 'innovation ecosystems' tend to be supportive of innovation and technology development, emphasising desirable social and economic outcomes of innovation.</p>
<p>Technical change, however, is not always and exclusively positive and desirable. Schumpeter's (2003) concept of creative destruction, frequently cited in innovation studies, is an indicator that not all consequences of change are desirable. Innovation typically produces winners, but it often also produces losers. The benefits and downsides of innovation are normally not equally distributed. Technological innovation can affect human rights, moral claims, economic status and many other aspects of the individuals and groups that are affected (Jasanoff, 2011;Nathan, 2015). While this statement is at least potentially true for all innovation, its importance is underlined by the prevalence of socio-technical change in current technology-enabled society. Such questions of ethical consequences of new technologies and how these are to be addressed are currently at the top of policy agendas and accompanied by a detailed and fine-grained public discussion (European Commission, 2021;European Parliament, 2020;Jasanoff, 2003).</p>
<p>This paper argues that the current innovation ecosystems discourse fails to pay sufficient attention to ethical questions. Analysing key publications from the discourse, the paper highlights this gap in the literature and asks how this gap can be overcome. It proposes that one avenue to integrate ethical and social awareness into the innovation ecosystems literature and practice is to draw on the discussion of responsible research and innovation (RRI). This concept has been prominently discussed, in particular with regards to publicly funded research and innovation projects (see Section 3.1 for an overview). RRI aims to ensure the acceptability, desirability and sustainability of research and innovation processes and outcomes. The paper argues that RRI takes place within innovation ecosystems but it also shapes these ecosystems and can spawn novel innovation ecosystems. The question is then how RRI can inform activities undertaken in innovation ecosystems but also which lessons from the innovation ecosystems ideas can be used to inform RRI. Or, to put it differently, the paper asks whether responsible innovation ecosystems are possible and what they might look like.</p>
<p>In order to provide the practical background and give illustrations, the paper applies the conceptual ideas concerning innovation ecosystems and RRI to artificial intelligence (AI), in particular to the current discussion of ethical and human rights implications of AI. This is a hugely prominent debate that has spawned the creation of numerous projects, groups, initiatives, and even entire new journals. The ecosystems concept has been prominently adopted by some participants in the AI ethics debate, but so far, no specific conclusions or implications for innovation ecosystems have been drawn from this (European Commission, 2020;OECD, 2019;UNESCO, 2020). The paper thus makes contributions to the innovation ecosystems literature, the literature on RRI and the ethics of AI debate.</p>
<p>The paper proceeds as follows. It reviews the discourses on innovation ecosystems and argues that this discourse fails to engage in sufficient depth with ethical and social issues. It then proceeds to introduce the concept of RRI and to draw out ethical implications of the ecosystem concept. In the subsequent section the paper then proceeds to demonstrate the validity of these ideas using the example of AI. It argues that AI can be understood as a system of ecosystems. The current discussion of ethics of AI is briefly summarised to highlight how the ethical implications of the ecosystem metaphor apply to AI. The discussion then shows how these insights can be used to shape responsible AI innovation ecosystems.</p>
<p>This section provides the motivation of the paper by showing that the innovation ecosystems literature fails to engage deeply with ethical concerns, even though these are a crucial constituent of all human systems and interaction. It starts by introducing the innovation ecosystems discourse and then explores how ethical and social dimensions are covered in this discourse.</p>
<p>The application of the metaphor of an ecosystem to businesses, the economy and innovation has its roots in the 1990s, in particular in Moore's (1993) seminal paper. The ecosystem concept has been highly successful and captured the imagination of scholars from various backgrounds and has motivated several structured literature reviews (Klimas &amp; Czakon, 2021;Scaringella &amp; Radziwon, 2018;Tsujimoto, Kajikawa, Tomita, &amp; Matsumoto, 2018). It is also of interest to business practitioners and policymakers (Granstrand &amp; Holgersson, 2020). Its attraction arises from the fact that it provides a straightforward and widely accepted concept that can be applied to understand the dynamics of economic systems and the mechanisms of innovation (Nylund, Ferras-Hernandez, &amp; Brem, 2019). It is particularly attractive when applied from an organisational perspective, where it can help to develop organisational strategy, for example with regards to the structuring of innovation management (Ritala &amp; Almpanopoulou, 2017). Thinking about an organisation as part of an ecosystem can help to identify opportunity for collaboration as well as competition (Gobble, 2014;Gomes, de, Facin, Salerno, &amp; Ikenami, 2018). More broadly, an ecosystems perspective can help an organisation identify opportunities for growth (Adner, 2006) and chart a course for economic success (Bandera &amp; Thomas, 2019).</p>
<p>Innovation ecosystems tend to focus on a particular aspect of economic activity. However, the way in which the boundary of the ecosystem is drawn depends on the relevant knowledge interest and may change over time. Examples of ecosystems discussed in the literature include particular technologies, such as smart tourism (Arenas et al., 2019;Park et al., 2016) or fintech (Leong et al., 2017), but ecosystems can also be delineated using geographical boundaries or confined to specific industries or platforms (H. Hu et al., 2016;Javed et al., 2020;Kim et al., 2016). Numerous studies combine different types of ecosystem boundaries, e.g. by focusing on the application of a particular technology within a geographical region or a particular type of organisational environment (Tejero, Pau, &amp; León, 2019).</p>
<p>The ecosystems metaphor furthermore allows taking a wider perspective as well, one that looks at the role of economic actors from a societal perspective (Carayannis, Grigoroudis, Stamati, &amp; Valvi, 2021). This is of interest for decisions on a policy level where an innovation ecosystem not only has the role of creating profits for individual organisations but potentially has a broader impact on societal stakeholders. From the societal perspective the struggles of individual actors within particular ecosystems are less interesting, but the overall success of particular ecosystem in producing innovation and the resulting benefits, or their failure to do so (Ghazinoory, Sarkissian, Farhanchi, &amp; Saghafi, 2020), are of importance. This implies that societies can also make decisions on which ecosystems they support and which ones they allow to perish.</p>
<p>The ecosystems concept has been applied to a number of different aspects of the business world (Tsujimoto et al., 2018). In addition to the term "innovation ecosystem", there are various others such as "business ecosystem" (Adner, 2006), "digital business ecosystem" (Senyo et al., 2019), "digital innovation ecosystems" (Chae, 2019), "digital ecosystem" (Senyo et al., 2019), "platform ecosystems" (Jacobides, Cennamo, &amp; Gawer, 2018) or "knowledge ecosystem" (Gomes et al., 2018). For the purposes of this paper the detailed distinctions between these concepts are less interesting than the question which defining features they share.</p>
<p>The ecosystem metaphor, which originally describes the relationship between living organisms in a particular environment, points to evolution as a central feature that plays an important role in explaining change. Darwinian evolution is almost universally accepted across natural and social sciences, to the point where it can be seen as a universal philosophy of change (Porra, 1999). Moore (1993) already pointed to stages of evolution of ecosystems as stages of change. And, while the concept of progress is highly contentious in biological sciences (Porra, 1999), the use of the ecosystem metaphor is used to imply not just change but also progress.</p>
<p>Innovation ecosystems can be divided into different types (e.g. industrial, service, technical, (Scaringella &amp; Radziwon, 2018)) which have different types of members (e.g. private and public bodies (Asplund, Björk, Magnusson, &amp; Patrick, 2021)) a number of further important features and characteristics. The members of innovation ecosystems are described as interdependent (Ritala &amp; Almpanopoulou, 2017) and co-evolving (Gomes et al., 2018;Hou &amp; Shi, 2021). Relationships between members are complex, due to simultaneous competition and collaboration between them. These ecosystems often evolve around a central node (Gobble, 2014) which can be a keystone actor, a platform leader or a particular technical platform (Gomes et al., 2018). Investigation of ecosystems can focus on their structures or the actors that constitute them (Adner, 2017). Ecosystems have spatial and temporal boundaries, but these can be difficult to identify. Ecosystems are embedded in broader environments and can include sub-ecosystems (Pombo-Juárez et al., 2017).</p>
<p>Fig. 1 provides an overview of some of the key features of innovation systems that are relevant to the question of what responsible innovation systems might look like.</p>
<p>The above overview of the innovation ecosystems literature shows the scope of the discourse and the importance of the concept. In light of the potentially broad impact that innovation ecosystems can have on their human members, their social and natural environment, one could assume that ethical and social concerns figure prominently in the innovation ecosystems literature. However, while there are clearly references to ethical concerns in the innovation ecosystems literature, there is little guidance on how these broader concerns can be explicitly and visibly integrated into the design and maintenance of innovation ecosystems. This section introduces the concept of ethical and social concerns and provides evidence of this gap in the literature which forms the motivation and justification for the following introduction of the concept of responsible innovation ecosystems.</p>
<p>Ethics in the common use of the term in English refers to a broad range of phenomena that are united by the fact that they point to a perception of something being good or bad, right or wrong, appropriate or inappropriate. This can refer to an immediate intuitive reaction one can have to such phenomena (e.g. "this is bad!"), it includes explicit statements about them (e.g. "it is always good to do X"), it can refer to justifications of such statement (e.g. "it is bad to do Y, because…") or abstract considerations (e.g. "the property Z makes an action appropriate") (Stahl, 2012). Ethics as a philosophical discipline has developed many theoretical approaches to reflecting on such ethical phenomena, notably theories that look at the consequences of actions for an ethical evaluation (i.e. consequentialist or utilitarian theories (Bentham, 1789;Mill, 1861)), theories that focus on duties of the agent (i.e. deontological theories (Kant, 1788(Kant, , 1797))) and theories that focus on the character of the agent (i.e. virtue ethics (Aristotle, 2007;MacIntyre, 2007)).</p>
<p>This article does not offer the space to discuss philosophical ethics in much detail. Suffice it to say that in everyday life we often encounter situations where moral consensus breaks down and ethical questions and reflection are called for. This is most prominently the case in situations of change when established practice is questioned. The paper uses the term "ethical and social concerns" to refer to such situations where there is disagreement on what exactly is the right and ethically justifiable course of action.</p>
<p>The role of ethics in the economic system has long been hotly debated. Friedman (1970) adage that it is the social responsibility of the organisation to increase its profit may still have some adherents. Overall, however, it is important to realise that the very idea of market exchanges is predicated on ethical premises (Smith, 1776). In practice, companies and other market participants tend to realise that their actions have ethically relevant consequences and they have a prima facie duty to engage with them, be it because they perceive an ethical duty to do so, or be it just to safeguard their reputation. How exactly such ethical engagement can be realised is discussed in fields such as business ethics (Bowie, 1999;De George, 1999;Velasquez, 2001), corporate social responsibility (European Commission, 2011; Garriga &amp; Melé, 2004;Porter &amp; Kramer, 2006) or stakeholder theory (Blok, Hoffmans et al., 2015;Carroll, 1991;Freeman &amp; Reed, 1983).</p>
<p>The key point is that innovation ecosystems have strong influence on individual and collective options and choices and are therefore likely to raise ethical and social concerns. It would therefore be reasonable to assume that scholarship on innovation ecosystems would take these concerns into account and deal with them on a theoretical and practical level. However, this is not the case to a significant degree.</p>
<p>It is methodologically difficult to prove this lack of ethical engagement. The paper therefore offers some pointers to support the observation that may resonate with the readers' understanding of the innovation ecosystems literature. A first indication may be taken from a search of the literature. A search of the Scopus literature database of the term "innovation ecosystem" in title, abstract and keywords brings up 1258 hits (10.07.2021). Adding "ethic" (to allow for "ethics" and "ethical") to the search brings up 9 hits, most of which do not focus on ethical and social concerns in innovation ecosystems.</p>
<p>A second indication comes from an overview of key studies that aims to explore whether and to which degree ethical and social concerns are reflected. For this purpose, the paper briefly discusses the five most highly cited academic papers according to the Scopus database that have the term "innovation ecosystem" in the title with a view to determining which ethical and social concerns they explicitly address. Adner and Kapoor (2010), in the most highly cited study in the innovation ecosystems literature (1014 citations), frame their contribution to the discourse as going beyond the prevailing focus of the rivals of a firm and extending the focus to include the firm's environment. This provides the basis for a conceptual framework which can help organisations create and capture value. The study explores a set of hypotheses using quantitative data from the semiconductor lithography industry. The second most highly cited publication, also by Adner (2006) (641 citations), draws on empirical examples, but offers a high-level overview of how ecosystems strategies can be formulated. It argues that organisations need to assess ecosystems risks, in order to develop a stronger understanding of the market and develop contingency plans which will increase profitability.</p>
<p>These two studies discuss and touch on a number of technologies (hardware development, airline industry, HDTV) that have broad impact on society and the natural environment. It is likely that these technologies and the ecosystems in which they are developed and used</p>
<p>would raise numerous ethical and social concerns. However, the publications remain within the prevailing business paradigm and focus exclusively on the question how organisations can benefit from an understanding of their innovation ecosystem.</p>
<p>The three papers that follow in the citation ranking all move outside of the setting of immediate private sector economic competition and could therefore be expected to engage more deeply with ethical and social issues. Zygiaris (2013) (274 citations) studies smart cities and looks at how smart city innovation ecosystems can be constructed by offering a smart city reference model. This model is discussed using example cities (Barcelona, Amsterdam, Edinburg) to demonstrate its validity. As the topic of inquiry is in the public sector, a broader consideration of stakeholder is included. A key point of the smart cities reference model is that it incorporates one of the most visible social concerns, namely environmental sustainability. Rohrbeck, Hölzle, and Gemünden (2009) (223 citations) provide an account of how a private company, albeit a spun-out former state agency, created an open innovation ecosystem. Drawing mostly on qualitative data, the study argues that the organisation has been successful in creating an open innovation ecosystem which it sees as crucial to its survival.</p>
<p>Both Zygiaris (2013) and Rohrbeck (2009) can thus be seen as explicitly embracing ethical and social concerns such as environmental sustainability, local democratic administration, fair distribution of access to knowledge and intellectual property. They recognise the importance of these issues but provide little guidance on how they can be integrated into the structure of the ecosystem.</p>
<p>The study by Carayannis and Campbell (2009) (595 citations) suggests that innovation ecosystems need to adopt "mode 3", i. e. include people, culture and technology. The quadruple helix refers to the interaction between academia, industry, the state and media in structuring ecosystems. In a more recent publication Carayannis et al. (2021) extend this idea to the quintuple helix which explicitly includes civil society and the natural and social environment. Carayannis and Campbell argue that these factors lead innovation ecosystems towards a "democracy of knowledge". This approach explicitly engages with ethical and social questions but does so at a high level of national and international strategy and thus provides little guidance on how these concerns can be integrated on the level of the individual innovation ecosystem.</p>
<p>This short overview of some key studies does not "prove" anything in a scientific sense, but it should lend credence to the observation that the innovation ecosystems literature so far pays relatively little explicit attention to ethical and social concerns on the level of the ecosystem itself. Even in cases where these aspects are covered this is either done implicitly or it is done in ways that do not provide practical insights or guidance on how these aspects can be addressed in practice. In light of the potentially significant impact that such ecosystems can have the paper asks how the social and ethical impact of innovation ecosystem can be addressed.</p>
<p>The argument that ethical and social issues are not well covered in the innovation ecosystems literature is the starting point for the following discussion of responsible innovation ecosystems. This paper introduces the term "responsible innovation ecosystem" rather than, for example, "ethical innovation ecosystem" for several reasons. One is the complexity of the concept of ethics that points at various levels of abstraction and interlocking theories and discourses. Another reason is that the concept of responsibility is broadly used to denote practical approaches to a range of ethical concerns, for example in corporate social responsibility. Finally, by choosing the term responsibility, the paper can draw on the well-established discourse on RRI, which is closely linked to innovation ecosystems.</p>
<p>The concept of RRI rose to prominence in the early 2010s. Building on roots in various prior discourses such as technology assessment, science and technology studies, philosophy of technology or computer ethics, RRI was developed as "a transparent, interactive process by which societal actors and innovators become mutually responsive to each other with a view to the (ethical) acceptability, sustainability and societal desirability of the innovation process and its marketable products (in order to allow a proper embedding of scientific and technological advances in our society)" (Von Schomberg, 2013, p. 63). It is most visible in publicly funded research and prominently adopted by the European Union which described it at the beginning of the Horizon 2020 Research Framework Programme as "the on-going process of aligning research and innovation to the values, needs and expectations of society." (Rome Declaration, 2014). RRI became an integral part of Horizon 2020 but it was equally adopted by a number of national research funders, for example in Norway, the Netherlands and the UK.</p>
<p>The academic discourse has addressed many different aspects of RRI, from broad conceptual questions to ways and means of integrating RRI into projects and organisations to questions of assessing and measuring outcomes and consequences. One prominent model of RRI that was adopted by the UK's Engineering and Physical Sciences Research Council (Owen, 2014) and that was based on the probably most widely cited conceptualisation of RRI proposed by Stilgoe, Owen, and Macnaghten (2013), focuses on four aspects: anticipation, reflection, engagement and action, which are represented in the AREA framework. This implies that research and innovation activities, in order to count as responsible need to anticipate possible outcomes. They need to engage with relevant stakeholders, they need to integrate systematic reflection of their assumptions and practices and they need to modify their actions in accordance to the insights generated by the earlier points. This paper does not offer the space to discuss geographical and disciplinary differences in interpreting RRI, such as the distinction between the UK AREA approach and the European Union's approach using a number of so-called keys (Owen, Pansera, Macnaghten, &amp; Randles, 2021). Suffice it to say that the AREA framework and the brief outline of RRI offered here should be sufficient to capture the spirit of RRI to provide an indication of what responsible innovation ecosystems could look like.</p>
<p>The application of the concept of RRI and in particular the AREA framework to innovation ecosystems provides some pointers to what an innovation ecosystem would need to look like, in order to be responsible. Responsible innovation ecosystems should have desirable outcomes, e.g. they should help to meet accepted goals, such as the achievement of the UN's Sustainable Development Goals (Nylund, Brem, &amp; Agarwal, 2021a, 2021b;Oliveira-Duarte et al., 2021). On the process level, responsible innovation ecosystems can be defined as innovation ecosystems that enable and foster responsibility of their members. In this sense responsible innovation ecosystems need to support the ability of their members to anticipate possible outcomes of their innovation activities. They need to have structures (Phillips &amp; Ritala, 2019) that enable reflection, not least by facilitating engagement with relevant stakeholders which can be members of the ecosystem, but who can also be situated outside of the ecosystem. And, finally, responsible innovation ecosystems should have mechanisms that allow their members to take action, including possible modifications of the ecosystem itself to act on the insights generated by anticipation, engagement and reflection.</p>
<p>All of this suggests that it should be possible to shape and design innovation ecosystems in ways that are more or less responsible, more or less aligned with the principles of RRI. This idea of shaping ecosystems is not new to the innovation ecosystems literature (Wareham, Fox, &amp; Cano Giner, 2014). Governance sets incentives for desirable developments in ecosystems and avoids undesirable ones. Nylund et al. (2019) suggest that innovation ecosystems can be activated, that their members can actively drive their growth. They furthermore suggest that innovation ecosystems can develop a certain level of reflexivity, i.e. an explicit understanding of how they work. Pombo-Juárez et al. (2017) explore the capacity of innovation ecosystems to anticipate possible futures and react accordingly. An important part of ecosystem governance is to strike a balance for the ecosystem to remain stable while simultaneously facilitating change (Wareham et al., 2014). These proposals indicate that ecosystems governance including the shaping and steering of innovation ecosystems is possible and desirable.</p>
<p>Overall, the application of RRI to innovation ecosystems with a view to establishing responsible innovation ecosystems is thus compatible with the innovation ecosystems literature. It is also consistent with our approach to natural ecosystems, in which humans often intervene to achieve desired outcomes, such as biodiversity or long-term sustainability. The main novelty, the aspect that does not figure prominently in the current innovation ecosystems literature, is that responsible innovation ecosystems should actively seek to align their processes and expected outcomes with societal needs and / or preferences, to ensure they are acceptable, desirable and sustainable. Accepting this premise is of course a big step. So far innovation ecosystems are typically seen as quasi-natural entities that emerge and evolve and whose main purpose is to grow and prosper. Whether this happens and how it relates to societal preferences is left to market mechanisms. If an innovation ecosystem does not lead to outputs that satisfy societal preferences, it will have no market and die (Moore, 1993). In order to become responsible, innovation ecosystems would have to move beyond this position and actively embrace the idea that societal needs are not only addressed through marketable outputs but also in other ways which include external stakeholders who may or may not be possible market participants.</p>
<p>This begs the question why an innovation ecosystem should or might want to become responsible. There are functional arguments for this, along the lines that an alignment with societal preferences is in the longterm interest of the survival of the ecosystem and its members. One can also imagine ethical arguments based on the idea that human action should be guided by a recognition of the moral nature of human interaction and that this calls for being responsible. These questions are variations of the question "why be good?" which has long been discussed in various fields of applied ethics, including business ethics and corporate social responsibility. This paper assumes that there is a desire for innovation ecosystems to act responsibly and asks what such responsible innovation ecosystems would look like.</p>
<p>Having discussed the characteristics of innovation ecosystems, introduced the concept of RRI and combined them to propose the idea of responsible innovation ecosystems, a further conceptual question is how such responsible innovation ecosystems could be realised. A detailed response to this question will depend on the specifics of the innovation ecosystem, the technological, scientific, or social innovations that are involved as well as numerous other factors such as the size of the ecosystem or its broader environment.</p>
<p>While there are unlikely to be one size fits all prescriptions, there are some requirements that interventions into innovation ecosystems will have to fulfil for them to be successful in shaping the ecosystem to become responsible. These requirements can be deduced from the characteristics of innovation ecosystems (see Fig. 1) as well as from the RRI discourse:</p>
<p>1. Delimitation of the ecosystem: The boundaries of ecosystems are typically not clearly defined, which is true for natural as well as innovation ecosystems (Phillips &amp; Ritala, 2019). Any targeted intervention into an ecosystem should therefore be clear on the delimitation of the ecosystem that it focuses on, in order to ensure that the effects can be properly planned and assessed. The delimitation of the innovation ecosystem should be clear across categories, e.g. the geographical boundaries, technological content or intended ecosystem membership. 2. Knowledge base: In order for an innovation ecosystem to be responsible, its members must be in possession of knowledge across a number of fields. This clearly includes knowledge of the innovation domain itself, such as technical expertise, but it also includes conceptual knowledge and procedural knowledge concerning various aspects of RRI. The members of the ecosystem need to be able to identify stakeholders, engage in anticipatory activities or develop viable policies. The knowledge base will be dynamic, so the innovation ecosystem must have mechanisms to maintain and update it and build the capacity of it is members to utilise the available knowledge. 3. Ecosystems governance: Governance mechanisms need to be sensitive to the dynamic and shifting nature of innovation ecosystems.</p>
<p>They should be flexible and adaptable and incorporate the ability to learn and react appropriately to changes in the system. They should also take into account existing responsibilities and work with those, a property sometimes referred to as meta-responsibility (Sonck, Asveld, &amp; Osseweijer, 2020).</p>
<p>These requirements for interventions are still somewhat abstract. They are, however, a suitable basis from which to deduce actionable insights and advice, as will be shown in the following section that applies these ideas to AI ecosystems.</p>
<p>This section shows that the concept of innovation ecosystem has already been fruitfully applied to AI. It picks up on the discourse on ethics and AI to underline the relevance of responsible approaches and applies the concept of responsible innovation ecosystems to the AI discourse.</p>
<p>There is an abundance of definitions of AI. These go back to the coining of the term in the 1956 Dartmouth summer research project on artificial intelligence (McCarthy, Minsky, Rochester, &amp; Shannon, 2006). In the proposal for the event McCarthy et al. suggest that machines can be made to simulate "every aspect of learning or any other feature of intelligence" which include the use of language, formation of abstractions and concepts, solving problems now reserved for humans and self-improvement. There is a proliferation of similar definitions that point to the technical replication of activities that would require intelligence in humans or animals (Brundage et al., 2018;Collins, 1990;Rai, Constantinides, &amp; Sarker, 2019) but there is little agreement on which activities exactly require intelligence. Despite a plethora of definitions of AI, there is little convergence between them. One reason for this appears to be that the discourse on AI covers a number of similar and overlapping topics, technologies and techniques which do not have an easily identified common core that all of them share (Elsevier, 2018).</p>
<p>This lack of a single defining focus or feature may be one of the reasons why the ecosystems concept has been used to describe AI. The use of this metaphor is very prominent in policy-oriented documents, such as the European Commission's White Paper on AI (2020) which proposes policies to establish an "ecosystem of excellence" and an "ecosystem of trust for" AI. The OECD's (2019, p. 3) recommendations on AI propose the "fostering of a digital ecosystem for AI".</p>
<p>The fact that AI is not so much one technology or platform and better understood as a family of overlapping aspects, technologies and techniques suggests that it is more useful to use the innovation ecosystem metaphor in the plural, i.e. to speak of AI innovation ecosystems. A brief B.C. STAHL look at the current AI landscape confirms this. In line with the criteria developed above, AI ecosystems can be divided by geography (e.g. a European, American, Chinese), they can be distinguished by technology (e.g. machine learning, natural language processing, fuzzy logic) or by application area (e.g. transport, healthcare, entertainment), to name some of the most obvious lines of demarcation between AI innovation ecosystems.</p>
<p>Having established the usefulness of the application of the innovation ecosystems metaphor to AI, the next step is to explore why responsible AI ecosystems would be called for. In the AI case this is easy to argue for, given the very prominent discussion of ethics and AI.</p>
<p>Questions of ethics and AI can be traced back to the very beginning of digital computing (Wiener, 1954) and resonate with many discussions of ethical aspects of computing (Floridi, 1999;Johnson, 2001). While these questions thus have decades of history, they have only recently gained prominence beyond a circle of interested scholars. The reason for this relatively late onset of public interest in the ethics of AI is that only from about the early or mid-2010s onward AI has developed to the point where it has broader social relevance. This is caused by co-occurrence of several developments that were required to give potency to AI. The first of these development is the refinement of machine learning algorithms, in particular deep learning (LeCun, Bengio, &amp; Hinton, 2015) and other implementations of artificial neural networks. These algorithms require large computational power, which has been made available through technical developments which is the second development. Thirdly, they rely on the availability of large amounts of data for the training of models and their validation, which have also become available in recent years (Borges, Laurindo, Spínola, Gonçalves, &amp; Mattos, 2021).</p>
<p>Because of these three developments, AI, in particular machine learning-based applications, has become very powerful and many promising applications have been developed. AI promises to allow companies and public administrations to make better use of their data, optimise their processes, cut waste, promote sustainability (Nishant, Kennedy, &amp; Corbett, 2020), and even develop entirely new ways of doing business (Cao, Duan, Edwards, &amp; Dwivedi, 2021;Duan, Edwards, &amp; Dwivedi, 2019). The Covid pandemic has provide more examples of how AI can be used to support the public interest, e.g. by strengthening diagnosis, contact tracing, workplace safety and more (Kumar, Dwivedi, &amp; Anand, 2021;Sipior, 2020).</p>
<p>This promising narrative has been taken up and promoted by many companies under the leadership of the big tech companies who have the technical capabilities and data required to exploit the opportunities. Policymakers worldwide have adopted the narrative and put their weight behind the development of AI ecosystems to ensure that their constituents can benefit from the new technical capabilities (European Commission, 2020;FRA, 2020;Hall &amp; Pesenti, 2017;UK AI Council, 2021).</p>
<p>At the same time, it has become clear that AI raises many challenges, ranging from technical and data questions to social and political issues (Ashok, Madan, Joha, &amp; Sivarajah, 2022;Dwivedi et al., 2021). One of these challenges is that AI has the potential to negatively impact ethical expectations and human rights (Coeckelbergh, 2020). It became clear to policymakers several years ago that promoting AI is a double-edged sword and that attempts to strengthen the AI industry can lead to push-back unless ethical concerns are addressed simultaneously. The most visible early example of this explicit attempt to weigh benefits and downsides is expressed in the dual report to the Executive Office of the President (2016a), (2016b) which has been followed up by governments and legislatures from many other countries and regions, including the European Union (European Commission, 2018) and the UK (House of Commons Science &amp; Technology Committee, 2016; House of Lords, 2018). These initial overview reports have led to further activities, for example the creation of the High Level Expert Group on AI in the EU or the Centre for Data Ethics and Innovation in the UK which are tasked with clarifying the nature of ethical and human rights concerns and possible ways of addressing them.</p>
<p>The issues of concern cover a broad spectrum (Müller, 2020). Specific AI techniques have characteristics that can raise specific concerns. Deep learning algorithms, for example, are difficult if not impossible to understand even for expert users. How an algorithm transforms input to output is scientifically and practically opaque. At the same time, learning systems change their internal states and the inputoutput relationship is not constant over time. This leads to concerns about transparency and explainability (Access Now Policy Team, 2018), in particular in those cases where algorithms prepare or execute decisions that affect humans, such as the approval of mortgages or the decision whether to grant parole. Automated decision may hide biases that an algorithm picks up from the training data which can lead to or exacerbate existing discrimination (Akter et al., 2021;Latonero, 2018).</p>
<p>The big data requirements of such systems for training and validation purposes raise concerns in those cases where they make use of personal data. Data protection and privacy considerations loom large (Kaplan &amp; Haenlein, 2019), partly because of the amount of data, partly because algorithms may allow novel insights and sometimes due to novel types of data, such as emotional data (Dignum, 2019). These systems also raise concern about safety, reliability and security (AIEI Group, 2020;Babuta et al., 2020).</p>
<p>In addition to these immediate concerns directly related to the characteristics of particular AI techniques, notably machine learning, there are broader concerns that arise from the way AI can support other socio-technical systems and how they impact our lives. Worries about automated decision making can go beyond bias and discrimination and point to well established (Weizenbaum, 1977) questions about the limits of machine autonomy (Q. Hu, Lu, Pan, Gong, &amp; Yang, 2021;Shneiderman, 2020) and what we should allow machines to do versus what should be unique to humans. There are worries about the economic consequences of wide-spread AI use which may lead to unemployment, at least in some sectors (Stone et al., 2016). AI undoubtedly has the potential to produce immense wealth but the distribution of the benefits of this wealth is contested. In her account of "surveillance capitalism", Zuboff (2019) develops a compelling narrative of unfair value extraction by the big tech companies to the detriment of consumers. AI-assisted systems furthermore raise concerns about their impact on political processes. Triggered by high-profile scandals like the Facebook / Cambridge Analytica one (Isaak &amp; Hanna, 2018), political consequences of technology use are highly visible on the public agenda. These concerns not only cover apparent misuse, but the trend that AI-assisted systems can lead to economic and political power concentration (Nemitz, 2018) which render the big owners of AI technologies beyond the reach of national governments and societal norms and expectations.</p>
<p>This list of ethical and human rights concerns about AI could easily be extended, ranging from the immediate to long-term considerations (Baum, 2018;Cave &amp; Óh Éigeartaigh, 2019;Stix &amp; Maas, 2021). Its purpose in this paper is to demonstrate that AI innovation ecosystems have potentially significant consequences on stakeholders and societies. There is thus a call to ensure that these concerns are integrated into the innovation processes of the innovation ecosystems or, to put it differently, that AI innovation ecosystems should be responsible. How this could be achieved is discussed in the next section.</p>
<p>This section draws out implications for practice and proposes mechanisms that can be used to shape responsible AI ecosystem. It then discusses implications for theory and looks at the limitations of the paper and how these may inform future research before proceeding to the conclusion.</p>
<p>The previous section has suggested AI can be seen as a set of interlocking innovation ecosystems that raise a range of ethical concerns. This leads to the question of how insights into shaping ecosystems in ways that are conducive to human flourishing can be implemented in practice. A key component of this question is how the three main requirements, i.e. the delineation of the ecosystem, the creation and maintenance of the knowledge base and adaptive governance structures can be achieved.</p>
<p>To start with the question of delineation of the innovation ecosystem, it is probably not too surprising that the AI innovation ecosystems tend to be divided geographically along jurisdictional boundaries. This makes sense, as many governance structures are dependent on legislation. At the same time, technology tends to be global in reach and scope. In the case of AI innovation ecosystems, it is important, however, to realise that geographical delineation will not be enough. The technologies and their resulting concerns differ vastly, so that it seems advisable to divide ecosystems according to technology (e.g. machine learning, sociotechnical systems) and application area (e.g. healthcare, transport). While there may be overlap between those, the differences appear big enough to warrant separate treatment. One can therefore deduce a need to clearly define the concept of AI that is relevant toand to a large degree determinesan AI ecosystem. This conceptual delimitation is a key issue that a number of policy initiatives are struggling with, as they find it difficult to determine which technologies and techniques fall under the AI umbrella.</p>
<p>The second main area of importance to render innovation ecosystems responsible is that of knowledge. Within a particular innovation ecosystem there must clearly be the technical knowledge to develop, use and apply the technology that is constitutive of the ecosystem. However, to allow for the development of responsibility within ecosystems, there must be additional knowledge available. This includes the conceptual knowledge of the reasonably foreseeable ethical and human rights issues that may be caused or affected by the technology. However, simply having such knowledge available is not enough. There must be ways of integrating the knowledge of ethics and human rights into the innovation processes themselves and there must be reverse knowledge flows where growing understanding of the technology informs in more depth the ethical and human rights implications. This integration of knowledge must provide for critical reflexivity (Grimpe et al., 2020) and allow for the integration of diverse views and opinions.</p>
<p>Several options are being explored to bridge these different areas of knowledge. One refers to a set of methodologies that aim to integrate non-technical insights into technical development. These often have the suffix of "by design". Most prominent at the moment, partly because it is mandated by the EU's General Data Protection Regulation is privacy by design (Hansen, 2016;Information Commissioner's Office, 2008). There have been calls to use the "by design" label to include security, human rights and other aspects. Most of these methodologies can be traced back to the idea of value-sensitive design (B. Friedman, Kahn, &amp; Borning, 2006;Manders-Huits &amp; van den Hoven, 2009). It is probably not surprising that value-sensitive design has been identified as an important way of implementing RRI (de Reuver, van Wynsberghe, Janssen, &amp; van de Poel, 2020; Simon, 2017;van den Hoven, 2013). In order to take the next step, it would be desirable to have well-defined method of ethics by design for AI (Brey, 2020;Martin &amp; Makoundou, 2017;WEF, 2020).</p>
<p>Another and often related route for integrating technical, ethical and human rights knowledge would be through specific impact assessments. Again, the idea is not new in principle and impact assessments have been around for decades. They are probably most prominent with regards to the natural environment (Fagan &amp; Sircar, 2010). There are, however, numerous other types of impact assessment of varying degrees of formality and enforcement power. General social impact assessment has been promoted substantially (Becker, 2001;Fenton, 2005). Technology assessment has been undertaken for decades (Genus &amp; Coles, 2005;Gethmann, 2002;Grunwald, 2009). In the field of privacy there is privacy impact assessment, now often replaced by data protection impact assessment (Clarke, 2009;CNIL, 2015). This idea of impact assessment has also been broadened to cover ethical issues (Wright &amp; Friedewald, 2013) and human rights. What is missing is a set of specific methodologies that are tailored to the particular questions and needs of AI or specific AI techniques, even though there are a number of candidates that are currently discussed (HLEG, 2020; AI Now Institute, 2018; ECP Platform for the Information Provision, 2019; IEEE, 2020).</p>
<p>One way of creating stable and accessible knowledge bases for the types of processes that bring together scientists, technologists and other stakeholders can be provided by standards. Standardisation processes are stakeholder-based and can provide structures that are conducive to the broader reflection that innovation ecosystems call for. In addition to well-established standards, for example around quality assurance or information security, there have been developments to standardise ethical reflection through impact assessment (CEN-CENELEC, 2017) as discussed in the preceding paragraph. The IEEE has taken the lead in promoting standards that specifically focus on ethical and social issues of autonomous systems (IEEE, 2017(IEEE, , 2020)). While one can thus observe a number of ways in which ethical and human rights concerns in AI be addressed in ways that build on existing knowledge and methodologies, there is still some way to go in making these measures widely available and tailoring them for the specific AI technologies and innovation ecosystems in which they reside.</p>
<p>Promoting this availability and incentivising its use are a key task of activities inspired by the third requirement, which is innovation ecosystems governance. Governance mechanisms provide ecosystems resilience and stability (Wareham et al., 2014) and allow ecosystems members to work together, structure interactions and promote specific outcomes (Adner, 2017). Part of these governance structures will be legislation and regulation which can be used to clearly indicate what a particular society deems to be desirable. In the field of AI there are numerous legislative proposals at various stages (Rodrigues et al., 2020). For Europeans the most relevant one is the proposal for a Regulation on AI (European Commission, 2021). Other legislative frameworks that influence AI innovation ecosystems include those governing taxation, intellectual property, liability or health and safety.</p>
<p>One proposal for AI governance with potential ramifications for AI innovation ecosystems is that of the creation of a dedicated agency, which could be a regulator or similar body to oversee the broader implications of the sector. This might be modelled along the lines of existing regulators, such as the data protection authorities that exist in many countries, but it could also be a different type of body or a network of bodies. Miller and Ohrvik-Stott (2018) proposed the idea of an Office of Responsible Technology which, while focused on the UK, might be a model of a governance mechanism that could be relevant across various AI innovation ecosystems. Proposal for international coordinating bodies have also been developed (Jelinek, Wallach, &amp; Kerimi, 2020).</p>
<p>AI innovation ecosystems governance does not need to focus on the large national and legislative frameworks but can also incorporate local measures. These may be specific to the innovation ecosystem in question, e.g. when delineated by technology or application area. There are also governance structures that may be suitable to foster responsibility in innovation ecosystems, for example by creating dedicated positions in organisations which are tasked with fostering RRI. One proposal along these lines is to establish the role of an AI Ethics Officer in organisations (Wallach &amp; Marchant, 2019), a role that could work in a way that is similar to the Data Protection Officer role that is now mandatory in many European companies. Such a role could combine technical and ethical knowledge, be the gatekeeper for appropriate methodologies and be an intermediate in discussions about possibly conflicting aims of AI development. The advantages of this proposal should be weighed against the danger that it might lead to the perception that ethics is the responsibility of this particular role and thus can be ignored by other Fig. 2 provides an overview of the interventions that could be used to support AI innovation ecosystems to be responsible.</p>
<p>These suggestions for interventions into AI innovation ecosystems are still quite broad, but it should be clear that they can easily be turned into concrete policy proposals or organisational practice. In accordance with the underlying theme of ecosystems, they should also be viewed in conjunction. The knowledge-related interventions, for example, need to connect with the delineation of the ecosystem, as different types of technology would need different types of expertise. Similarly, there is a close link between knowledge and governance, as governance structures can promote the creation of knowledge and need to build on existing knowledge to be effective.</p>
<p>Despite the broad nature of these implications, they do provide indications of what practical consequences might be drawn on the organisational level. For example, an SME that forms part of an AI innovation ecosystem can draw conclusions from the structure outlined above. While many aspects of the innovation ecosystem and its governance will not be directly subject to interventions by the SME, the concept of responsible innovation ecosystems can guide specific action. This starts with the recognition on the part of the SME that it is part of the innovation ecosystem, that its work has social and ethical consequences and that it has a part to play in addressing these. The SME can then make use of some of the suggestions provided above, e.g. it could undertake and AI impact assessment, it could integrate ethics by design into its processes or it could appoint an AI ethics officer. Maybe even more importantly, the SME can recognise its role in shaping and maintaining the ecosystem in a way that supports its nature as a responsible ecosystem. This could be one by sharing experience and good practice, e. g. by contributing to standardisation activities, offering training opportunities on ethics and responsible innovation to its employees and transparently working with regulators and other ecosystems members to highlight the benefits and ethical issues related to the technology in question. While there are still many ways in which these recommendations might materialise in practice, it should be clear that they can be taken up by organisations and do not need to wait for political interventions.</p>
<p>The ideas developed in this paper can provide pointers to practical ways of integrating ethical concerns into the design and practice of AI innovation ecosystems. They are similarly of relevance to theoretical descriptions of these ecosystems. Two bodies of theory for which the arguments developed here are particularly relevant are those on innovation ecosystems and RRI.</p>
<p>The paper positions itself primarily as a contribution to the innovation ecosystems literature. The main argument is that ethics is an important aspect of such ecosystems but does not receive the attention it deserves. Using the example of the current AI debate, the paper has shown that this theoretical weakness of the innovation ecosystems literature can be identified conceptually and provide the basis of practical interventions.</p>
<p>Accepting this position may raise theoretical challenges for innovation ecosystems scholars. The adoption of the metaphor from the natural science may well have been motivated to some degree by the desire to eliminate human agency from an understanding of innovation processes. Moore's (1993) seminal and foundational paper is a case in point. His focus on the relationship between different animals allows a purely functional and detached observation of ecosystems. By integrating ethical concerns into the innovation ecosystems discourse, some of the elegance and simplicity of the ecosystems approach is lost. Ethical questions are complex and rarely subject to simple solutions. Accommodating them in a theoretically sound manner will lead to more complex models of ecosystems.</p>
<p>However, while this added complexity may lead to resistance by some innovation ecosystems scholars, one can argue that the innovation ecosystems discourse as a whole is likely to benefit from this added complexity. Ethics is an important part of what drives humans and their decisions. A theoretical approach that ignores an important variable that can influence models, descriptions and predictions is likely to have less explanatory value than one that includes them.</p>
<p>The challenge of integrating ethical concerns and responsible innovation into innovation ecosystem theory are substantial. It is not just a question of simplicity and elegance of models but of the basic Fig. 2. Possible interventions in AI innovation ecosystems to support responsibility (adapted from (Stahl, 2021).</p>
<p>understanding of what drives phenomena in question. Philosophical ethics is typically described in terms of individual decisions and often assumes a certain level of knowledge of the outcomes of such decisions. These assumptions do not sit well with systems theoretical approaches including the innovation ecosystems literature which typically do not locate agency at the level of the individual but describe social phenomena in terms of interactions that follow the logic of systems. Some philosophers, notably the German philosopher and social scientist Jürgen Habermas (1998) have therefore suggested that systems approaches and those drawing on the individual life-world where ethical considerations are rooted, are largely incompatible. The theoretical relationship between ethics and innovation systems is thus complex and calls for a more detailed examination that can be offered here. A starting point, however, may be that social and socio-technical systems are constituted from humans and the moral preferences and ethical reflections of these individuals contribute to the way in which the system unfolds. It therefore seems plausible that it is possible to bridge the gap between a description of socio-technical phenomena based on innovation ecosystems theory and one that focuses on ethical aspects carried by individuals.</p>
<p>The innovation ecosystems literature will not have to start from scratch when integrating these concerns. It can draw on related fields that have long incorporated a richer understanding of human behaviour in systems-oriented explanations of human behaviour and technology use, leading to outcomes such as soft systems methods (Checkland &amp; Poulter, 2006, 2010) or Enid Mumford's ETHICS methods (Mumford &amp; Ward, 1968;Mumford, 1995). This paper can thus be read as an invitation to innovation ecosystems scholars to broaden their theoretical and methodological toolbox to ensure that they include crucial factors such as ethical concerns by engaging with the idea of responsible innovation ecosystems. This also includes a call for further research on how this can be achieved.</p>
<p>The other body of literature that may benefit from the argument put forward in this paper is that on RRI. RRI research is sensitive to the question how political structures shape research and innovation processes (van Oudheusden, 2014). Most of the more detailed studies on RRI focus either on a particular technology, such as nanotechnology (Kjolberg &amp; Strand, 2011;Shelley-Egan &amp; Davies, 2013), neurotechnology (Arentshorst, de Cock Buning, &amp; Broerse, 2016;Salles &amp; Farisco, 2020) or synthetic biology (J. Y. Zhang, Marris, &amp; Rose, 2011) or they focus on specific projects (Aicardi, Reinsborough, &amp; Rose, 2017).</p>
<p>What these approaches to RRI miss is the systems character of research and innovation and how to deal with it. This is not to say that the RRI discourse uncritically accepts a simplistic concept of research and innovation. In fact, the critique of such simplistic concepts of research and innovation forms a well-established part of the RRI discourse (Blok &amp; Lemmens, 2015;Nathan, 2015;Owen &amp; Pansera, 2019). However, so far the RRI discourse has not integrated these concerns into theoretical approaches that lend themselves to practical implementation. By paying more attention to the innovation ecosystems literature, RRI could benefit by drawing on important concepts from the business and management literature (Owen et al., 2021) as well as the various themes of systems theory.</p>
<p>The paper has proposed the concept of responsible innovation ecosystems as a response to a lack of attention to ethical concerns in the innovation ecosystems literature. In doing so, however, it glosses over a number of conceptual difficulties and possible objections. It ignores the criticism that has been levelled at the innovation ecosystem concept itself and the limitation of this concept. It is not universally agreed that the ecosystem concept is the best one to understand innovation (Oh, Phillips, Park, &amp; Lee, 2016). Furthermore, from an ethical perspective, ecosystems are problematic because as natural occurrences they raise the question of how to move from a descriptive perspective to a prescriptive one, the problem known in philosophy as the is-ought problem. The reliance on principles of evolution can also suggest an affinity to social Darwinism, where outcomes of "natural" processes of selection are seen as justified. Similarly, the paper has adopted a high-level view of the RRI discourse without being able to do justice to well-grounded criticism, such as the contentious nature of the term "innovation" (von Schomberg &amp; Blok, 2019).</p>
<p>A second limitation of the paper is its conceptual nature. The identification of the gap in the innovation ecosystems literature, the proposal of the creation of the concept of responsible innovation ecosystems and the discussion of what these might look like in the AI space are all purely conceptual. The paper proposes a different way of thinking about innovation ecosystems but does not provide empirical evidence that this way of thinking would prove productive.</p>
<p>These limitations point in the direction of various avenues that future research could undertake. It is worthwhile to continue conceptual research on the strengths and limitations of the innovation ecosystems concept. A more detailed analysis of the links between innovation ecosystems and different concepts of ethics and types of ethical concern would allow for a more fine-grained analysis. Similarly, a more explicit integration of systems theoretical thinking into the RRI discourse may well lead to new avenues of thinking about and implementing RRI.</p>
<p>In addition, the ideas presented here lend themselves to a range of empirical research activities. This could start with the simple integration of ethics-related data into innovation ecosystems research. This could be data about ethical perceptions and views of ecosystem members or about how ethical concerns drive the structure and interaction of ecosystems. Such research could be used to provide evidence to decide whether this paper's proposal to include ethical issues actually allows a richer understanding of real-life innovation ecosystems. Another avenue could be to integrate established systems-theoretical work that incorporates ethical concerns into empirical innovation ecosystems research. This could translate into new types of innovation ecosystems research, based on methods like action research (Baskerville &amp; Wood-Harper, 1998) or design science research (Hevner, March, Park, &amp; Ram, 2004) that could help to develop innovation ecosystems that explicitly incorporate ethical awareness.</p>
<p>This paper has argued that innovation ecosystems have consequences beyond their immediate technical environment and should therefore be enabled to be responsible. Based on a conceptual analysis, it has developed requirements for responsible innovation ecosystems. It has then argued that AI can be understood as a set over interlinking and overlapping innovation ecosystems. The high-profile discussion of ethics of AI suggests that these AI ecosystems should strive to be responsible. The paper then set out some suggestions of how this could be achieved, how AI innovation ecosystems can be shaped in ways that render them responsible. It has spelled out implications for theory and suggested avenues for future research.</p>
<p>The paper therefore makes a contribution to the innovation ecosystems discourse and shows that the integration of broader concerns into innovation ecosystems is possible and points to ways in which this can be achieved. While ethical preferences and social expectations have inevitably played a practical part in innovation ecosystems, the paper suggests that there are ways of structuring innovation ecosystems in ways that give them more prominence to ensure that the processes and products arising from innovation ecosystems activities are desirable, acceptable and sustainable.</p>
<p>The paper makes a similar conceptual contribution to the RRI discourse by showing that there are ways to integrate core ideas of RRI in related discourses in innovation studies, a closely related discipline that may not have received the attention it deserves in RRI. It shows that RRI can make relevant contributions to innovation studies, but that there are similarly ways in which established conceptual approaches Finally, the paper contributes to the discourse on AI, in particular AI ethics and AI policy. By framing AI as a set of interlinked innovation ecosystems and applying the concept of responsible innovation ecosystems, the paper has been able to deduce a set of specific suggestions and recommendations that will allow dealing with ethical, social and human rights concerns. The AI ethics and AI policy discourses are currently in a very intense phase with national and European legislation being prepared and many mitigation options being discussed. The innovation systems metaphor may be particularly useful in this crowded space, as the three requirements for responsible innovation ecosystems, i.e. delimitation, knowledge base and adaptive governance can help to order this discourse and clarify possible relationships between different options.</p>
<p>The author acknowledges the contribution of the members of several group to the development of the argument presented here. These notably include the members of the SHERPA project consortium, members of the Human Brain Project, and member of the Centre for Computing and Social Responsibility at De Montfort University, Leicester, UK.</p>
<p>This research was funded by the European Union's Horizon 2020 Research and Innovation Programme Under Grant Agreement No. 786641 (SHERPA), 785907 (Human Brain Project SGA2) and 945539 (Human Brain Project SGA3).</p>
<p>The paper is a single authored paper wholly and exclusively prepared and written by the author: Bernd Carsten STAHL.</p>
</text>
</tei>